{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2seOirvygK-",
        "outputId": "aab1e852-c859-46f4-eb50-b1380c937c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EGiDy8pwg_Uu-JRKHutu-gfDe0tXbpp2\n",
            "To: /content/ViQuAD2.0.zip\n",
            "100% 14.2M/14.2M [00:00<00:00, 27.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1EGiDy8pwg_Uu-JRKHutu-gfDe0tXbpp2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git\n",
        "# B·∫≠t sparse-checkout\n",
        "!git clone --depth 1 --filter=blob:none --sparse https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!git sparse-checkout set examples/pytorch/question-answering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX9Yp7XZ0aXr",
        "outputId": "6b6ca4ec-70f7-46ae-e652-dab6443ae28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 855, done.\u001b[K\n",
            "remote: Counting objects: 100% (855/855), done.\u001b[K\n",
            "remote: Compressing objects: 100% (821/821), done.\u001b[K\n",
            "remote: Total 855 (delta 1), reused 279 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (855/855), 208.11 KiB | 4.73 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 3 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 55.31 KiB | 4.61 MiB/s, done.\n",
            "/content/transformers\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 10 (delta 4), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), 37.27 KiB | 7.45 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ViQuAD2.0.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5J0iMEJ0aoF",
        "outputId": "1ce04e76-9801-41da-f388-8d6b1210f36b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ViQuAD2.0.zip\n",
            "  inflating: ViQuAD2.0/test/ground_truth_private_test.json  \n",
            "  inflating: ViQuAD2.0/train/train.json  \n",
            "  inflating: ViQuAD2.0/dev/dev.json  \n",
            "  inflating: ViQuAD2.0/test/Private_Test_ref.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/transformers/examples/pytorch/question-answering/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XItwMvl70bpH",
        "outputId": "566a8a80-1954-420c-b976-39d756b498c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (1.2.1)\n",
            "Collecting datasets>=1.8.0 (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (2.5.1+cu121)\n",
            "Collecting evaluate (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 4))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.67.1)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers -y\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgn9oGCV0e9Q",
        "outputId": "51f6d597-dcdc-45f1-f0c3-fbb0bc393802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.47.1\n",
            "Uninstalling transformers-4.47.1:\n",
            "  Successfully uninstalled transformers-4.47.1\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-550myz37\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-550myz37\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit e5fd865ebae062b7cf03a81b8c6affeb39f30bec\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2024.12.14)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.48.0.dev0-py3-none-any.whl size=10329101 sha256=f4d004a307811efdd68377326d17a08a0bfd4249623042646eb1a2c6f8d9b1cf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l_8903na/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.48.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "  --model_name_or_path bert-base-multilingual-cased \\\n",
        "  --train_file /content/transformers/ViQuAD2.0/train/train.json \\\n",
        "  --validation_file /content/transformers/ViQuAD2.0/dev/dev.json \\\n",
        "  --test_file /content/transformers/ViQuAD2.0/test/Private_Test_ref.json \\\n",
        "  --output_dir ./results \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --overwrite_output_dir \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjXK9UaB0gHe",
        "outputId": "96f4e717-1d4c-4393-c622-343fc7937870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-03 15:49:08.505895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-03 15:49:08.525402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-03 15:49:08.531244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-03 15:49:08.545840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-03 15:49:09.725491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/03/2025 15:49:11 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "01/03/2025 15:49:11 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./results/runs/Jan03_15-49-11_0811d96ebebd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./results,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./results,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-7d7ac5ea573e91b3\n",
            "01/03/2025 15:49:12 - INFO - datasets.builder - Using custom data configuration default-7d7ac5ea573e91b3\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "01/03/2025 15:49:12 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "01/03/2025 15:49:12 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "01/03/2025 15:49:12 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "Downloading took 0.0 min\n",
            "01/03/2025 15:49:12 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "01/03/2025 15:49:12 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "01/03/2025 15:49:12 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 22765 examples [00:01, 11687.23 examples/s]\n",
            "Generating validation split\n",
            "01/03/2025 15:49:14 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 5692 examples [00:00, 12969.05 examples/s]\n",
            "Generating test split\n",
            "01/03/2025 15:49:15 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 7301 examples [00:00, 16188.84 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "01/03/2025 15:49:15 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "01/03/2025 15:49:15 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "config.json: 100% 625/625 [00:00<00:00, 4.10MB/s]\n",
            "[INFO|configuration_utils.py:696] 2025-01-03 15:49:16,080 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-03 15:49:16,082 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 240kB/s]\n",
            "[INFO|configuration_utils.py:696] 2025-01-03 15:49:16,589 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-03 15:49:16,590 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "vocab.txt: 100% 996k/996k [00:00<00:00, 1.34MB/s]\n",
            "tokenizer.json: 100% 1.96M/1.96M [00:00<00:00, 7.41MB/s]\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:696] 2025-01-03 15:49:20,322 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-03 15:49:20,323 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 714M/714M [00:05<00:00, 129MB/s]\n",
            "[INFO|modeling_utils.py:3893] 2025-01-03 15:49:26,928 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n",
            "[INFO|logging.py:343] 2025-01-03 15:49:27,066 >> A pretrained model of type `BertForQuestionAnswering` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
            "* `bert.embeddings.LayerNorm.beta` -> `bert.embeddings.LayerNorm.bias`\n",
            "* `bert.embeddings.LayerNorm.gamma` -> `bert.embeddings.LayerNorm.weight`\n",
            "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
            "[INFO|modeling_utils.py:4838] 2025-01-03 15:49:27,092 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:4850] 2025-01-03 15:49:27,092 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/22765 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f83a4f40cb5fae15.arrow\n",
            "01/03/2025 15:49:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f83a4f40cb5fae15.arrow\n",
            "Running tokenizer on train dataset: 100% 22765/22765 [00:21<00:00, 1074.87 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/5692 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-34dd4d2106e5c28d.arrow\n",
            "01/03/2025 15:49:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-34dd4d2106e5c28d.arrow\n",
            "Running tokenizer on validation dataset: 100% 5692/5692 [00:06<00:00, 909.88 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/7301 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-30ca8675cfa61c3b.arrow\n",
            "01/03/2025 15:49:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-30ca8675cfa61c3b.arrow\n",
            "Running tokenizer on prediction dataset: 100% 7301/7301 [00:07<00:00, 960.64 examples/s] \n",
            "Downloading builder script: 100% 4.53k/4.53k [00:00<00:00, 14.7MB/s]\n",
            "Downloading extra modules: 100% 3.32k/3.32k [00:00<00:00, 14.7MB/s]\n",
            "[INFO|trainer.py:2369] 2025-01-03 15:50:06,827 >> ***** Running training *****\n",
            "[INFO|trainer.py:2370] 2025-01-03 15:50:06,827 >>   Num examples = 24,438\n",
            "[INFO|trainer.py:2371] 2025-01-03 15:50:06,827 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2372] 2025-01-03 15:50:06,827 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:2375] 2025-01-03 15:50:06,827 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2376] 2025-01-03 15:50:06,827 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2377] 2025-01-03 15:50:06,827 >>   Total optimization steps = 4,584\n",
            "[INFO|trainer.py:2378] 2025-01-03 15:50:06,828 >>   Number of trainable parameters = 177,264,386\n",
            "[INFO|integration_utils.py:811] 2025-01-03 15:50:06,836 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/transformers/wandb/run-20250103_155220-a9x4m5tz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./results\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/21522798-uit/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/21522798-uit/huggingface/runs/a9x4m5tz\u001b[0m\n",
            "{'loss': 2.2019, 'grad_norm': 18.974044799804688, 'learning_rate': 2.6727748691099477e-05, 'epoch': 0.33}\n",
            " 11% 500/4584 [09:34<1:18:08,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:01:55,817 >> Saving model checkpoint to ./results/checkpoint-500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:01:55,819 >> Configuration saved in ./results/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:02:03,114 >> Model weights saved in ./results/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:02:03,116 >> tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:02:03,116 >> Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 1.6754, 'grad_norm': 19.527013778686523, 'learning_rate': 2.3455497382198953e-05, 'epoch': 0.65}\n",
            " 22% 1000/4584 [19:25<1:08:31,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:11:47,359 >> Saving model checkpoint to ./results/checkpoint-1000\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:11:47,361 >> Configuration saved in ./results/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:11:58,586 >> Model weights saved in ./results/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:11:58,589 >> tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:11:58,589 >> Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.5526, 'grad_norm': 26.89529800415039, 'learning_rate': 2.018324607329843e-05, 'epoch': 0.98}\n",
            " 33% 1500/4584 [29:18<59:00,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:21:40,436 >> Saving model checkpoint to ./results/checkpoint-1500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:21:40,438 >> Configuration saved in ./results/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:21:47,681 >> Model weights saved in ./results/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:21:47,683 >> tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:21:47,683 >> Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 1.2215, 'grad_norm': 22.804689407348633, 'learning_rate': 1.6910994764397905e-05, 'epoch': 1.31}\n",
            " 44% 2000/4584 [39:09<49:24,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:31:30,606 >> Saving model checkpoint to ./results/checkpoint-2000\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:31:30,608 >> Configuration saved in ./results/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:31:39,148 >> Model weights saved in ./results/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:31:39,150 >> tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:31:39,150 >> Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 1.1682, 'grad_norm': 23.251840591430664, 'learning_rate': 1.3638743455497383e-05, 'epoch': 1.64}\n",
            " 55% 2500/4584 [48:59<39:54,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:41:21,258 >> Saving model checkpoint to ./results/checkpoint-2500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:41:21,261 >> Configuration saved in ./results/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:41:32,924 >> Model weights saved in ./results/checkpoint-2500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:41:32,926 >> tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:41:32,927 >> Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 1.1427, 'grad_norm': 21.312780380249023, 'learning_rate': 1.0366492146596857e-05, 'epoch': 1.96}\n",
            " 65% 3000/4584 [58:54<30:21,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:51:16,038 >> Saving model checkpoint to ./results/checkpoint-3000\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:51:16,039 >> Configuration saved in ./results/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:51:32,109 >> Model weights saved in ./results/checkpoint-3000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:51:32,112 >> tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:51:32,112 >> Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.8837, 'grad_norm': 25.836029052734375, 'learning_rate': 7.094240837696335e-06, 'epoch': 2.29}\n",
            " 76% 3500/4584 [1:09:10<20:48,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 17:01:32,484 >> Saving model checkpoint to ./results/checkpoint-3500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:01:32,486 >> Configuration saved in ./results/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:02:17,874 >> Model weights saved in ./results/checkpoint-3500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:02:17,875 >> tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:02:17,876 >> Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.8188, 'grad_norm': 16.85807991027832, 'learning_rate': 3.821989528795812e-06, 'epoch': 2.62}\n",
            " 87% 4000/4584 [1:19:37<11:11,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 17:11:59,088 >> Saving model checkpoint to ./results/checkpoint-4000\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:11:59,089 >> Configuration saved in ./results/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:12:03,966 >> Model weights saved in ./results/checkpoint-4000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:12:03,968 >> tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:12:03,968 >> Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.8223, 'grad_norm': 31.01511001586914, 'learning_rate': 5.497382198952879e-07, 'epoch': 2.95}\n",
            " 98% 4500/4584 [1:29:33<01:36,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 17:21:54,642 >> Saving model checkpoint to ./results/checkpoint-4500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:21:54,645 >> Configuration saved in ./results/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:22:07,379 >> Model weights saved in ./results/checkpoint-4500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:22:07,381 >> tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:22:07,382 >> Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
            "100% 4584/4584 [1:31:31<00:00,  1.06it/s][INFO|trainer.py:3911] 2025-01-03 17:23:53,400 >> Saving model checkpoint to ./results/checkpoint-4584\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:23:53,402 >> Configuration saved in ./results/checkpoint-4584/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:24:13,694 >> Model weights saved in ./results/checkpoint-4584/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:24:13,697 >> tokenizer config file saved in ./results/checkpoint-4584/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:24:13,697 >> Special tokens file saved in ./results/checkpoint-4584/special_tokens_map.json\n",
            "[INFO|trainer.py:2643] 2025-01-03 17:25:13,399 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5706.5714, 'train_samples_per_second': 12.847, 'train_steps_per_second': 0.803, 'train_loss': 1.2669352569713226, 'epoch': 3.0}\n",
            "100% 4584/4584 [1:32:51<00:00,  1.22s/it]\n",
            "[INFO|trainer.py:3911] 2025-01-03 17:25:13,424 >> Saving model checkpoint to ./results\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:25:13,426 >> Configuration saved in ./results/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:25:20,035 >> Model weights saved in ./results/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:25:20,037 >> tokenizer config file saved in ./results/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:25:20,037 >> Special tokens file saved in ./results/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 13380807GF\n",
            "  train_loss               =     1.2669\n",
            "  train_runtime            = 1:35:06.57\n",
            "  train_samples            =      24438\n",
            "  train_samples_per_second =     12.847\n",
            "  train_steps_per_second   =      0.803\n",
            "01/03/2025 17:25:20 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:917] 2025-01-03 17:25:20,122 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:4227] 2025-01-03 17:25:20,125 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4229] 2025-01-03 17:25:20,125 >>   Num examples = 6105\n",
            "[INFO|trainer.py:4232] 2025-01-03 17:25:20,125 >>   Batch size = 8\n",
            "100% 763/764 [02:22<00:00,  5.37it/s]01/03/2025 17:27:56 - INFO - utils_qa - Post-processing 5692 example predictions split into 6105 features.\n",
            "\n",
            "  0% 0/5692 [00:00<?, ?it/s]\u001b[A\n",
            "  0% 26/5692 [00:00<00:22, 257.05it/s]\u001b[A\n",
            "  1% 53/5692 [00:00<00:21, 263.94it/s]\u001b[A\n",
            "  1% 80/5692 [00:00<00:21, 266.27it/s]\u001b[A\n",
            "  2% 113/5692 [00:00<00:19, 290.17it/s]\u001b[A\n",
            "  3% 143/5692 [00:00<00:20, 276.30it/s]\u001b[A\n",
            "  3% 172/5692 [00:00<00:19, 279.81it/s]\u001b[A\n",
            "  4% 203/5692 [00:00<00:19, 286.97it/s]\u001b[A\n",
            "  4% 235/5692 [00:00<00:18, 296.68it/s]\u001b[A\n",
            "  5% 265/5692 [00:00<00:18, 294.31it/s]\u001b[A\n",
            "  5% 295/5692 [00:01<00:18, 295.53it/s]\u001b[A\n",
            "  6% 325/5692 [00:01<00:18, 296.22it/s]\u001b[A\n",
            "  6% 355/5692 [00:01<00:18, 290.19it/s]\u001b[A\n",
            "  7% 386/5692 [00:01<00:18, 293.51it/s]\u001b[A\n",
            "  7% 419/5692 [00:01<00:17, 300.64it/s]\u001b[A\n",
            "  8% 450/5692 [00:01<00:17, 294.55it/s]\u001b[A\n",
            "  8% 480/5692 [00:01<00:18, 286.43it/s]\u001b[A\n",
            "  9% 509/5692 [00:01<00:18, 282.48it/s]\u001b[A\n",
            "  9% 540/5692 [00:01<00:17, 289.11it/s]\u001b[A\n",
            " 10% 569/5692 [00:01<00:17, 288.16it/s]\u001b[A\n",
            " 11% 598/5692 [00:02<00:17, 288.15it/s]\u001b[A\n",
            " 11% 627/5692 [00:02<00:17, 283.45it/s]\u001b[A\n",
            " 12% 659/5692 [00:02<00:17, 293.01it/s]\u001b[A\n",
            " 12% 689/5692 [00:02<00:17, 290.49it/s]\u001b[A\n",
            " 13% 722/5692 [00:02<00:16, 300.39it/s]\u001b[A\n",
            " 13% 753/5692 [00:02<00:16, 295.07it/s]\u001b[A\n",
            " 14% 783/5692 [00:02<00:16, 292.03it/s]\u001b[A\n",
            " 14% 814/5692 [00:02<00:16, 295.79it/s]\u001b[A\n",
            " 15% 846/5692 [00:02<00:16, 301.04it/s]\u001b[A\n",
            " 15% 877/5692 [00:03<00:16, 290.38it/s]\u001b[A\n",
            " 16% 910/5692 [00:03<00:15, 299.83it/s]\u001b[A\n",
            " 17% 941/5692 [00:03<00:16, 294.67it/s]\u001b[A\n",
            " 17% 973/5692 [00:03<00:15, 299.88it/s]\u001b[A\n",
            " 18% 1004/5692 [00:03<00:15, 299.95it/s]\u001b[A\n",
            " 18% 1035/5692 [00:03<00:15, 300.61it/s]\u001b[A\n",
            " 19% 1066/5692 [00:03<00:15, 300.63it/s]\u001b[A\n",
            " 19% 1098/5692 [00:03<00:15, 304.60it/s]\u001b[A\n",
            " 20% 1129/5692 [00:03<00:14, 305.22it/s]\u001b[A\n",
            " 20% 1160/5692 [00:03<00:14, 305.35it/s]\u001b[A\n",
            " 21% 1192/5692 [00:04<00:14, 308.92it/s]\u001b[A\n",
            " 22% 1225/5692 [00:04<00:14, 313.22it/s]\u001b[A\n",
            " 22% 1257/5692 [00:04<00:14, 296.92it/s]\u001b[A\n",
            " 23% 1287/5692 [00:04<00:14, 293.73it/s]\u001b[A\n",
            " 23% 1320/5692 [00:04<00:14, 303.35it/s]\u001b[A\n",
            " 24% 1351/5692 [00:04<00:14, 303.23it/s]\u001b[A\n",
            " 24% 1382/5692 [00:04<00:14, 302.95it/s]\u001b[A\n",
            " 25% 1413/5692 [00:04<00:14, 294.45it/s]\u001b[A\n",
            " 25% 1443/5692 [00:04<00:14, 286.95it/s]\u001b[A\n",
            " 26% 1473/5692 [00:05<00:14, 290.35it/s]\u001b[A\n",
            " 26% 1503/5692 [00:05<00:14, 291.87it/s]\u001b[A\n",
            " 27% 1533/5692 [00:05<00:14, 290.68it/s]\u001b[A\n",
            " 27% 1563/5692 [00:05<00:14, 285.91it/s]\u001b[A\n",
            " 28% 1596/5692 [00:05<00:13, 296.91it/s]\u001b[A\n",
            " 29% 1626/5692 [00:05<00:13, 297.78it/s]\u001b[A\n",
            " 29% 1656/5692 [00:05<00:13, 294.08it/s]\u001b[A\n",
            " 30% 1688/5692 [00:05<00:13, 300.18it/s]\u001b[A\n",
            " 30% 1719/5692 [00:05<00:13, 296.16it/s]\u001b[A\n",
            " 31% 1750/5692 [00:05<00:13, 300.14it/s]\u001b[A\n",
            " 31% 1781/5692 [00:06<00:13, 295.01it/s]\u001b[A\n",
            " 32% 1811/5692 [00:06<00:13, 290.53it/s]\u001b[A\n",
            " 32% 1841/5692 [00:06<00:13, 292.80it/s]\u001b[A\n",
            " 33% 1871/5692 [00:06<00:13, 280.47it/s]\u001b[A\n",
            " 33% 1900/5692 [00:06<00:13, 281.36it/s]\u001b[A\n",
            " 34% 1929/5692 [00:06<00:13, 283.47it/s]\u001b[A\n",
            " 34% 1958/5692 [00:06<00:13, 282.34it/s]\u001b[A\n",
            " 35% 1987/5692 [00:06<00:13, 276.30it/s]\u001b[A\n",
            " 35% 2015/5692 [00:06<00:13, 275.33it/s]\u001b[A\n",
            " 36% 2044/5692 [00:06<00:13, 278.21it/s]\u001b[A\n",
            " 36% 2072/5692 [00:07<00:12, 278.61it/s]\u001b[A\n",
            " 37% 2100/5692 [00:07<00:13, 267.88it/s]\u001b[A\n",
            " 37% 2127/5692 [00:07<00:13, 261.15it/s]\u001b[A\n",
            " 38% 2154/5692 [00:07<00:13, 252.77it/s]\u001b[A\n",
            " 38% 2182/5692 [00:07<00:13, 258.45it/s]\u001b[A\n",
            " 39% 2214/5692 [00:07<00:12, 273.94it/s]\u001b[A\n",
            " 39% 2243/5692 [00:07<00:12, 277.81it/s]\u001b[A\n",
            " 40% 2272/5692 [00:07<00:12, 279.05it/s]\u001b[A\n",
            " 40% 2300/5692 [00:08<00:14, 228.14it/s]\u001b[A\n",
            " 41% 2325/5692 [00:08<00:16, 204.31it/s]\u001b[A\n",
            " 41% 2347/5692 [00:08<00:17, 191.10it/s]\u001b[A\n",
            " 42% 2368/5692 [00:08<00:19, 169.51it/s]\u001b[A\n",
            " 42% 2386/5692 [00:08<00:19, 169.38it/s]\u001b[A\n",
            " 42% 2404/5692 [00:08<00:19, 164.89it/s]\u001b[A\n",
            " 43% 2423/5692 [00:08<00:19, 169.37it/s]\u001b[A\n",
            " 43% 2441/5692 [00:08<00:19, 169.71it/s]\u001b[A\n",
            " 43% 2459/5692 [00:09<00:19, 169.12it/s]\u001b[A\n",
            " 44% 2477/5692 [00:09<00:18, 170.64it/s]\u001b[A\n",
            " 44% 2495/5692 [00:09<00:18, 171.20it/s]\u001b[A\n",
            " 44% 2513/5692 [00:09<00:18, 171.39it/s]\u001b[A\n",
            " 44% 2531/5692 [00:09<00:19, 161.99it/s]\u001b[A\n",
            " 45% 2549/5692 [00:09<00:18, 166.52it/s]\u001b[A\n",
            " 45% 2566/5692 [00:09<00:19, 163.53it/s]\u001b[A\n",
            " 45% 2584/5692 [00:09<00:18, 165.99it/s]\u001b[A\n",
            " 46% 2601/5692 [00:09<00:18, 163.27it/s]\u001b[A\n",
            " 46% 2618/5692 [00:09<00:18, 164.69it/s]\u001b[A\n",
            " 46% 2636/5692 [00:10<00:18, 163.85it/s]\u001b[A\n",
            " 47% 2653/5692 [00:10<00:18, 164.88it/s]\u001b[A\n",
            " 47% 2670/5692 [00:10<00:19, 157.56it/s]\u001b[A\n",
            " 47% 2686/5692 [00:10<00:19, 157.18it/s]\u001b[A\n",
            " 47% 2702/5692 [00:10<00:19, 151.58it/s]\u001b[A\n",
            " 48% 2718/5692 [00:10<00:20, 148.56it/s]\u001b[A\n",
            " 48% 2734/5692 [00:10<00:19, 151.09it/s]\u001b[A\n",
            " 48% 2750/5692 [00:10<00:19, 149.35it/s]\u001b[A\n",
            " 49% 2767/5692 [00:10<00:18, 154.38it/s]\u001b[A\n",
            " 49% 2783/5692 [00:11<00:19, 149.63it/s]\u001b[A\n",
            " 49% 2800/5692 [00:11<00:18, 155.36it/s]\u001b[A\n",
            " 49% 2816/5692 [00:11<00:18, 152.85it/s]\u001b[A\n",
            " 50% 2832/5692 [00:11<00:18, 154.75it/s]\u001b[A\n",
            " 50% 2848/5692 [00:11<00:18, 155.14it/s]\u001b[A\n",
            " 50% 2864/5692 [00:11<00:18, 150.65it/s]\u001b[A\n",
            " 51% 2880/5692 [00:11<00:18, 151.04it/s]\u001b[A\n",
            " 51% 2898/5692 [00:11<00:17, 156.98it/s]\u001b[A\n",
            " 51% 2916/5692 [00:11<00:17, 161.17it/s]\u001b[A\n",
            " 52% 2933/5692 [00:12<00:16, 163.43it/s]\u001b[A\n",
            " 52% 2950/5692 [00:12<00:17, 157.34it/s]\u001b[A\n",
            " 52% 2968/5692 [00:12<00:16, 163.24it/s]\u001b[A\n",
            " 52% 2985/5692 [00:12<00:16, 164.28it/s]\u001b[A\n",
            " 53% 3002/5692 [00:12<00:16, 165.33it/s]\u001b[A\n",
            " 53% 3020/5692 [00:12<00:15, 169.13it/s]\u001b[A\n",
            " 53% 3043/5692 [00:12<00:14, 185.68it/s]\u001b[A\n",
            " 54% 3076/5692 [00:12<00:11, 226.40it/s]\u001b[A\n",
            " 54% 3099/5692 [00:12<00:11, 226.94it/s]\u001b[A\n",
            " 55% 3130/5692 [00:12<00:10, 251.47it/s]\u001b[A\n",
            " 56% 3163/5692 [00:13<00:09, 272.83it/s]\u001b[A\n",
            " 56% 3191/5692 [00:13<00:09, 272.11it/s]\u001b[A\n",
            " 57% 3222/5692 [00:13<00:08, 282.03it/s]\u001b[A\n",
            " 57% 3253/5692 [00:13<00:08, 288.39it/s]\u001b[A\n",
            " 58% 3282/5692 [00:13<00:08, 286.65it/s]\u001b[A\n",
            " 58% 3311/5692 [00:13<00:08, 284.57it/s]\u001b[A\n",
            " 59% 3340/5692 [00:13<00:08, 275.66it/s]\u001b[A\n",
            " 59% 3368/5692 [00:13<00:08, 274.41it/s]\u001b[A\n",
            " 60% 3396/5692 [00:13<00:08, 263.69it/s]\u001b[A\n",
            " 60% 3427/5692 [00:13<00:08, 274.89it/s]\u001b[A\n",
            " 61% 3455/5692 [00:14<00:08, 264.34it/s]\u001b[A\n",
            " 61% 3483/5692 [00:14<00:08, 264.47it/s]\u001b[A\n",
            " 62% 3513/5692 [00:14<00:08, 271.96it/s]\u001b[A\n",
            " 62% 3545/5692 [00:14<00:07, 285.63it/s]\u001b[A\n",
            " 63% 3574/5692 [00:14<00:07, 286.43it/s]\u001b[A\n",
            " 63% 3603/5692 [00:14<00:07, 284.52it/s]\u001b[A\n",
            " 64% 3632/5692 [00:14<00:07, 280.05it/s]\u001b[A\n",
            " 64% 3661/5692 [00:14<00:07, 277.22it/s]\u001b[A\n",
            " 65% 3690/5692 [00:14<00:07, 279.93it/s]\u001b[A\n",
            " 65% 3719/5692 [00:15<00:06, 282.32it/s]\u001b[A\n",
            " 66% 3748/5692 [00:15<00:06, 282.15it/s]\u001b[A\n",
            " 66% 3781/5692 [00:15<00:06, 295.30it/s]\u001b[A\n",
            " 67% 3811/5692 [00:15<00:06, 290.40it/s]\u001b[A\n",
            " 67% 3841/5692 [00:15<00:06, 286.95it/s]\u001b[A\n",
            " 68% 3873/5692 [00:15<00:06, 296.01it/s]\u001b[A\n",
            " 69% 3905/5692 [00:15<00:05, 300.66it/s]\u001b[A\n",
            " 69% 3936/5692 [00:15<00:06, 291.91it/s]\u001b[A\n",
            " 70% 3968/5692 [00:15<00:05, 298.87it/s]\u001b[A\n",
            " 70% 4001/5692 [00:15<00:05, 303.85it/s]\u001b[A\n",
            " 71% 4032/5692 [00:16<00:05, 305.13it/s]\u001b[A\n",
            " 71% 4063/5692 [00:16<00:05, 299.56it/s]\u001b[A\n",
            " 72% 4097/5692 [00:16<00:05, 309.57it/s]\u001b[A\n",
            " 73% 4129/5692 [00:16<00:05, 299.31it/s]\u001b[A\n",
            " 73% 4160/5692 [00:16<00:05, 297.77it/s]\u001b[A\n",
            " 74% 4190/5692 [00:16<00:05, 297.48it/s]\u001b[A\n",
            " 74% 4220/5692 [00:16<00:05, 292.63it/s]\u001b[A\n",
            " 75% 4250/5692 [00:16<00:05, 279.14it/s]\u001b[A\n",
            " 75% 4283/5692 [00:16<00:04, 291.40it/s]\u001b[A\n",
            " 76% 4315/5692 [00:17<00:04, 297.32it/s]\u001b[A\n",
            " 76% 4345/5692 [00:17<00:04, 295.61it/s]\u001b[A\n",
            " 77% 4375/5692 [00:17<00:04, 287.92it/s]\u001b[A\n",
            " 77% 4406/5692 [00:17<00:04, 292.59it/s]\u001b[A\n",
            " 78% 4436/5692 [00:17<00:04, 285.94it/s]\u001b[A\n",
            " 78% 4465/5692 [00:17<00:04, 283.75it/s]\u001b[A\n",
            " 79% 4496/5692 [00:17<00:04, 288.22it/s]\u001b[A\n",
            " 79% 4525/5692 [00:17<00:04, 283.59it/s]\u001b[A\n",
            " 80% 4554/5692 [00:17<00:04, 280.32it/s]\u001b[A\n",
            " 81% 4585/5692 [00:17<00:03, 287.49it/s]\u001b[A\n",
            " 81% 4616/5692 [00:18<00:03, 292.17it/s]\u001b[A\n",
            " 82% 4647/5692 [00:18<00:03, 296.14it/s]\u001b[A\n",
            " 82% 4677/5692 [00:18<00:03, 286.77it/s]\u001b[A\n",
            " 83% 4709/5692 [00:18<00:03, 296.09it/s]\u001b[A\n",
            " 83% 4739/5692 [00:18<00:03, 285.49it/s]\u001b[A\n",
            " 84% 4772/5692 [00:18<00:03, 296.49it/s]\u001b[A\n",
            " 84% 4803/5692 [00:18<00:03, 295.94it/s]\u001b[A\n",
            " 85% 4833/5692 [00:18<00:03, 276.23it/s]\u001b[A\n",
            " 85% 4862/5692 [00:18<00:02, 278.80it/s]\u001b[A\n",
            " 86% 4893/5692 [00:19<00:02, 286.12it/s]\u001b[A\n",
            " 86% 4922/5692 [00:19<00:02, 284.29it/s]\u001b[A\n",
            " 87% 4956/5692 [00:19<00:02, 297.95it/s]\u001b[A\n",
            " 88% 4986/5692 [00:19<00:02, 295.21it/s]\u001b[A\n",
            " 88% 5017/5692 [00:19<00:02, 297.17it/s]\u001b[A\n",
            " 89% 5047/5692 [00:19<00:02, 294.34it/s]\u001b[A\n",
            " 89% 5077/5692 [00:19<00:02, 295.06it/s]\u001b[A\n",
            " 90% 5107/5692 [00:19<00:01, 293.89it/s]\u001b[A\n",
            " 90% 5137/5692 [00:19<00:01, 287.94it/s]\u001b[A\n",
            " 91% 5168/5692 [00:19<00:01, 292.28it/s]\u001b[A\n",
            " 91% 5198/5692 [00:20<00:01, 282.49it/s]\u001b[A\n",
            " 92% 5228/5692 [00:20<00:01, 286.37it/s]\u001b[A\n",
            " 92% 5259/5692 [00:20<00:01, 292.71it/s]\u001b[A\n",
            " 93% 5289/5692 [00:20<00:01, 289.43it/s]\u001b[A\n",
            " 93% 5319/5692 [00:20<00:01, 284.06it/s]\u001b[A\n",
            " 94% 5348/5692 [00:20<00:01, 273.59it/s]\u001b[A\n",
            " 94% 5378/5692 [00:20<00:01, 279.63it/s]\u001b[A\n",
            " 95% 5408/5692 [00:20<00:00, 284.48it/s]\u001b[A\n",
            " 96% 5437/5692 [00:20<00:00, 277.53it/s]\u001b[A\n",
            " 96% 5467/5692 [00:21<00:00, 282.21it/s]\u001b[A\n",
            " 97% 5497/5692 [00:21<00:00, 285.48it/s]\u001b[A\n",
            " 97% 5530/5692 [00:21<00:00, 293.12it/s]\u001b[A\n",
            " 98% 5562/5692 [00:21<00:00, 298.67it/s]\u001b[A\n",
            " 98% 5592/5692 [00:21<00:00, 296.67it/s]\u001b[A\n",
            " 99% 5622/5692 [00:21<00:00, 286.38it/s]\u001b[A\n",
            " 99% 5651/5692 [00:21<00:00, 272.97it/s]\u001b[A\n",
            "100% 5692/5692 [00:21<00:00, 260.46it/s]\n",
            "01/03/2025 17:28:17 - INFO - utils_qa - Saving predictions to ./results/eval_predictions.json.\n",
            "01/03/2025 17:28:17 - INFO - utils_qa - Saving nbest_preds to ./results/eval_nbest_predictions.json.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 715, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 672, in main\n",
            "    metrics = trainer.evaluate()\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 73, in evaluate\n",
            "    metrics = self.compute_metrics(eval_preds)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 634, in compute_metrics\n",
            "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/evaluate/module.py\", line 467, in compute\n",
            "    output = self._compute(**inputs, **compute_kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/squad.py\", line 110, in _compute\n",
            "    score = compute_score(dataset=dataset, predictions=pred_dict)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 67, in compute_score\n",
            "    exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 52, in metric_max_over_ground_truths\n",
            "    return max(scores_for_ground_truths)\n",
            "ValueError: max() arg is an empty sequence\n",
            "\u001b[1;34mwandb\u001b[0m: üöÄ View run \u001b[33m./results\u001b[0m at: \u001b[34mhttps://wandb.ai/21522798-uit/huggingface/runs/a9x4m5tz\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250103_155220-a9x4m5tz/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "# ƒê∆∞·ªùng d·∫´n checkpoint\n",
        "checkpoint_path = \"/content/transformers/results/checkpoint-4584\"\n",
        "\n",
        "# Load m√¥ h√¨nh v√† tokenizer\n",
        "model = BertForQuestionAnswering.from_pretrained(checkpoint_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)"
      ],
      "metadata": {
        "id": "A2sKK5XwNtkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load t·∫≠p test\n",
        "test_dataset = ViQuAD2(\n",
        "    '/content/Private_Test_ref.json',\n",
        "    '/content/ground_truth_private_test.json',\n",
        "    include_labels=True  # ƒê·ªÉ t√≠nh F1 v√† EM n·∫øu c·∫ßn\n",
        ")\n",
        "\n",
        "# Ho·∫∑c s·ª≠ d·ª•ng DataLoader n·∫øu c·∫ßn\n",
        "from torch.utils.data import DataLoader\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "ySGDZRqsNvFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r compressed_file.zip /content/transformers/results/checkpoint-4584"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7cX1aZCMSak",
        "outputId": "1f2dc8c5-ab32-4462-e924-a17383cbec70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/transformers/results/checkpoint-4584/ (stored 0%)\n",
            "  adding: content/transformers/results/checkpoint-4584/vocab.txt (deflated 45%)\n",
            "  adding: content/transformers/results/checkpoint-4584/tokenizer.json (deflated 67%)\n",
            "  adding: content/transformers/results/checkpoint-4584/scheduler.pt (deflated 57%)\n",
            "  adding: content/transformers/results/checkpoint-4584/trainer_state.json (deflated 66%)\n",
            "  adding: content/transformers/results/checkpoint-4584/optimizer.pt (deflated 50%)\n",
            "  adding: content/transformers/results/checkpoint-4584/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/transformers/results/checkpoint-4584/model.safetensors (deflated 7%)\n",
            "  adding: content/transformers/results/checkpoint-4584/training_args.bin (deflated 51%)\n",
            "  adding: content/transformers/results/checkpoint-4584/rng_state.pth (deflated 25%)\n",
            "  adding: content/transformers/results/checkpoint-4584/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/transformers/results/checkpoint-4584/config.json (deflated 53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8akVXwm_MpK5",
        "outputId": "42369af5-ce97-4ba1-a670-5fcde5519a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/transformers/mBert.zip /content/drive/MyDrive/data"
      ],
      "metadata": {
        "id": "2F1zCh_EOGp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/master/evaluate-v2.0.py -O evaluate_v2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeZJh6JRO1s5",
        "outputId": "3ae63047-87da-4702-8535-57b2d66d8e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-03 17:36:47--  https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/master/evaluate-v2.0.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10547 (10K) [text/plain]\n",
            "Saving to: ‚Äòevaluate_v2.py‚Äô\n",
            "\n",
            "evaluate_v2.py      100%[===================>]  10.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-03 17:36:47 (76.6 MB/s) - ‚Äòevaluate_v2.py‚Äô saved [10547/10547]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "    --model_name_or_path /content/transformers/results/checkpoint-4584 \\\n",
        "    --test_file /content/transformers/ViQuAD2.0/test/Private_Test_ref.json \\\n",
        "    --do_predict \\\n",
        "    --output_dir /content/transformers/results/predictions \\\n",
        "    --max_seq_length 384 \\\n",
        "    --doc_stride 128 \\\n",
        "    --pad_to_max_length \\\n",
        "    --per_device_eval_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU0j6G5FPmK8",
        "outputId": "be65fe65-2b63-4d75-cfbf-5febcc514579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-03 17:47:24.718576: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-03 17:47:24.742040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-03 17:47:24.748420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-03 17:47:24.763840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-03 17:47:25.982719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/03/2025 17:47:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "01/03/2025 17:47:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/transformers/results/predictions/runs/Jan03_17-47-28_0811d96ebebd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/transformers/results/predictions,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/transformers/results/predictions,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-28d194c0c95f7714\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Using custom data configuration default-28d194c0c95f7714\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "01/03/2025 17:47:29 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "Downloading took 0.0 min\n",
            "01/03/2025 17:47:29 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "01/03/2025 17:47:29 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating test split\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 7301 examples [00:00, 19804.37 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "01/03/2025 17:47:29 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:694] 2025-01-03 17:47:29,643 >> loading configuration file /content/transformers/results/checkpoint-4584/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-03 17:47:29,646 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/transformers/results/checkpoint-4584\",\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file chat_template.jinja\n",
            "[INFO|modeling_utils.py:3890] 2025-01-03 17:47:29,909 >> loading weights file /content/transformers/results/checkpoint-4584/model.safetensors\n",
            "[INFO|modeling_utils.py:4848] 2025-01-03 17:47:30,289 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
            "\n",
            "[INFO|modeling_utils.py:4856] 2025-01-03 17:47:30,289 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /content/transformers/results/checkpoint-4584.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
            "Running tokenizer on prediction dataset:   0% 0/7301 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f7cbcdec0445826a.arrow\n",
            "01/03/2025 17:47:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f7cbcdec0445826a.arrow\n",
            "Running tokenizer on prediction dataset: 100% 7301/7301 [00:08<00:00, 821.66 examples/s]\n",
            "01/03/2025 17:47:44 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:917] 2025-01-03 17:47:44,930 >> The following columns in the test set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:4227] 2025-01-03 17:47:44,938 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4229] 2025-01-03 17:47:44,939 >>   Num examples = 7743\n",
            "[INFO|trainer.py:4232] 2025-01-03 17:47:44,939 >>   Batch size = 16\n",
            "100% 484/484 [02:47<00:00,  2.89it/s]01/03/2025 17:50:44 - INFO - utils_qa - Post-processing 7301 example predictions split into 7743 features.\n",
            "\n",
            "  0% 0/7301 [00:00<?, ?it/s]\u001b[A\n",
            "  0% 34/7301 [00:00<00:21, 331.95it/s]\u001b[A\n",
            "  1% 70/7301 [00:00<00:21, 342.81it/s]\u001b[A\n",
            "  1% 105/7301 [00:00<00:22, 319.90it/s]\u001b[A\n",
            "  2% 140/7301 [00:00<00:21, 330.53it/s]\u001b[A\n",
            "  2% 174/7301 [00:00<00:26, 273.66it/s]\u001b[A\n",
            "  3% 203/7301 [00:00<00:35, 201.60it/s]\u001b[A\n",
            "  3% 227/7301 [00:00<00:36, 192.26it/s]\u001b[A\n",
            "  3% 249/7301 [00:01<00:43, 162.54it/s]\u001b[A\n",
            "  4% 268/7301 [00:01<00:42, 165.84it/s]\u001b[A\n",
            "  4% 300/7301 [00:01<00:34, 201.28it/s]\u001b[A\n",
            "  5% 332/7301 [00:01<00:30, 230.52it/s]\u001b[A\n",
            "  5% 368/7301 [00:01<00:26, 262.54it/s]\u001b[A\n",
            "  5% 400/7301 [00:01<00:24, 276.68it/s]\u001b[A\n",
            "  6% 433/7301 [00:01<00:23, 289.20it/s]\u001b[A\n",
            "  6% 467/7301 [00:01<00:22, 302.50it/s]\u001b[A\n",
            "  7% 502/7301 [00:01<00:21, 313.81it/s]\u001b[A\n",
            "  7% 536/7301 [00:02<00:21, 319.19it/s]\u001b[A\n",
            "  8% 570/7301 [00:02<00:20, 324.81it/s]\u001b[A\n",
            "  8% 603/7301 [00:02<00:21, 311.80it/s]\u001b[A\n",
            "  9% 635/7301 [00:02<00:21, 310.94it/s]\u001b[A\n",
            "  9% 670/7301 [00:02<00:20, 322.16it/s]\u001b[A\n",
            " 10% 705/7301 [00:02<00:20, 328.82it/s]\u001b[A\n",
            " 10% 739/7301 [00:02<00:19, 328.25it/s]\u001b[A\n",
            " 11% 773/7301 [00:02<00:19, 331.02it/s]\u001b[A\n",
            " 11% 808/7301 [00:02<00:19, 335.34it/s]\u001b[A\n",
            " 12% 842/7301 [00:03<00:22, 288.39it/s]\u001b[A\n",
            " 12% 876/7301 [00:03<00:21, 301.82it/s]\u001b[A\n",
            " 12% 909/7301 [00:03<00:20, 305.43it/s]\u001b[A\n",
            " 13% 941/7301 [00:03<00:21, 300.51it/s]\u001b[A\n",
            " 13% 976/7301 [00:03<00:20, 312.45it/s]\u001b[A\n",
            " 14% 1010/7301 [00:03<00:19, 319.89it/s]\u001b[A\n",
            " 14% 1043/7301 [00:03<00:20, 303.69it/s]\u001b[A\n",
            " 15% 1075/7301 [00:03<00:20, 306.09it/s]\u001b[A\n",
            " 15% 1106/7301 [00:03<00:20, 301.33it/s]\u001b[A\n",
            " 16% 1137/7301 [00:04<00:21, 292.37it/s]\u001b[A\n",
            " 16% 1170/7301 [00:04<00:20, 300.75it/s]\u001b[A\n",
            " 16% 1204/7301 [00:04<00:19, 310.46it/s]\u001b[A\n",
            " 17% 1236/7301 [00:04<00:19, 305.17it/s]\u001b[A\n",
            " 17% 1269/7301 [00:04<00:19, 309.97it/s]\u001b[A\n",
            " 18% 1303/7301 [00:04<00:19, 315.25it/s]\u001b[A\n",
            " 18% 1335/7301 [00:04<00:19, 310.19it/s]\u001b[A\n",
            " 19% 1370/7301 [00:04<00:18, 321.63it/s]\u001b[A\n",
            " 19% 1406/7301 [00:04<00:17, 331.34it/s]\u001b[A\n",
            " 20% 1442/7301 [00:04<00:17, 337.64it/s]\u001b[A\n",
            " 20% 1476/7301 [00:05<00:18, 315.54it/s]\u001b[A\n",
            " 21% 1510/7301 [00:05<00:18, 320.59it/s]\u001b[A\n",
            " 21% 1543/7301 [00:05<00:17, 320.17it/s]\u001b[A\n",
            " 22% 1576/7301 [00:05<00:18, 315.40it/s]\u001b[A\n",
            " 22% 1611/7301 [00:05<00:17, 323.34it/s]\u001b[A\n",
            " 23% 1644/7301 [00:05<00:19, 283.38it/s]\u001b[A\n",
            " 23% 1674/7301 [00:05<00:23, 244.45it/s]\u001b[A\n",
            " 23% 1700/7301 [00:05<00:25, 223.21it/s]\u001b[A\n",
            " 24% 1724/7301 [00:06<00:26, 209.54it/s]\u001b[A\n",
            " 24% 1746/7301 [00:06<00:27, 202.24it/s]\u001b[A\n",
            " 24% 1767/7301 [00:06<00:28, 196.55it/s]\u001b[A\n",
            " 24% 1787/7301 [00:06<00:29, 188.62it/s]\u001b[A\n",
            " 25% 1807/7301 [00:06<00:30, 182.07it/s]\u001b[A\n",
            " 25% 1826/7301 [00:06<00:30, 179.66it/s]\u001b[A\n",
            " 25% 1845/7301 [00:06<00:29, 181.91it/s]\u001b[A\n",
            " 26% 1864/7301 [00:06<00:31, 174.90it/s]\u001b[A\n",
            " 26% 1885/7301 [00:07<00:29, 183.38it/s]\u001b[A\n",
            " 26% 1904/7301 [00:07<00:29, 181.92it/s]\u001b[A\n",
            " 26% 1923/7301 [00:07<00:29, 183.56it/s]\u001b[A\n",
            " 27% 1942/7301 [00:07<00:28, 184.97it/s]\u001b[A\n",
            " 27% 1961/7301 [00:07<00:29, 180.16it/s]\u001b[A\n",
            " 27% 1980/7301 [00:07<00:30, 173.60it/s]\u001b[A\n",
            " 27% 1998/7301 [00:07<00:31, 169.04it/s]\u001b[A\n",
            " 28% 2016/7301 [00:07<00:31, 169.99it/s]\u001b[A\n",
            " 28% 2035/7301 [00:07<00:30, 175.02it/s]\u001b[A\n",
            " 28% 2054/7301 [00:07<00:29, 177.95it/s]\u001b[A\n",
            " 28% 2072/7301 [00:08<00:30, 168.80it/s]\u001b[A\n",
            " 29% 2090/7301 [00:08<00:30, 171.42it/s]\u001b[A\n",
            " 29% 2108/7301 [00:08<00:31, 167.45it/s]\u001b[A\n",
            " 29% 2125/7301 [00:08<00:31, 166.10it/s]\u001b[A\n",
            " 29% 2143/7301 [00:08<00:30, 169.20it/s]\u001b[A\n",
            " 30% 2160/7301 [00:08<00:30, 169.17it/s]\u001b[A\n",
            " 30% 2177/7301 [00:08<00:30, 165.76it/s]\u001b[A\n",
            " 30% 2196/7301 [00:08<00:30, 169.92it/s]\u001b[A\n",
            " 30% 2214/7301 [00:08<00:30, 168.70it/s]\u001b[A\n",
            " 31% 2231/7301 [00:09<00:30, 166.56it/s]\u001b[A\n",
            " 31% 2248/7301 [00:09<00:30, 166.53it/s]\u001b[A\n",
            " 31% 2266/7301 [00:09<00:29, 168.65it/s]\u001b[A\n",
            " 31% 2283/7301 [00:09<00:30, 167.17it/s]\u001b[A\n",
            " 32% 2300/7301 [00:09<00:30, 165.50it/s]\u001b[A\n",
            " 32% 2318/7301 [00:09<00:29, 166.67it/s]\u001b[A\n",
            " 32% 2335/7301 [00:09<00:30, 165.49it/s]\u001b[A\n",
            " 32% 2352/7301 [00:09<00:29, 164.98it/s]\u001b[A\n",
            " 32% 2369/7301 [00:09<00:30, 162.95it/s]\u001b[A\n",
            " 33% 2387/7301 [00:09<00:29, 167.37it/s]\u001b[A\n",
            " 33% 2406/7301 [00:10<00:28, 172.76it/s]\u001b[A\n",
            " 33% 2424/7301 [00:10<00:28, 174.04it/s]\u001b[A\n",
            " 33% 2443/7301 [00:10<00:27, 176.59it/s]\u001b[A\n",
            " 34% 2461/7301 [00:10<00:27, 176.88it/s]\u001b[A\n",
            " 34% 2479/7301 [00:10<00:27, 175.66it/s]\u001b[A\n",
            " 34% 2509/7301 [00:10<00:22, 210.76it/s]\u001b[A\n",
            " 35% 2538/7301 [00:10<00:20, 233.76it/s]\u001b[A\n",
            " 35% 2570/7301 [00:10<00:18, 258.30it/s]\u001b[A\n",
            " 36% 2599/7301 [00:10<00:17, 267.40it/s]\u001b[A\n",
            " 36% 2626/7301 [00:11<00:18, 253.44it/s]\u001b[A\n",
            " 36% 2658/7301 [00:11<00:17, 271.98it/s]\u001b[A\n",
            " 37% 2691/7301 [00:11<00:16, 287.91it/s]\u001b[A\n",
            " 37% 2724/7301 [00:11<00:15, 299.00it/s]\u001b[A\n",
            " 38% 2757/7301 [00:11<00:14, 304.39it/s]\u001b[A\n",
            " 38% 2788/7301 [00:11<00:15, 293.67it/s]\u001b[A\n",
            " 39% 2818/7301 [00:11<00:15, 289.88it/s]\u001b[A\n",
            " 39% 2848/7301 [00:11<00:16, 276.34it/s]\u001b[A\n",
            " 39% 2880/7301 [00:11<00:15, 287.83it/s]\u001b[A\n",
            " 40% 2913/7301 [00:11<00:14, 299.44it/s]\u001b[A\n",
            " 40% 2948/7301 [00:12<00:13, 313.69it/s]\u001b[A\n",
            " 41% 2981/7301 [00:12<00:13, 316.56it/s]\u001b[A\n",
            " 41% 3014/7301 [00:12<00:13, 319.21it/s]\u001b[A\n",
            " 42% 3047/7301 [00:12<00:13, 318.98it/s]\u001b[A\n",
            " 42% 3079/7301 [00:12<00:13, 317.94it/s]\u001b[A\n",
            " 43% 3114/7301 [00:12<00:12, 325.63it/s]\u001b[A\n",
            " 43% 3147/7301 [00:12<00:13, 316.19it/s]\u001b[A\n",
            " 44% 3179/7301 [00:12<00:14, 286.93it/s]\u001b[A\n",
            " 44% 3211/7301 [00:12<00:13, 293.79it/s]\u001b[A\n",
            " 44% 3246/7301 [00:13<00:13, 308.07it/s]\u001b[A\n",
            " 45% 3278/7301 [00:13<00:13, 295.33it/s]\u001b[A\n",
            " 45% 3311/7301 [00:13<00:13, 303.97it/s]\u001b[A\n",
            " 46% 3342/7301 [00:13<00:14, 276.74it/s]\u001b[A\n",
            " 46% 3371/7301 [00:13<00:15, 259.37it/s]\u001b[A\n",
            " 47% 3406/7301 [00:13<00:13, 281.10it/s]\u001b[A\n",
            " 47% 3435/7301 [00:13<00:13, 279.23it/s]\u001b[A\n",
            " 47% 3464/7301 [00:13<00:13, 275.89it/s]\u001b[A\n",
            " 48% 3496/7301 [00:13<00:13, 287.55it/s]\u001b[A\n",
            " 48% 3526/7301 [00:14<00:13, 285.40it/s]\u001b[A\n",
            " 49% 3558/7301 [00:14<00:12, 293.60it/s]\u001b[A\n",
            " 49% 3591/7301 [00:14<00:12, 302.85it/s]\u001b[A\n",
            " 50% 3622/7301 [00:14<00:12, 301.22it/s]\u001b[A\n",
            " 50% 3655/7301 [00:14<00:11, 307.58it/s]\u001b[A\n",
            " 51% 3688/7301 [00:14<00:11, 313.46it/s]\u001b[A\n",
            " 51% 3722/7301 [00:14<00:11, 320.52it/s]\u001b[A\n",
            " 51% 3755/7301 [00:14<00:12, 287.33it/s]\u001b[A\n",
            " 52% 3788/7301 [00:14<00:11, 297.57it/s]\u001b[A\n",
            " 52% 3821/7301 [00:14<00:11, 305.41it/s]\u001b[A\n",
            " 53% 3852/7301 [00:15<00:13, 250.38it/s]\u001b[A\n",
            " 53% 3883/7301 [00:15<00:12, 263.78it/s]\u001b[A\n",
            " 54% 3911/7301 [00:15<00:12, 262.91it/s]\u001b[A\n",
            " 54% 3942/7301 [00:15<00:12, 275.43it/s]\u001b[A\n",
            " 54% 3971/7301 [00:15<00:12, 263.63it/s]\u001b[A\n",
            " 55% 4005/7301 [00:15<00:11, 283.71it/s]\u001b[A\n",
            " 55% 4035/7301 [00:15<00:11, 279.66it/s]\u001b[A\n",
            " 56% 4068/7301 [00:15<00:11, 293.14it/s]\u001b[A\n",
            " 56% 4099/7301 [00:16<00:10, 296.63it/s]\u001b[A\n",
            " 57% 4129/7301 [00:16<00:10, 296.74it/s]\u001b[A\n",
            " 57% 4163/7301 [00:16<00:10, 307.54it/s]\u001b[A\n",
            " 57% 4194/7301 [00:16<00:10, 292.41it/s]\u001b[A\n",
            " 58% 4227/7301 [00:16<00:10, 302.17it/s]\u001b[A\n",
            " 58% 4262/7301 [00:16<00:09, 313.96it/s]\u001b[A\n",
            " 59% 4295/7301 [00:16<00:09, 318.51it/s]\u001b[A\n",
            " 59% 4327/7301 [00:16<00:09, 315.82it/s]\u001b[A\n",
            " 60% 4359/7301 [00:16<00:11, 249.45it/s]\u001b[A\n",
            " 60% 4393/7301 [00:17<00:10, 271.70it/s]\u001b[A\n",
            " 61% 4425/7301 [00:17<00:10, 283.68it/s]\u001b[A\n",
            " 61% 4459/7301 [00:17<00:09, 296.80it/s]\u001b[A\n",
            " 61% 4490/7301 [00:17<00:10, 278.31it/s]\u001b[A\n",
            " 62% 4525/7301 [00:17<00:09, 297.40it/s]\u001b[A\n",
            " 62% 4559/7301 [00:17<00:08, 307.42it/s]\u001b[A\n",
            " 63% 4591/7301 [00:17<00:08, 309.15it/s]\u001b[A\n",
            " 63% 4625/7301 [00:17<00:08, 317.79it/s]\u001b[A\n",
            " 64% 4658/7301 [00:17<00:08, 306.52it/s]\u001b[A\n",
            " 64% 4692/7301 [00:17<00:08, 315.89it/s]\u001b[A\n",
            " 65% 4725/7301 [00:18<00:08, 318.25it/s]\u001b[A\n",
            " 65% 4759/7301 [00:18<00:07, 324.20it/s]\u001b[A\n",
            " 66% 4792/7301 [00:18<00:07, 324.77it/s]\u001b[A\n",
            " 66% 4825/7301 [00:18<00:07, 321.32it/s]\u001b[A\n",
            " 67% 4858/7301 [00:18<00:07, 323.41it/s]\u001b[A\n",
            " 67% 4891/7301 [00:18<00:07, 313.26it/s]\u001b[A\n",
            " 67% 4923/7301 [00:18<00:07, 309.57it/s]\u001b[A\n",
            " 68% 4958/7301 [00:18<00:07, 316.74it/s]\u001b[A\n",
            " 68% 4990/7301 [00:18<00:07, 290.89it/s]\u001b[A\n",
            " 69% 5020/7301 [00:19<00:07, 291.89it/s]\u001b[A\n",
            " 69% 5052/7301 [00:19<00:07, 297.53it/s]\u001b[A\n",
            " 70% 5082/7301 [00:19<00:07, 292.37it/s]\u001b[A\n",
            " 70% 5116/7301 [00:19<00:07, 304.40it/s]\u001b[A\n",
            " 71% 5148/7301 [00:19<00:07, 307.41it/s]\u001b[A\n",
            " 71% 5183/7301 [00:19<00:06, 317.46it/s]\u001b[A\n",
            " 71% 5215/7301 [00:19<00:06, 317.29it/s]\u001b[A\n",
            " 72% 5247/7301 [00:19<00:06, 308.28it/s]\u001b[A\n",
            " 72% 5282/7301 [00:19<00:06, 318.99it/s]\u001b[A\n",
            " 73% 5314/7301 [00:19<00:06, 311.42it/s]\u001b[A\n",
            " 73% 5348/7301 [00:20<00:06, 319.51it/s]\u001b[A\n",
            " 74% 5383/7301 [00:20<00:05, 326.31it/s]\u001b[A\n",
            " 74% 5416/7301 [00:20<00:05, 326.97it/s]\u001b[A\n",
            " 75% 5450/7301 [00:20<00:05, 328.38it/s]\u001b[A\n",
            " 75% 5483/7301 [00:20<00:05, 324.35it/s]\u001b[A\n",
            " 76% 5516/7301 [00:20<00:07, 252.42it/s]\u001b[A\n",
            " 76% 5544/7301 [00:20<00:07, 229.59it/s]\u001b[A\n",
            " 76% 5569/7301 [00:21<00:08, 210.69it/s]\u001b[A\n",
            " 77% 5592/7301 [00:21<00:08, 202.26it/s]\u001b[A\n",
            " 77% 5614/7301 [00:21<00:08, 192.19it/s]\u001b[A\n",
            " 77% 5634/7301 [00:21<00:08, 185.46it/s]\u001b[A\n",
            " 77% 5653/7301 [00:21<00:09, 181.83it/s]\u001b[A\n",
            " 78% 5672/7301 [00:21<00:09, 177.33it/s]\u001b[A\n",
            " 78% 5690/7301 [00:21<00:09, 162.20it/s]\u001b[A\n",
            " 78% 5707/7301 [00:21<00:11, 144.63it/s]\u001b[A\n",
            " 78% 5722/7301 [00:22<00:11, 139.49it/s]\u001b[A\n",
            " 79% 5742/7301 [00:22<00:10, 153.42it/s]\u001b[A\n",
            " 79% 5759/7301 [00:22<00:09, 157.34it/s]\u001b[A\n",
            " 79% 5778/7301 [00:22<00:09, 164.42it/s]\u001b[A\n",
            " 79% 5797/7301 [00:22<00:08, 170.28it/s]\u001b[A\n",
            " 80% 5816/7301 [00:22<00:08, 173.24it/s]\u001b[A\n",
            " 80% 5835/7301 [00:22<00:08, 176.71it/s]\u001b[A\n",
            " 80% 5854/7301 [00:22<00:08, 177.57it/s]\u001b[A\n",
            " 80% 5873/7301 [00:22<00:07, 178.85it/s]\u001b[A\n",
            " 81% 5892/7301 [00:22<00:07, 181.29it/s]\u001b[A\n",
            " 81% 5911/7301 [00:23<00:07, 179.26it/s]\u001b[A\n",
            " 81% 5929/7301 [00:23<00:07, 174.67it/s]\u001b[A\n",
            " 81% 5947/7301 [00:23<00:07, 174.71it/s]\u001b[A\n",
            " 82% 5965/7301 [00:23<00:07, 171.97it/s]\u001b[A\n",
            " 82% 5983/7301 [00:23<00:07, 168.89it/s]\u001b[A\n",
            " 82% 6001/7301 [00:23<00:07, 171.11it/s]\u001b[A\n",
            " 82% 6019/7301 [00:23<00:07, 173.34it/s]\u001b[A\n",
            " 83% 6038/7301 [00:23<00:07, 176.43it/s]\u001b[A\n",
            " 83% 6056/7301 [00:23<00:07, 174.25it/s]\u001b[A\n",
            " 83% 6074/7301 [00:24<00:07, 174.70it/s]\u001b[A\n",
            " 83% 6092/7301 [00:24<00:06, 173.18it/s]\u001b[A\n",
            " 84% 6110/7301 [00:24<00:06, 174.80it/s]\u001b[A\n",
            " 84% 6129/7301 [00:24<00:06, 177.16it/s]\u001b[A\n",
            " 84% 6148/7301 [00:24<00:06, 178.05it/s]\u001b[A\n",
            " 84% 6166/7301 [00:24<00:06, 175.39it/s]\u001b[A\n",
            " 85% 6184/7301 [00:24<00:06, 166.36it/s]\u001b[A\n",
            " 85% 6202/7301 [00:24<00:06, 169.24it/s]\u001b[A\n",
            " 85% 6220/7301 [00:24<00:06, 166.60it/s]\u001b[A\n",
            " 85% 6237/7301 [00:24<00:06, 161.30it/s]\u001b[A\n",
            " 86% 6256/7301 [00:25<00:06, 167.40it/s]\u001b[A\n",
            " 86% 6273/7301 [00:25<00:06, 166.00it/s]\u001b[A\n",
            " 86% 6290/7301 [00:25<00:06, 163.00it/s]\u001b[A\n",
            " 86% 6308/7301 [00:25<00:05, 165.86it/s]\u001b[A\n",
            " 87% 6326/7301 [00:25<00:05, 168.66it/s]\u001b[A\n",
            " 87% 6346/7301 [00:25<00:05, 176.21it/s]\u001b[A\n",
            " 87% 6378/7301 [00:25<00:04, 214.87it/s]\u001b[A\n",
            " 88% 6408/7301 [00:25<00:03, 239.56it/s]\u001b[A\n",
            " 88% 6441/7301 [00:25<00:03, 265.86it/s]\u001b[A\n",
            " 89% 6475/7301 [00:26<00:02, 286.20it/s]\u001b[A\n",
            " 89% 6509/7301 [00:26<00:02, 300.86it/s]\u001b[A\n",
            " 90% 6540/7301 [00:26<00:02, 281.96it/s]\u001b[A\n",
            " 90% 6574/7301 [00:26<00:02, 296.08it/s]\u001b[A\n",
            " 91% 6611/7301 [00:26<00:02, 315.30it/s]\u001b[A\n",
            " 91% 6646/7301 [00:26<00:02, 323.11it/s]\u001b[A\n",
            " 92% 6681/7301 [00:26<00:01, 329.97it/s]\u001b[A\n",
            " 92% 6715/7301 [00:26<00:01, 332.22it/s]\u001b[A\n",
            " 92% 6749/7301 [00:26<00:01, 332.80it/s]\u001b[A\n",
            " 93% 6783/7301 [00:26<00:01, 328.91it/s]\u001b[A\n",
            " 93% 6818/7301 [00:27<00:01, 333.71it/s]\u001b[A\n",
            " 94% 6852/7301 [00:27<00:01, 329.68it/s]\u001b[A\n",
            " 94% 6886/7301 [00:27<00:01, 302.62it/s]\u001b[A\n",
            " 95% 6919/7301 [00:27<00:01, 309.03it/s]\u001b[A\n",
            " 95% 6953/7301 [00:27<00:01, 316.14it/s]\u001b[A\n",
            " 96% 6987/7301 [00:27<00:00, 322.17it/s]\u001b[A\n",
            " 96% 7020/7301 [00:27<00:00, 316.40it/s]\u001b[A\n",
            " 97% 7052/7301 [00:27<00:00, 315.95it/s]\u001b[A\n",
            " 97% 7084/7301 [00:27<00:00, 303.86it/s]\u001b[A\n",
            " 97% 7115/7301 [00:28<00:00, 300.18it/s]\u001b[A\n",
            " 98% 7146/7301 [00:28<00:00, 276.91it/s]\u001b[A\n",
            " 98% 7177/7301 [00:28<00:00, 281.52it/s]\u001b[A\n",
            " 99% 7206/7301 [00:28<00:00, 256.61it/s]\u001b[A\n",
            " 99% 7238/7301 [00:28<00:00, 269.43it/s]\u001b[A\n",
            "100% 7268/7301 [00:28<00:00, 276.45it/s]\u001b[A\n",
            "100% 7301/7301 [00:28<00:00, 253.98it/s]\n",
            "01/03/2025 17:51:13 - INFO - utils_qa - Saving predictions to /content/transformers/results/predictions/predict_predictions.json.\n",
            "01/03/2025 17:51:13 - INFO - utils_qa - Saving nbest_preds to /content/transformers/results/predictions/predict_nbest_predictions.json.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 715, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 683, in main\n",
            "    results = trainer.predict(predict_dataset, predict_examples)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 130, in predict\n",
            "    metrics = self.compute_metrics(predictions)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 634, in compute_metrics\n",
            "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/evaluate/module.py\", line 467, in compute\n",
            "    output = self._compute(**inputs, **compute_kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/squad.py\", line 110, in _compute\n",
            "    score = compute_score(dataset=dataset, predictions=pred_dict)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 67, in compute_score\n",
            "    exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 52, in metric_max_over_ground_truths\n",
            "    return max(scores_for_ground_truths)\n",
            "ValueError: max() arg is an empty sequence\n",
            "100% 484/484 [03:30<00:00,  2.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def get_ground_truth_format(path):\n",
        "  with open(path, 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "  res = {}\n",
        "  for i in range(len(json_data[\"data\"])):\n",
        "    data = json_data[\"data\"][i][\"paragraphs\"]\n",
        "    for qa in data:\n",
        "      for content in qa[\"qas\"]:\n",
        "        if content[\"is_impossible\"] == 0:\n",
        "          res[content[\"id\"]] = content[\"answers\"][-1][\"text\"]\n",
        "        else:\n",
        "          res[content[\"id\"]] = content[\"plausible_answers\"][-1][\"text\"]\n",
        "  with open('/content/ground_truth.json', 'w') as f:\n",
        "    json.dump(res, f)\n",
        "  return res"
      ],
      "metadata": {
        "id": "Sux5k9oSTmU5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ground_truth_format('/content/ViQuAD2.0/test/ground_truth_private_test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZYPsnW5VCa0",
        "outputId": "8a8a9245-85e7-4261-a450-40bb01a9551e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'uit_000001': 'Th√°i B√¨nh D∆∞∆°ng',\n",
              " 'uit_000002': 'trong thung l≈©ng Trung t√¢m',\n",
              " 'uit_000003': '411,000 km2 (160,000 mi2)',\n",
              " 'uit_000004': 'B·∫Øc Fork',\n",
              " 'uit_000005': 'California',\n",
              " 'uit_000006': 'ti·ªÉu bang Baja California',\n",
              " 'uit_000007': 'trong thung l≈©ng Trung t√¢m',\n",
              " 'uit_000008': '411,000 km2',\n",
              " 'uit_000009': 'ti·ªÉu bang Baja California',\n",
              " 'uit_000010': 's√°t hay g·∫ßn b·ªù bi·ªÉn Th√°i B√¨nh D∆∞∆°ng',\n",
              " 'uit_000011': 'd√£y n√∫i ƒë√° granit Sierra Nevada',\n",
              " 'uit_000012': 'v·ª´a l√† c·ª≠a s√¥ng quan tr·ªçng h·ªó tr·ª£ h·ªá sinh th√°i n∆∞·ªõc m·∫∑n v√† v·ª´a l√† ngu·ªìn n∆∞·ªõc ch·ªß y·∫øu c·ªßa ph·∫ßn l·ªõn d√¢n c∆∞ ti·ªÉu bang',\n",
              " 'uit_000013': 'd√£y n√∫i ƒë√° granit Sierra Nevada',\n",
              " 'uit_000014': 'v·ª´a l√† c·ª≠a s√¥ng quan tr·ªçng h·ªó tr·ª£ h·ªá sinh th√°i n∆∞·ªõc m·∫∑n v√† v·ª´a l√† ngu·ªìn n∆∞·ªõc ch·ªß y·∫øu c·ªßa ph·∫ßn l·ªõn d√¢n c∆∞ ti·ªÉu bang',\n",
              " 'uit_000015': 'Tehachapi',\n",
              " 'uit_000016': 'vi·ªác tr·ªìng tr·ªçt b·ªã t√†n ph√°',\n",
              " 'uit_000017': 'nhi·ªát ƒë·ªô th·∫•p g·∫ßn ƒëi·ªÉm ƒë√¥ng trong m√πa ƒë√¥ng',\n",
              " 'uit_000018': 'v√†i con s√¥ng ƒë√£ ƒë·ªß r·ªông v√† s√¢u ƒë·ªÉ cho v√†i th√†nh ph·ªë n·ªôi ƒë·ªãa (nh·∫•t l√† Stockton) ƒë∆∞·ª£c tr·ªü th√†nh h·∫£i c·∫£ng',\n",
              " 'uit_000019': 'h·ªó tr·ª£ h·ªá th·ªëng n∆∞·ªõc c·ªßa m·ªôt s·ªë th√†nh ph·ªë, nh∆∞ng ch·ªß y·∫øu cung c·∫•p cho vi·ªác t∆∞·ªõi ti√™u n√¥ng nghi·ªáp',\n",
              " 'uit_000020': '4,421 m√©t',\n",
              " 'uit_000021': '4,421 m√©t (14,505 feet)',\n",
              " 'uit_000022': 'chim bi·ªÉn',\n",
              " 'uit_000023': 'Clear',\n",
              " 'uit_000024': 'h·ªì Clear',\n",
              " 'uit_000025': 'c·ª±c nam',\n",
              " 'uit_000026': 'h·ªì Tahoe',\n",
              " 'uit_000027': 'c·ª±c nam',\n",
              " 'uit_000028': 'ph√≠a t√¢y b·∫Øc ti·ªÉu bang v√† tri·ªÅn ph√≠a t√¢y d√£y Sierra Nevada',\n",
              " 'uit_000029': 'kho·∫£ng 35%',\n",
              " 'uit_000030': 'ph√≠a t√¢y b·∫Øc ti·ªÉu bang v√† tri·ªÅn ph√≠a t√¢y d√£y Sierra Nevada',\n",
              " 'uit_000031': 'Alaska',\n",
              " 'uit_000032': 'd·ªçc theo nh·ªØng d√£y n√∫i California g·∫ßn b·ªù bi·ªÉn h∆°n, v√† c·∫£ nh·ªØng ƒë·ªìi th·∫•p d∆∞·ªõi ch√¢n d√£y Sierra Nevada',\n",
              " 'uit_000033': 'th√¥ng',\n",
              " 'uit_000034': 'Alaska',\n",
              " 'uit_000035': '35%',\n",
              " 'uit_000036': 'd·ªçc theo nh·ªØng d√£y n√∫i California g·∫ßn b·ªù bi·ªÉn h∆°n, v√† c·∫£ nh·ªØng ƒë·ªìi th·∫•p d∆∞·ªõi ch√¢n d√£y Sierra Nevada',\n",
              " 'uit_000037': 'bi·ªÉn Salton',\n",
              " 'uit_000038': 'r·∫•t cao',\n",
              " 'uit_000039': 'ƒë√¥ng nam',\n",
              " 'uit_000040': '25%',\n",
              " 'uit_000041': 'l√† thung l≈©ng Ch·∫øt, l√† n∆°i c√≥ Badwater Flat ‚Äì ƒëi·ªÉm th·∫•p nh·∫•t v√† n√≥ng nh·∫•t c·ªßa B·∫Øc M·ªπ',\n",
              " 'uit_000042': 'l√† n∆°i c√≥ Badwater Flat ‚Äì ƒëi·ªÉm th·∫•p nh·∫•t v√† n√≥ng nh·∫•t c·ªßa B·∫Øc M·ªπ',\n",
              " 'uit_000043': '√≠t h∆°n 322 km (200 d·∫∑m)',\n",
              " 'uit_000044': 'bi·ªÉn Salton',\n",
              " 'uit_000045': '25%',\n",
              " 'uit_000046': 'Vi·ªác bu√¥n b√°n, h√¥n nh√¢n kh√°c d√¢n t·ªôc, v√† li√™n minh qu√¢n s·ª±',\n",
              " 'uit_000047': 'Vi·ªác bu√¥n b√°n, h√¥n nh√¢n kh√°c d√¢n t·ªôc, v√† li√™n minh qu√¢n s·ª±',\n",
              " 'uit_000048': 'nh√≥m, b·ªô l·∫°c, ti·ªÉu b·ªô l·∫°c, v√† c√°c c·ªông ƒë·ªìng l·ªõn h∆°n tr√™n b·ªù bi·ªÅn d·ªìi d√†o t√†i nguy√™n',\n",
              " 'uit_000049': 'ƒëi sƒÉn th√∫ r·ª´ng v√† h√°i l∆∞·ª£m nh·ªØng qu·∫£ h·∫°ch, qu·∫£ ƒë·∫ßu, v√† qu·∫£ m·ªçng',\n",
              " 'uit_000050': 'm·ªôt trong nh·ªØng v√πng ƒëa d·∫°ng v·ªÅ vƒÉn h√≥a v√† ng√¥n ng·ªØ nh·∫•t ·ªü B·∫Øc M·ªπ th·ªùi th·ªï d√¢n',\n",
              " 'uit_000051': 'sƒÉn nh·ªØng con th√∫ bi·ªÉn, c√¢u c√° h·ªìi v√† thu nh·∫∑t t√¥m cua',\n",
              " 'uit_000052': 'm·ªôt trong nh·ªØng v√πng ƒëa d·∫°ng v·ªÅ vƒÉn h√≥a v√† ng√¥n ng·ªØ nh·∫•t ·ªü B·∫Øc M·ªπ th·ªùi th·ªï d√¢n',\n",
              " 'uit_000053': 'ƒëi sƒÉn th√∫ r·ª´ng v√† h√°i l∆∞·ª£m nh·ªØng qu·∫£ h·∫°ch, qu·∫£ ƒë·∫ßu, v√† qu·∫£ m·ªçng',\n",
              " 'uit_000054': 'sƒÉn th√∫ r·ª´ng v√† h√°i l∆∞·ª£m nh·ªØng qu·∫£ h·∫°ch, qu·∫£ ƒë·∫ßu, v√† qu·∫£ m·ªçng',\n",
              " 'uit_000055': 'theo h√≤n ƒë·∫£o l·∫°c vi√™n California trong Las sergas de Esplandi√°n (C√°c truy·ªán phi√™u l∆∞u c·ªßa Splandian)',\n",
              " 'uit_000056': 'California',\n",
              " 'uit_000057': 'v√πng t√¢y b·∫Øc c·ªßa ƒê·∫ø qu·ªëc T√¢y Ban Nha, t·ª©c l√† b√°n ƒë·∫£o Baja California (H·∫° California)',\n",
              " 'uit_000058': 'Alta California (Th∆∞·ª£ng California)',\n",
              " 'uit_000059': 'Alta California',\n",
              " 'uit_000060': 'nh·ªØng ranh gi·ªõi c·ªßa bi·ªÉn Cortez v√† b·ªù bi·ªÉn Th√°i B√¨nh D∆∞∆°ng ch∆∞a ƒë∆∞·ª£c th√°m hi·ªÉm ƒë·∫ßy ƒë·ªß',\n",
              " 'uit_000061': 'nh·ªØng ranh gi·ªõi c·ªßa bi·ªÉn Cortez v√† b·ªù bi·ªÉn Th√°i B√¨nh D∆∞∆°ng ch∆∞a ƒë∆∞·ª£c th√°m hi·ªÉm ƒë·∫ßy ƒë·ªß',\n",
              " 'uit_000062': 'Garci Rodr√≠guez de Montalvo',\n",
              " 'uit_000063': 'theo h√≤n ƒë·∫£o l·∫°c vi√™n California trong Las sergas de Esplandi√°n',\n",
              " 'uit_000064': '1821',\n",
              " 'uit_000065': 'cu·ªëi th·∫ø k·ª∑ 18',\n",
              " 'uit_000066': 'C√°c tr·∫°i r·∫•t l·ªõn nu√¥i b√≤',\n",
              " 'uit_000067': 'ng∆∞·ªùi th·ªï d√¢n',\n",
              " 'uit_000068': 'C√°c tr·∫°i r·∫•t l·ªõn nu√¥i b√≤',\n",
              " 'uit_000069': 'Mexico gi√†nh ƒë∆∞·ª£c ƒë·ªôc l·∫≠p trong cu·ªôc Chi·∫øn tranh ƒê·ªôc l·∫≠p Mexico',\n",
              " 'uit_000070': 'khi Mexico gi√†nh ƒë∆∞·ª£c ƒë·ªôc l·∫≠p trong cu·ªôc Chi·∫øn tranh ƒê·ªôc l·∫≠p Mexico (1810‚Äì1821)',\n",
              " 'uit_000071': 'C√°c th∆∞∆°ng gia v√† th·ª±c d√¢n b·∫Øt ƒë·∫ßu ƒë·∫øn t·ª´ Hoa K·ª≥',\n",
              " 'uit_000072': 'mu·ªói v√† b·ªç ch√©t',\n",
              " 'uit_000073': 'kh√¥ng c√≥ nhi·ªÅu ng∆∞·ªùi sinh s·ªëng',\n",
              " 'uit_000074': 'x√¢y d·ª±ng m·ªôt s·ªë ph√°o ƒë√†i (presidio)',\n",
              " 'uit_000075': 's·ªët v√†ng, s·ªët r√©t, v√† d·ªãch h·∫°ch g√¢y ra b·ªüi mu·ªói v√† b·ªç ch√©t',\n",
              " 'uit_000076': 'b√πng n·ªï c√°c b·ªánh s·ªët v√†ng, s·ªët r√©t, v√† d·ªãch h·∫°ch g√¢y ra b·ªüi mu·ªói v√† b·ªç ch√©t',\n",
              " 'uit_000077': 'do Sa ho√†ng kh√¥ng quan t√¢m',\n",
              " 'uit_000078': 'ph√≠a nam',\n",
              " 'uit_000079': 'Baja California v√† Baja California Sur',\n",
              " 'uit_000080': 'm·ªôt con g·∫•u v√†ng v√† m·ªôt ng√¥i sao',\n",
              " 'uit_000081': 'Baja California v√† Baja California Sur',\n",
              " 'uit_000082': 'Thi·∫øu t∆∞·ªõng John D. Sloat c·ªßa H·∫£i qu√¢n Hoa K·ª≥ ti·∫øn v√†o v·ªãnh San Francisco v√† tuy√™n b·ªë ch·ªß quy·ªÅn c·ªßa Hoa K·ª≥ ƒë·ªëi v·ªõi California',\n",
              " 'uit_000083': '1846‚Äì1848',\n",
              " 'uit_000084': 'Alta California',\n",
              " 'uit_000085': 'Thi·∫øu t∆∞·ªõng John D. Sloat c·ªßa H·∫£i qu√¢n Hoa K·ª≥ ti·∫øn v√†o v·ªãnh San Francisco v√† tuy√™n b·ªë ch·ªß quy·ªÅn c·ªßa Hoa K·ª≥ ƒë·ªëi v·ªõi California',\n",
              " 'uit_000086': 'Alta California',\n",
              " 'uit_000087': 'ƒë∆∞·ªùng xe l·ª≠a xuy√™n l·ª•c ƒë·ªãa ƒë·∫ßu ti√™n ƒë∆∞·ª£c ho√†n th√†nh',\n",
              " 'uit_000088': 'n·∫øu t∆∞·ªõi ƒë·∫•t v√†o nh·ªØng th√°ng h√® kh√¥ c·∫°n, ƒë·∫•t ƒë√≥ r·∫•t h·ª£p ƒë·ªÉ tr·ªìng c√¢y ƒÉn qu·∫£ v√† l√†m n√¥ng nghi·ªáp',\n",
              " 'uit_000089': 'ƒëi theo c√°c chuy·∫øn ƒë∆∞·ªùng bi·ªÉn d√†i ho·∫∑c ƒëi b·∫±ng xe ng·ª±a hay ƒëi b·ªô r·∫•t kh√≥ khƒÉn tr√™n nh·ªØng con ƒë∆∞·ªùng ƒë·∫•t',\n",
              " 'uit_000090': 'c√¢y ƒÉn qu·∫£ v√† l√†m n√¥ng nghi·ªáp n√≥i chung',\n",
              " 'uit_000091': 'cam qu√Ωt',\n",
              " 'uit_000092': 'vi·ªác ƒëi l·∫°i l·∫°i gi·ªØa mi·ªÅn T√¢y v√† c√°c trung t√¢m ·ªü mi·ªÅn ƒê√¥ng t·ªën th√¨ gi·ªù v√† nguy hi·ªÉm',\n",
              " 'uit_000093': 't·ªën th√¨ gi·ªù v√† nguy hi·ªÉm',\n",
              " 'uit_000094': 'C√°c lo·∫°i c√¢y gi·ªëng cam qu√Ωt',\n",
              " 'uit_000095': 'm·ªôt trong nh·ªØng ƒë·ªãa ƒëi·ªÉm c√≥ nhi·ªÅu lo·∫°i ng∆∞·ªùi nh·∫•t tr√™n th·∫ø gi·ªõi',\n",
              " 'uit_000096': 'California',\n",
              " 'uit_000097': 'California',\n",
              " 'uit_000098': 'ƒê∆∞·ªùng Lincoln v√† Xa l·ªô 66',\n",
              " 'uit_000099': 'ho√†n th√†nh nh·ªØng con ƒë∆∞·ªùng xuy√™n l·ª•c ƒë·ªãa l·ªõn',\n",
              " 'uit_000100': 's·ª± di tr√∫ ƒë·∫øn California tƒÉng nhanh',\n",
              " 'uit_000101': 'k·ªπ thu·∫≠t v√† vƒÉn h√≥a, v√† l√† trung t√¢m qu·ªëc t·∫ø v·ªÅ c√¥ng ty k·ªπ thu·∫≠t, ng√†nh c√¥ng nghi·ªáp ƒëi·ªán ·∫£nh v√† truy·ªÅn h√¨nh, c√¥ng nghi·ªáp √¢m nh·∫°c',\n",
              " 'uit_000102': 'g·∫ßn m·ªôt tri·ªáu',\n",
              " 'uit_000103': 'g·∫ßn m·ªôt tri·ªáu',\n",
              " 'uit_000104': 'California tr·ªü th√†nh m·ªôt trong nh·ªØng ƒë·ªãa ƒëi·ªÉm c√≥ nhi·ªÅu lo·∫°i ng∆∞·ªùi nh·∫•t tr√™n th·∫ø gi·ªõi',\n",
              " 'uit_000105': '751.419 ng∆∞·ªùi',\n",
              " 'uit_000106': 'kho·∫£ng 36.132.147 ng∆∞·ªùi',\n",
              " 'uit_000107': 'th·ª© 13',\n",
              " 'uit_000108': 'kho·∫£ng 36.132.147 ng∆∞·ªùi',\n",
              " 'uit_000109': '4 tri·ªáu d√¢n',\n",
              " 'uit_000110': '4 tri·ªáu',\n",
              " 'uit_000111': '6,7%',\n",
              " 'uit_000112': 'th·ª© 13',\n",
              " 'uit_000113': 'California',\n",
              " 'uit_000114': 'm√°y t√≠nh v√† c√¥ng ngh·ªá cao',\n",
              " 'uit_000115': '13%',\n",
              " 'uit_000116': 'California',\n",
              " 'uit_000117': 'th·ª© s√°u',\n",
              " 'uit_000118': 'th·ª© s√°u',\n",
              " 'uit_000119': 'v·ªÅ m√°y t√≠nh v√† c√¥ng ngh·ªá cao',\n",
              " 'uit_000120': 'n√¥ng nghi·ªáp, h√†ng kh√¥ng v≈© tr·ª•, gi·∫£i tr√≠, c√¥ng nghi·ªáp nh·∫π, v√† du l·ªãch',\n",
              " 'uit_000121': 'n√¥ng nghi·ªáp, h√†ng kh√¥ng v≈© tr·ª•, gi·∫£i tr√≠, c√¥ng nghi·ªáp nh·∫π, v√† du l·ªãch',\n",
              " 'uit_000122': 'Hollywood',\n",
              " 'uit_000123': 'Hollywood',\n",
              " 'uit_000124': 'nh·ªØng tr∆∞·ªùng trung h·ªçc c∆° s·ªü c√≥ l·ªõp t√πy ch·ªçn v·ªõi ch∆∞∆°ng tr√¨nh t·∫≠p trung v√†o c√°ch h·ªçc',\n",
              " 'uit_000125': 'kho·∫£ng 14‚Äì18 tu·ªï',\n",
              " 'uit_000126': '16 tu·ªïi',\n",
              " 'uit_000127': '6 tu·ªïi',\n",
              " 'uit_000128': 'c√°ch h·ªçc, l·ªãch s·ª≠, v√† x√£ h·ªôi',\n",
              " 'uit_000129': 'ngh·ªÅ nghi·ªáp, ng√¥n ng·ªØ, v√† khoa h·ªçc nh√¢n vƒÉn',\n",
              " 'uit_000130': 'kho·∫£ng 14‚Äì18 tu·ªïi',\n",
              " 'uit_000131': 'nh·ªØng l·ªõp t√πy ch·ªçn v·ªÅ ngh·ªÅ nghi·ªáp, ng√¥n ng·ªØ',\n",
              " 'uit_000132': 'nh·ªØng tr∆∞·ªùng trung h·ªçc c∆° s·ªü',\n",
              " 'uit_000133': 'B·ªô NƒÉng l∆∞·ª£ng Hoa K·ª≥',\n",
              " 'uit_000134': 'h·ªá th·ªëng Vi·ªán ƒê·∫°i h·ªçc California (UC)',\n",
              " 'uit_000135': 'nh·∫≠n 12,5% c·ªßa nh·ªØng h·ªçc sinh cao ƒëi·ªÉm nh·∫•t v√† th·ª±c hi·ªán nghi√™n c·ª©u sau ƒë·∫°i h·ªçc',\n",
              " 'uit_000136': 'sinh cao ƒëi·ªÉm nh·∫•t v√† th·ª±c hi·ªán nghi√™n c·ª©u sau ƒë·∫°i h·ªçc',\n",
              " 'uit_000137': 'm·ªôt trong nh·ªØng h·ªá th·ªëng vi·ªán ƒë·∫°i h·ªçc c√¥ng l·∫≠p h√†ng ƒë·∫ßu c·ªßa Hoa K·ª≥',\n",
              " 'uit_000138': 'sinh vi√™n sau ƒë·∫°i h·ªçc ng√†nh y',\n",
              " 'uit_000139': 'Ph√≤ng th√≠ nghi·ªám Qu·ªëc gia Lawrence t·∫°i Livermore, Ph√≤ng th√≠ nghi·ªám Qu·ªëc gia Lawrence t·∫°i Berkeley, v√† Ph√≤ng th√≠ nghi·ªám Qu·ªëc gia Los Alamos',\n",
              " 'uit_000140': 'nh·ªØng sinh vi√™n sau ƒë·∫°i h·ªçc ng√†nh y',\n",
              " 'uit_000141': 'ƒë∆∞·ª£c coi nh∆∞ m·ªôt trong nh·ªØng h·ªá th·ªëng vi·ªán ƒë·∫°i h·ªçc c√¥ng l·∫≠p h√†ng ƒë·∫ßu c·ªßa Hoa K·ª≥',\n",
              " 'uit_000142': 'h∆°n 400.000 sinh vi√™n',\n",
              " 'uit_000143': 'C√°n b·ªô Th∆∞ vi·ªán Ti·ªÉu bang Kevin Star',\n",
              " 'uit_000144': 'ph·∫ßn ba h·ªçc sinh trung h·ªçc ph·ªï th√¥ng cao ƒëi·ªÉm nh·∫•t',\n",
              " 'uit_000145': 'ƒë·∫°i h·ªçc',\n",
              " 'uit_000146': 'ƒë·∫°i h·ªçc',\n",
              " 'uit_000147': 'nh·∫≠n ph·∫ßn ba h·ªçc sinh trung h·ªçc ph·ªï th√¥ng cao ƒëi·ªÉm nh·∫•t',\n",
              " 'uit_000148': 'CSU-Long Beach, CSU-Fresno, San Diego State University, v√† San Jose State University',\n",
              " 'uit_000149': 'khoa h·ªçc ·ª©ng d·ª•ng',\n",
              " 'uit_000150': 'khoa h·ªçc ·ª©ng d·ª•ng',\n",
              " 'uit_000151': 'h∆°n 400.000',\n",
              " 'uit_000152': 'kho·∫£ng 4,6 t·ª∑ nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000153': 'tinh v√¢n M·∫∑t Tr·ªùi',\n",
              " 'uit_000154': 'kho·∫£ng 150 tri·ªáu kil√¥m√©t',\n",
              " 'uit_000155': 'Khi M·∫∑t Tr·ªùi ng√†y c√†ng ƒë·∫∑c l·∫°i, n√≥ n√≥ng l√™n, ph·∫£n ·ª©ng h·∫°t nh√¢n b√πng n·ªï v√† t·∫°o n√™n gi√≥ M·∫∑t Tr·ªùi',\n",
              " 'uit_000156': 'tinh v√¢n M·∫∑t Tr·ªùi',\n",
              " 'uit_000157': 'kho·∫£ng 4,6 t·ª∑ nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000158': 'l·ª±c h·∫•p d·∫´n v√† qu√°n t√≠nh',\n",
              " 'uit_000159': '150 tri·ªáu kil√¥m√©t',\n",
              " 'uit_000160': 'l·ª±c h·∫•p d·∫´n v√† qu√°n t√≠nh',\n",
              " 'uit_000161': 'tinh v√¢n M·∫∑t Tr·ªùi',\n",
              " 'uit_000162': 'Sao Ho·∫£',\n",
              " 'uit_000163': '23,5¬∞',\n",
              " 'uit_000164': 'Theia',\n",
              " 'uit_000165': 'kho·∫£ng 4.533 t·ª∑ nƒÉm (c√≥ l·∫Ω 0 gi·ªù 05 ph√∫t ƒë√™m theo gi·ªù c√°i ƒë·ªìng h·ªì c·ªßa ch√∫ng ta)',\n",
              " 'uit_000166': '23,5¬∞',\n",
              " 'uit_000167': '150 tri·ªáu km',\n",
              " 'uit_000168': 'Theia',\n",
              " 'uit_000169': 'Theia',\n",
              " 'uit_000170': 'Sao Ho·∫£',\n",
              " 'uit_000171': 'amoniac, m√™tan, h∆°i n∆∞·ªõc, cacbon ƒëi√¥x√≠t, v√† nit∆°, c≈©ng nh∆∞ m·ªôt l∆∞·ª£ng nh·ªè c√°c ch·∫•t kh√≠',\n",
              " 'uit_000172': 'H∆°i n∆∞·ªõc tho√°t ra t·ª´ l·ªõp v·ªè khi c√°c kh√≠ gas b·ªã n√∫i l·ª≠a phun l√™n',\n",
              " 'uit_000173': 'amoniac, m√™tan, h∆°i n∆∞·ªõc, cacbon ƒëi√¥x√≠t, v√† nit∆°, c≈©ng nh∆∞ m·ªôt l∆∞·ª£ng nh·ªè c√°c ch·∫•t kh√≠',\n",
              " 'uit_000174': 'l·ªõp ozone',\n",
              " 'uit_000175': 'Th√°i Vi·ªÖn C·ªï',\n",
              " 'uit_000176': 'gi√≥ m·∫∑t tr·ªùi v√† ch√≠nh nhi·ªát l∆∞·ª£ng c·ªßa Tr√°i ƒê·∫•t',\n",
              " 'uit_000177': 'nh·ªØng cu·ªôc va ch·∫°m c·ªßa sao bƒÉng',\n",
              " 'uit_000178': 'v≈© tr·ª•',\n",
              " 'uit_000179': 'm·ªôt ph√¢n t·ª≠ (hay th·∫≠m ch√≠ l√† m·ªôt th·ª© g√¨ kh√°c) ƒë√£ c√≥ kh·∫£ nƒÉng t·ª± ph√¢n chia th√†nh c√°c b·∫£n sao c·ªßa ch√≠nh n√≥',\n",
              " 'uit_000180': 'c√°c d√≤ng d√µi sau ƒë√≥ c√≥ th·ªÉ khai th√°c c√°c nguy√™n li·ªáu kh√°c, hay c√≥ l·∫Ω l√† h·ªçc c√°ch ti·∫øn tri·ªÉn c·ªßa c√°c ki·ªÉu d√≤ng d√µi kh√°c, v√† tr·ªü n√™n ƒë√¥ng ƒë·∫£o h∆°n',\n",
              " 'uit_000181': 't·ª´ v≈© tr·ª•',\n",
              " 'uit_000182': 'kho·∫£ng 4 t·ª∑ nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000183': 'th√∫c ƒë·∫©y c√°c ph·∫£n ·ª©ng h√≥a h·ªçc t·∫°o th√†nh b·∫£n sao c·ªßa ch√≠nh n√≥',\n",
              " 'uit_000184': 'may m·∫Øn',\n",
              " 'uit_000185': 'DNA',\n",
              " 'uit_000186': 'DNA',\n",
              " 'uit_000187': 'protein hi·ªán ƒë·∫°i c·ªßa c√°c acid nucleic, phospholipid, tinh th·ªÉ, hay th·∫≠m ch√≠ c√°c h·ªá l∆∞·ª£ng t·ª≠',\n",
              " 'uit_000188': 'c√≥ t√≠nh ch·∫•t k·ª≥ d·ªã th√∫c ƒë·∫©y c√°c ph·∫£n ·ª©ng h√≥a h·ªçc t·∫°o th√†nh b·∫£n sao c·ªßa ch√≠nh n√≥, v√† ti·∫øn tr√¨nh ph√°t tri·ªÉn th·ª±c s·ª± b·∫Øt ƒë·∫ßu',\n",
              " 'uit_000189': 'ph√¢n t·ª≠ phospholipid',\n",
              " 'uit_000190': 'enzym',\n",
              " 'uit_000191': 'C√°c protein',\n",
              " 'uit_000192': 'RNA',\n",
              " 'uit_000193': 'trao ƒë·ªïi th√¥ng tin v√† t·ªïng h·ª£p protein, v√† c√°c enzyme l√†m x√∫c t√°c cho ph·∫£n ·ª©ng',\n",
              " 'uit_000194': 'ƒëi√¥x√≠t cacbon v√† n∆∞·ªõc',\n",
              " 'uit_000195': 'th·ª© ba',\n",
              " 'uit_000196': 'h·∫•p th·ª• √°nh s√°ng m·∫∑t tr·ªùi nh∆∞ m·ªôt ngu·ªìn nƒÉng l∆∞·ª£ng',\n",
              " 'uit_000197': 'bi·∫øn ƒë·ªïi l·ªõn',\n",
              " 'uit_000198': '√îxy l√† ch·∫•t ƒë·ªôc',\n",
              " 'uit_000199': 'ƒëi√¥x√≠t cacbon v√† n∆∞·ªõc',\n",
              " 'uit_000200': '√îxy l√† ch·∫•t ƒë·ªôc ƒë·ªëi v·ªõi nhi·ªÅu d·∫°ng s·ªëng v√†o th·ªùi k·ª≥ n√†y',\n",
              " 'uit_000201': 'kh√≠ √¥xy',\n",
              " 'uit_000202': 'th·ªùi k·ª≥ kh√≠ quy·ªÉn th·ª© ba',\n",
              " 'uit_000203': 'Archarea v√† Eukarya',\n",
              " 'uit_000204': 'chloroplast',\n",
              " 'uit_000205': 't·∫ø b√†o nh·ªè t√¨m c√°ch k√Ω sinh tr√™n t·∫ø b√†o l·ªõn',\n",
              " 'uit_000206': 'mitochondria',\n",
              " 'uit_000207': 'Ph√©p ph√¢n lo·∫°i hi·ªán ƒë·∫°i',\n",
              " 'uit_000208': 'Archarea v√† Eukarya',\n",
              " 'uit_000209': 'V·ª±c Bacteria',\n",
              " 'uit_000210': 's·ª± ph√¢n chia gi·ªØa m·ªôt t·∫≠p ƒëo√†n v·ªõi c√°c t·∫ø b√†o',\n",
              " 'uit_000211': 't·∫•t c·∫£ c√°c t·∫ø b√†o ƒë·ªÅu mang t√≠nh to√†n nƒÉng (totipotent)',\n",
              " 'uit_000212': 'kho·∫£ng 750 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000213': 'Kho·∫£ng 1.1 t·ª∑ nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000214': 't·∫£o l·ª•c',\n",
              " 'uit_000215': 'eukaryotes',\n",
              " 'uit_000216': 'c√≥ l·∫Ω l√† t·∫£o l·ª•c',\n",
              " 'uit_000217': 'kho·∫£ng 750 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000218': 'Kho·∫£ng 1.1 t·ª∑ nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000219': 'kho·∫£ng 530 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000220': 'kho·∫£ng 600 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000221': 'c√°c sinh v·∫≠t ƒë∆°n b√†o ƒëi l√™n m·∫∑t ƒë·∫•t s·∫Ω c√≥ c∆° h·ªôi s·ªëng s√≥t cao h∆°n, v√† c√°c sinh v·∫≠t ch∆∞a c√≥ nh√¢n ƒë√£ b·∫Øt ƒë·∫ßu sinh s√¥i v√† tr·ªü n√™n th√≠ch ·ª©ng t·ªët h∆°n v·ªõi m√¥i tr∆∞·ªùng s·ªëng b√™n ngo√†i ƒë·∫°i d∆∞∆°ng',\n",
              " 'uit_000222': 'C√°',\n",
              " 'uit_000223': 'C√°, nh·ªØng ƒë·ªông v·∫≠t c√≥ x∆∞∆°ng s·ªëng',\n",
              " 'uit_000224': '50 tri·ªáu nƒÉm',\n",
              " 'uit_000225': 'kho·∫£ng 600 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000226': '50 tri·ªáu nƒÉm',\n",
              " 'uit_000227': 'ch√¢n ƒë·ªët',\n",
              " 'uit_000228': '530 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000229': 'c√°c v√¢y',\n",
              " 'uit_000230': '530 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000231': '1 t·ª∑ nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000232': 'Kho·∫£ng 365 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000233': 's·ª± l·∫°nh ƒëi to√†n c·∫ßu',\n",
              " 'uit_000234': 'Pangea',\n",
              " 'uit_000235': 'Jura',\n",
              " 'uit_000236': 'Pangea',\n",
              " 'uit_000237': '10 kil√¥m√©t',\n",
              " 'uit_000238': '340 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000239': '340 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000240': 'kho·∫£ng th·ªùi gian ph√¢n t√°ch gi·ªØa k·ª∑ Permi v√† Trias',\n",
              " 'uit_000241': 'Pangaea',\n",
              " 'uit_000242': 'hai tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000243': 'lo√†i tinh tinh',\n",
              " 'uit_000244': 'kh·∫£ nƒÉng ƒë·ª©ng th·∫≥ng',\n",
              " 'uit_000245': 'ch√¢u Phi',\n",
              " 'uit_000246': 'kh·∫£ nƒÉng ƒë·ª©ng th·∫≥ng',\n",
              " 'uit_000247': 'Ng∆∞·ªùi Cro-Magnon',\n",
              " 'uit_000248': 'Ng∆∞·ªùi Cro-Magnon',\n",
              " 'uit_000249': 'ch√¢u Phi',\n",
              " 'uit_000250': 'hai tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_000251': 'th·ªÉ hi·ªán ƒë·ª©c tin t√¥n gi√°o',\n",
              " 'uit_000252': 'Th·ªùi ƒëi·ªÉm n√†o ƒë√≥ trong kho·∫£ng 8500 t·ªõi 7000 tr∆∞·ªõc C√¥ng Nguy√™n',\n",
              " 'uit_000253': 'sƒÉn b·∫Øt - h√°i l∆∞·ª£m',\n",
              " 'uit_000254': 'L∆∞·ª°ng H√†',\n",
              " 'uit_000255': 't·∫ßng l·ªõp cai tr·ªã v√† th·∫ßy c√∫ng xu·∫•t hi·ªán, ti·∫øp ƒë√≥ l√† s·ª± ph√¢n c√¥ng lao ƒë·ªông',\n",
              " 'uit_000256': 'Sumer v√πng Trung ƒê√¥ng',\n",
              " 'uit_000257': 'm·ªôt h√¨nh th·ª©c t√°i truy·ªÅn t·∫£i th√¥ng tin m·ªõi',\n",
              " 'uit_000258': 'sƒÉn b·∫Øt - h√°i l∆∞·ª£m',\n",
              " 'uit_000259': 'nh·ªØng th∆∞ kh·ªë v√† c√°c th∆∞ vi·ªán tr·ªü th√†nh n∆°i l∆∞u gi·ªØ nh·ªØng hi·ªÉu bi·∫øt c·ªßa nh√¢n lo·∫°i c≈©ng nh∆∞ tƒÉng c∆∞·ªùng s·ª± chuy·ªÉn giao vƒÉn h√≥a v√† th√¥ng tin',\n",
              " 'uit_000260': 'S·ª± ph√°t minh ra ch·ªØ vi·∫øt',\n",
              " 'uit_000261': 't√≠nh t√≤ m√≤ v√† gi√°o d·ª•c',\n",
              " 'uit_000262': 't√≠nh t√≤ m√≤ v√† gi√°o d·ª•c khi·∫øn m·ªçi ng∆∞·ªùi nhanh ch√≥ng c√≥ ƒë∆∞·ª£c s·ª± hi·ªÉu bi·∫øt v√† kh√¥n ngoan',\n",
              " 'uit_000263': 'ph√°t tri·ªÉn t·ªõi c·ª±c ƒëi·ªÉm',\n",
              " 'uit_000264': 'cho ph√©p c√°c x√£ h·ªôi ph·ª©c t·∫°p h∆°n xu·∫•t hi·ªán',\n",
              " 'uit_000265': 'ph√°t tri·ªÉn t·ªõi c·ª±c ƒëi·ªÉm',\n",
              " 'uit_000266': 'H·ªôi qu·ªëc li√™n',\n",
              " 'uit_000267': 'T·ª´ nƒÉm 1914 ƒë·∫øn 1918',\n",
              " 'uit_000268': 'H·ªôi qu·ªëc li√™n',\n",
              " 'uit_000269': 'v·∫≠n t·∫£i v√† th√¥ng tin ph√°t tri·ªÉn',\n",
              " 'uit_000270': 'Italia',\n",
              " 'uit_000271': 'Li√™n minh ch√¢u √Çu',\n",
              " 'uit_000272': 'kho·∫£ng nƒÉm 1500',\n",
              " 'uit_000273': 'T·ª´ nƒÉm 1914 ƒë·∫øn 1918',\n",
              " 'uit_000274': 'Italia',\n",
              " 'uit_000275': 's·ª± tuy·ªát ch·ªßng h√†ng lo·∫°t v√† s·ª± ·∫•m l√™n to√†n c·∫ßu',\n",
              " 'uit_000276': 'S·ª± thay ƒë·ªïi ti·∫øp t·ª•c di·ªÖn ra v·ªõi t·ªëc ƒë·ªô ng√†y c√†ng nhanh trong ph·∫ßn ngh√¨n gi√¢y cu·ªëi c√πng c·ªßa 24 gi·ªù t∆∞·ªüng t∆∞·ª£ng',\n",
              " 'uit_000277': 's·ª± tuy·ªát ch·ªßng h√†ng lo·∫°t v√† s·ª± ·∫•m l√™n to√†n c·∫ßu',\n",
              " 'uit_000278': 'Li√™n bang x√¥ vi·∫øt',\n",
              " 'uit_000279': 'Li√™n bang x√¥ vi·∫øt',\n",
              " 'uit_000280': 'nƒÉm 2000',\n",
              " 'uit_000281': 'Li√™n bang x√¥ vi·∫øt',\n",
              " 'uit_000282': 'Yuri Gagarin',\n",
              " 'uit_000283': 'NƒÉm',\n",
              " 'uit_000284': 'nƒÉm 2000',\n",
              " 'uit_000285': 'hi·ªán di·ªán th∆∞·ªùng xuy√™n tr√™n v≈© tr·ª• hay th·∫≠m ch√≠ chi·∫øm l√†m thu·ªôc ƒë·ªãa nh·ªØng th·∫ø gi·ªõi xa x√¥i',\n",
              " 'uit_000286': 'hi·ªán di·ªán th∆∞·ªùng xuy√™n tr√™n v≈© tr·ª• hay th·∫≠m ch√≠ chi·∫øm l√†m thu·ªôc ƒë·ªãa nh·ªØng th·∫ø gi·ªõi xa x√¥i',\n",
              " 'uit_000287': 'Yuri Gagarin',\n",
              " 'uit_000513': 'b·ª©c x·∫° Hawking',\n",
              " 'uit_000514': 's·ª± th·ªëng nh·∫•t gi·ªØa thuy·∫øt t∆∞∆°ng ƒë·ªëi t·ªïng qu√°t v√† c∆° h·ªçc l∆∞·ª£ng t·ª≠',\n",
              " 'uit_000515': 'l√† m·ªôt nh√† v·∫≠t l√Ω l√Ω thuy·∫øt, v≈© tr·ª• h·ªçc, t√°c gi·∫£ vi·∫øt s√°ch khoa h·ªçc th∆∞·ªùng th·ª©c ng∆∞·ªùi Anh, nguy√™n Gi√°m ƒë·ªëc Nghi√™n c·ª©u t·∫°i Trung t√¢m V≈© tr·ª• h·ªçc l√Ω thuy·∫øt thu·ªôc ƒê·∫°i h·ªçc Cambridge',\n",
              " 'uit_000516': 'Hawking l√† ng∆∞·ªùi ƒë·∫ßu ti√™n kh·ªüi ƒë·∫ßu m·ªôt n·ªÅn v≈© tr·ª• h·ªçc d·ª±a tr√™n s·ª± th·ªëng nh·∫•t gi·ªØa thuy·∫øt t∆∞∆°ng ƒë·ªëi t·ªïng qu√°t v√† c∆° h·ªçc l∆∞·ª£ng t·ª≠',\n",
              " 'uit_000517': 'Roger Penrose',\n",
              " 'uit_000518': 'l√† m·ªôt nh√† v·∫≠t l√Ω l√Ω thuy·∫øt, v≈© tr·ª• h·ªçc, t√°c gi·∫£ vi·∫øt s√°ch khoa h·ªçc th∆∞·ªùng th·ª©c ng∆∞·ªùi Anh, nguy√™n Gi√°m ƒë·ªëc Nghi√™n c·ª©u t·∫°i Trung t√¢m V≈© tr·ª• h·ªçc l√Ω thuy·∫øt thu·ªôc ƒê·∫°i h·ªçc Cambridge',\n",
              " 'uit_000519': 'b·ª©c x·∫° Hawking',\n",
              " 'uit_000520': '21 tu·ªïi',\n",
              " 'uit_000521': 'cƒÉn b·ªánh ALS s·∫Ω khi·∫øn √¥ng ch·ªâ s·ªëng th√™m ƒë∆∞·ª£c v√†i nƒÉm',\n",
              " 'uit_000522': 'kh·∫£ nƒÉng n√≥i chuy·ªán',\n",
              " 'uit_000523': 'ch·ª©ng b·ªánh x∆° c·ª©ng teo c∆°',\n",
              " 'uit_000524': '√¥ng ch·ªâ s·ªëng th√™m ƒë∆∞·ª£c v√†i nƒÉm',\n",
              " 'uit_000525': 'giao ti·∫øp th√¥ng qua m·ªôt thi·∫øt b·ªã t·∫°o gi·ªçng n√≥i ƒë∆∞·ª£c g·∫Øn tr·ª±c ti·∫øp v√†o chi·∫øc xe lƒÉn c·ªßa √¥ng',\n",
              " 'uit_000526': 'th√¥ng qua m·ªôt thi·∫øt b·ªã t·∫°o gi·ªçng n√≥i ƒë∆∞·ª£c g·∫Øn tr·ª±c ti·∫øp v√†o chi·∫øc xe lƒÉn c·ªßa √¥ng',\n",
              " 'uit_000527': 'x∆° c·ª©ng teo c∆°',\n",
              " 'uit_000528': 'th·∫ßy d·∫°y to√°n n·ªïi ti·∫øng Dikran Tahta',\n",
              " 'uit_000529': 'Hawking duy tr√¨ ƒë∆∞·ª£c m·ªôt nh√≥m b·∫°n th√¢n m√† √¥ng th∆∞·ªùng tham gia ch∆°i b√†i, l√†m ph√°o hoa, c√°c m√¥ h√¨nh phi c∆° v√† t√†u thuy·ªÅn, c≈©ng nh∆∞ th·∫£o lu·∫≠n v·ªÅ C∆° ƒë·ªëc gi√°o v√† nƒÉng l·ª±c ngo·∫°i c·∫£m. T·ª´ 1958, v·ªõi s·ª± gi√∫p ƒë·ª° c·ªßa th·∫ßy d·∫°y to√°n n·ªïi ti·∫øng Dikran Tahta, h·ªç x√¢y d·ª±ng m·ªôt m√°y t√≠nh v·ªõi c√°c linh ki·ªán l·∫•y t·ª´ ƒë·ªìng h·ªì, m·ªôt m√°y t·ªïng ƒë√†i ƒëi·ªán tho·∫°i c≈© v√† c√°c thi·∫øt b·ªã t√°i ch·∫ø kh√°c',\n",
              " 'uit_000530': 'th·∫ßy d·∫°y to√°n n·ªïi ti·∫øng Dikran Tahta',\n",
              " 'uit_000531': 'r·∫•t ƒë·ªÅ cao gi√° tr·ªã c·ªßa vi·ªác h·ªçc h√†nh',\n",
              " 'uit_000532': 'Hawking duy tr√¨ ƒë∆∞·ª£c m·ªôt nh√≥m b·∫°n th√¢n m√† √¥ng th∆∞·ªùng tham gia ch∆°i b√†i, l√†m ph√°o hoa, c√°c m√¥ h√¨nh phi c∆° v√† t√†u thuy·ªÅn, c≈©ng nh∆∞ th·∫£o lu·∫≠n v·ªÅ C∆° ƒë·ªëc gi√°o v√† nƒÉng l·ª±c ngo·∫°i c·∫£m',\n",
              " 'uit_000533': 'b·ªã ·ªëm v√†o ƒë√∫ng ng√†y thi l·∫•y h·ªçc b·ªïng',\n",
              " 'uit_000534': 'lo ng·∫°i r·∫±ng kh√¥ng c√≥ m·∫•y vi·ªác l√†m cho m·ªôt sinh vi√™n ng√†nh to√°n ra tr∆∞·ªùng',\n",
              " 'uit_000535': 'Einstein',\n",
              " 'uit_000536': 'm·∫∑c d√π ƒëi·ªÉm s·ªë kh√¥ng t·ªët nh∆∞ng c·∫£ gi√°o vi√™n v√† b·∫°n b√® ƒë·ªÅu th·∫•y ƒë∆∞·ª£c t·ªë ch·∫•t thi√™n t√†i c·ªßa √¥ng',\n",
              " 'uit_000537': 'c√°c m√¥n khoa h·ªçc t·ª± nhi√™n',\n",
              " 'uit_000538': 'v√¨ lo ng·∫°i r·∫±ng kh√¥ng c√≥ m·∫•y vi·ªác l√†m cho m·ªôt sinh vi√™n ng√†nh to√°n ra tr∆∞·ªùng',\n",
              " 'uit_000539': 's·ª± tr·ªÖ n·∫£i c·ªßa √¥ng',\n",
              " 'uit_000540': 'l√°i ƒë·ªôi ƒëua theo nh·ªØng h∆∞·ªõng nguy hi·ªÉm th∆∞·ªùng d·∫´n t·ªõi thuy·ªÅn b·ªã h∆∞ h·∫°i',\n",
              " 'uit_000541': 'ph·∫•n ƒë·∫•u v√† tr·ªü th√†nh m·ªôt sinh vi√™n ƒë∆∞·ª£c qu√Ω m·∫øn, ho·∫°t b√°t, d√≠ d·ªèm, h·ª©ng th√∫ v·ªõi nh·∫°c c·ªï ƒëi·ªÉn v√† ti·ªÉu thuy·∫øt vi·ªÖn t∆∞·ªüng',\n",
              " 'uit_000542': 'ƒê·ªëi v·ªõi c·∫≠u ta ch·ªâ c·∫ßn bi·∫øt ƒëi·ªÅu g√¨ ƒë√≥ c√≥ th·ªÉ th·ª±c hi·ªán, v√† c·∫≠u c√≥ th·ªÉ l√†m n√≥ m√† kh√¥ng c·∫ßn ph·∫£i ng√≥ xem nh·ªØng ng∆∞·ªùi kh√°c ƒë√£ l√†m th·∫ø n√†o',\n",
              " 'uit_000543': 'Hawking ph·∫•n ƒë·∫•u v√† tr·ªü th√†nh m·ªôt sinh vi√™n ƒë∆∞·ª£c qu√Ω m·∫øn, ho·∫°t b√°t, d√≠ d·ªèm, h·ª©ng th√∫ v·ªõi nh·∫°c c·ªï ƒëi·ªÉn v√† ti·ªÉu thuy·∫øt vi·ªÖn t∆∞·ªüng',\n",
              " 'uit_000544': 't√°o b·∫°o',\n",
              " 'uit_000545': '√¥ng √≠t tu·ªïi h∆°n ph·∫ßn l·ªõn sinh vi√™n',\n",
              " 'uit_000546': 'l√°i ƒë·ªôi ƒëua theo nh·ªØng h∆∞·ªõng nguy hi·ªÉm th∆∞·ªùng d·∫´n t·ªõi thuy·ªÅn b·ªã h∆∞ h·∫°i',\n",
              " 'uit_000547': 'ƒê·ªëi v·ªõi c·∫≠u ta ch·ªâ c·∫ßn bi·∫øt ƒëi·ªÅu g√¨ ƒë√≥ c√≥ th·ªÉ th·ª±c hi·ªán, v√† c·∫≠u c√≥ th·ªÉ l√†m n√≥ m√† kh√¥ng c·∫ßn ph·∫£i ng√≥ xem nh·ªØng ng∆∞·ªùi kh√°c ƒë√£ l√†m th·∫ø n√†o',\n",
              " 'uit_000548': '1000 gi·ªù',\n",
              " 'uit_000549': 'k·∫øt qu·∫£ n·∫±m ·ªü ƒë√∫ng ƒëi·ªÉm s·ªë ranh gi·ªõi gi·ªØa h·∫°ng nh·∫•t v√† h·∫°ng nh√¨',\n",
              " 'uit_000550': 'ƒë·ªÉ ph√¢n h·∫°ng',\n",
              " 'uit_000551': 'ph·∫£i c√≥ m·ªôt b·∫±ng danh d·ª± h·∫°ng nh·∫•t',\n",
              " 'uit_000552': 'k·∫øt qu·∫£ n·∫±m ·ªü ƒë√∫ng ƒëi·ªÉm s·ªë ranh gi·ªõi gi·ªØa h·∫°ng nh·∫•t v√† h·∫°ng nh√¨',\n",
              " 'uit_000553': 'ch·ªâ tr·∫£ l·ªùi nh·ªØng c√¢u h·ªèi v·∫≠t l√Ω l√Ω thuy·∫øt v√† b·ªè qua nh·ªØng c√¢u ƒë√≤i h·ªèi ki·∫øn th·ª©c th·ª±c t·∫ø',\n",
              " 'uit_000554': 'ph√¢n h·∫°ng',\n",
              " 'uit_000555': 'N·∫øu c√°c v·ªã trao cho t√¥i h·∫°ng Nh·∫•t, t√¥i s·∫Ω t·ªõi Cambridge. N·∫øu t√¥i nh·∫≠n h·∫°ng Nh√¨, t√¥i s·∫Ω ·ªü l·∫°i Oxford, v√¨ v·∫≠y t√¥i hi v·ªçng c√°c v·ªã cho t√¥i h·∫°ng Nh·∫•t',\n",
              " 'uit_000556': 'b·ªã xem l√† m·ªôt sinh vi√™n l∆∞·ªùi nh√°c v√† kh√≥ t√≠nh',\n",
              " 'uit_000557': 'b·∫Øt ƒë·∫ßu v√†o h·ªçc b·∫≠c tr√™n ƒë·∫°i h·ªçc t·∫°i Trinity Hall (ƒê·∫°i h·ªçc Cambridge)',\n",
              " 'uit_000558': 'th√°ng 10 nƒÉm 1962',\n",
              " 'uit_000559': 'Hawking r∆°i v√†o tr·∫ßm u·∫•t; m·∫∑c d√π c√°c b√°c sƒ© khuy√™n √¥ng ti·∫øp t·ª•c h·ªçc h√†nh, √¥ng c·∫£m th·∫•y ch·∫≥ng c√≤n m·∫•y √Ω nghƒ©a',\n",
              " 'uit_000560': 'r∆°i v√†o tr·∫ßm u·∫•t',\n",
              " 'uit_000561': 'Jane Wilde, b·∫°n c·ªßa em g√°i √¥ng',\n",
              " 'uit_000562': 'Jane Wilde',\n",
              " 'uit_000563': 'ng∆∞·ª£c ng·∫°o',\n",
              " 'uit_000564': 'C√°c k·ª≥ d·ªã v√† H√¨nh h·ªçc c·ªßa Kh√¥ng-Th·ªùi gian',\n",
              " 'uit_000565': 'gi·∫£i d√†nh cho nghi√™n c·ª©u to√°n h·ªçc xu·∫•t s·∫Øc nh·∫•t h√†ng nƒÉm c·ªßa Cambridge',\n",
              " 'uit_000566': 'C√°c k·ª≥ d·ªã v√† H√¨nh h·ªçc c·ªßa Kh√¥ng-Th·ªùi gian',\n",
              " 'uit_000567': 'c√°c l√Ω thuy·∫øt ƒëang th·ªãnh h√†nh li√™n quan t·ªõi s·ª± khai sinh v≈© tr·ª•: thuy·∫øt V·ª• N·ªï L·ªõn v√† thuy·∫øt v≈© tr·ª• tƒ©nh t·∫°i',\n",
              " 'uit_000568': '√°p d·ª•ng √Ω t∆∞·ªüng t∆∞∆°ng t·ª± cho to√†n th·ªÉ v≈© tr·ª•',\n",
              " 'uit_000569': 'c√°c l√Ω thuy·∫øt ƒëang th·ªãnh h√†nh li√™n quan t·ªõi s·ª± khai sinh v≈© tr·ª•',\n",
              " 'uit_000570': 'ƒë·ªãnh l√Ω v·ªÅ k√¨ d·ªã kh√¥ng-th·ªùi gian trong t√¢m c√°c h·ªë ƒëen c·ªßa Roger Penrose',\n",
              " 'uit_000571': 'D∆∞·ªõi ·∫£nh h∆∞·ªüng c·ªßa ƒë·ªãnh l√Ω v·ªÅ k√¨ d·ªã kh√¥ng-th·ªùi gian trong t√¢m c√°c h·ªë ƒëen c·ªßa Roger Penrose, Hawking √°p d·ª•ng √Ω t∆∞·ªüng t∆∞∆°ng t·ª± cho to√†n th·ªÉ v≈© tr·ª•',\n",
              " 'uit_000572': 'v·ªÅ nh√¨ trong cu·ªôc thi c·ªßa Qu·ªπ Nghi√™n c·ª©u L·ª±c H·∫•p d·∫´n nƒÉm 1968',\n",
              " 'uit_000573': 'v≈© tr·ª• t·ª± n√≥ c√≥ th·ªÉ kh·ªüi ƒë·∫ßu t·ª´ m·ªôt k√¨ d·ªã',\n",
              " 'uit_000574': 'c√°c quan ni·ªám v·ªÅ ƒë·ªãnh l√Ω ƒëi·ªÉm k√¨ d·ªã',\n",
              " 'uit_000575': 'c√°c quan ni·ªám v·ªÅ ƒë·ªãnh l√Ω ƒëi·ªÉm k√¨ d·ªã m√† √¥ng kh√°m ph√° trong lu·∫≠n √°n ti·∫øn sƒ©',\n",
              " 'uit_000576': 'c√¥ng b·ªë m·ªôt ph√©p ch·ª©ng minh r·∫±ng n·∫øu v≈© tr·ª• tu√¢n theo l√Ω thuy·∫øt t∆∞∆°ng ƒë·ªëi t·ªïng qu√°t v√† ph√π h·ª£p v·ªõi b·∫•t k·ª≥ m√¥ h√¨nh n√†o v·ªÅ v≈© tr·ª• h·ªçc v·∫≠t l√Ω ph√°t tri·ªÉn b·ªüi Alexander Friedmann, th√¨ n√≥ ph·∫£i kh·ªüi ƒë·∫ßu t·ª´ m·ªôt k√¨ d·ªã',\n",
              " 'uit_000577': 'c√¥ng b·ªë m·ªôt ph√©p ch·ª©ng minh r·∫±ng n·∫øu v≈© tr·ª• tu√¢n theo l√Ω thuy·∫øt t∆∞∆°ng ƒë·ªëi t·ªïng qu√°t v√† ph√π h·ª£p v·ªõi b·∫•t k·ª≥ m√¥ h√¨nh n√†o v·ªÅ v≈© tr·ª• h·ªçc v·∫≠t l√Ω ph√°t tri·ªÉn b·ªüi Alexander Friedmann, th√¨ n√≥ ph·∫£i kh·ªüi ƒë·∫ßu t·ª´ m·ªôt k√¨ d·ªã',\n",
              " 'uit_000578': 'v·ªÅ nh√¨ trong cu·ªôc thi c·ªßa Qu·ªπ Nghi√™n c·ª©u L·ª±c H·∫•p d·∫´n nƒÉm 1968',\n",
              " 'uit_000579': 'tr∆∞·ªõc h·∫øt nh∆∞ m·ªôt nh√† khoa h·ªçc, th·ª© ƒë·∫øn nh∆∞ m·ªôt nh√† vƒÉn ph·ªï bi·∫øn khoa h·ªçc, v√†, trong m·ªçi c√°ch m√† n√≥ ƒë√°ng k·ªÉ, m·ªôt ng∆∞·ªùi b√¨nh th∆∞·ªùng v·ªõi c√πng nh·ªØng ham mu·ªën, ngh·ªã l·ª±c, ∆∞·ªõc m∆° v√† tham v·ªçng nh∆∞ nh·ªØng ng∆∞·ªùi xung quanh',\n",
              " 'uit_000580': 'ƒë·ªôc l·∫≠p m·ªôt c√°ch m√£nh li·ªát v√† kh√¥ng b·∫±ng l√≤ng ch·∫•p nh·∫≠n gi√∫p ƒë·ª° hay ch·ªãu nh∆∞·ª£ng b·ªô v√¨ s·ª± t√†n t·∫≠t c·ªßa m√¨nh',\n",
              " 'uit_000581': '√¥ng ph√°t tri·ªÉn c√°c ph∆∞∆°ng ph√°p th·ªã gi√°c ƒë·ªÉ b√π ƒë·∫Øp, bao g·ªìm nh√¨n c√°c ph∆∞∆°ng tr√¨nh theo c√°ch hi·ªÉu h√¨nh h·ªçc',\n",
              " 'uit_000582': '√¥ng b·∫Øt ƒë·∫ßu ph·∫£i d√πng n·∫°ng v√† th∆∞·ªùng xuy√™n h·ªßy c√°c bu·ªïi gi·∫£ng',\n",
              " 'uit_000583': 'tr∆∞·ªõc h·∫øt nh∆∞ m·ªôt nh√† khoa h·ªçc, th·ª© ƒë·∫øn nh∆∞ m·ªôt nh√† vƒÉn ph·ªï bi·∫øn khoa h·ªçc, v√†, trong m·ªçi c√°ch m√† n√≥ ƒë√°ng k·ªÉ, m·ªôt ng∆∞·ªùi b√¨nh th∆∞·ªùng v·ªõi c√πng nh·ªØng ham mu·ªën, ngh·ªã l·ª±c, ∆∞·ªõc m∆° v√† tham v·ªçng nh∆∞ nh·ªØng ng∆∞·ªùi xung quanh',\n",
              " 'uit_000584': '√¥ng ph√°t tri·ªÉn c√°c ph∆∞∆°ng ph√°p th·ªã gi√°c ƒë·ªÉ b√π ƒë·∫Øp, bao g·ªìm nh√¨n c√°c ph∆∞∆°ng tr√¨nh theo c√°ch hi·ªÉu h√¨nh h·ªçc',\n",
              " 'uit_000585': 'ƒë·ªÉ gi·ªØ √¥ng l·∫°i ·ªü Caius',\n",
              " 'uit_000586': 'b·ªánh t·∫≠t c≈©ng nh∆∞ danh ti·∫øng v·ªÅ tr√≠ tu·ªá v√† s·ª± ng·∫°o ng∆∞·ª£c c·ªßa √¥ng',\n",
              " 'uit_000587': 'b·ªánh t·∫≠t c≈©ng nh∆∞ danh ti·∫øng v·ªÅ tr√≠ tu·ªá v√† s·ª± ng·∫°o ng∆∞·ª£c c·ªßa √¥ng',\n",
              " 'uit_000588': 'cu·ªëi nh·ªØng nƒÉm 1960',\n",
              " 'uit_000589': 'cu·ªëi nh·ªØng nƒÉm 1960',\n",
              " 'uit_000590': 'Carter, Werner Israel v√† David C. Robinson',\n",
              " 'uit_000591': 'Jacob Bekenstein, m·ªôt nghi√™n c·ª©u sinh c·ªßa John Wheeler',\n",
              " 'uit_000592': 'b·∫•t k·ªÉ h·ªë ƒëen ban ƒë·∫ßu t·∫°o th√†nh t·ª´ v·∫≠t li·ªáu n√†o, n√≥ ho√†n to√†n c√≥ th·ªÉ m√¥ t·∫£ b·∫±ng ba t√≠nh ch·∫•t kh·ªëi l∆∞·ª£ng, ƒëi·ªán t√≠ch v√† s·ª± t·ª± quay',\n",
              " 'uit_000593': 'C·∫•u tr√∫c Vƒ© m√¥ c·ªßa Kh√¥ng-Th·ªùi gian',\n",
              " 'uit_000594': 'ch√¢n tr·ªùi s·ª± ki·ªán c·ªßa h·ªë ƒëen kh√¥ng bao gi·ªù c√≥ th·ªÉ thu nh·ªè h∆°n',\n",
              " 'uit_000595': 'b·∫•t k·ªÉ h·ªë ƒëen ban ƒë·∫ßu t·∫°o th√†nh t·ª´ v·∫≠t li·ªáu n√†o, n√≥ ho√†n to√†n c√≥ th·ªÉ m√¥ t·∫£ b·∫±ng ba t√≠nh ch·∫•t kh·ªëi l∆∞·ª£ng, ƒëi·ªán t√≠ch v√† s·ª± t·ª± quay',\n",
              " 'uit_000596': 'Jacob Bekenstein',\n",
              " 'uit_000597': 'kh·∫≥ng ƒë·ªãnh r·∫±ng ch√¢n tr·ªùi s·ª± ki·ªán c·ªßa h·ªë ƒëen kh√¥ng bao gi·ªù c√≥ th·ªÉ thu nh·ªè h∆°n',\n",
              " 'uit_000598': \"m·ªôt chuy·∫øn thƒÉm t·ªõi Moskva v√† nh·ªØng cu·ªôc th·∫£o lu·∫≠n v·ªõi Yakov Borisovich Zel'dovich v√† Alexander Starobinsky\",\n",
              " 'uit_000599': 'h·ªë ƒëen ph√°t ra b·ª©c x·∫° - m√† ng√†y nay ƒë∆∞·ª£c g·ªçi l√† b·ª©c x·∫° Hawking - cho ƒë·∫øn khi ch√∫ng c·∫°n ki·ªát nƒÉng l∆∞·ª£ng v√† bay h∆°i',\n",
              " 'uit_000600': 'kh√°m ph√° n√†y ƒë∆∞·ª£c ch·∫•p nh·∫≠n r·ªông r√£i nh∆∞ m·ªôt ƒë·ªôt ph√° quan tr·ªçng trong v·∫≠t l√Ω l√Ω thuy·∫øt',\n",
              " 'uit_000601': 'h·ªë ƒëen ph√°t ra b·ª©c x·∫° - m√† ng√†y nay ƒë∆∞·ª£c g·ªçi l√† b·ª©c x·∫° Hawking',\n",
              " 'uit_000602': 'theo nguy√™n l√Ω b·∫•t ƒë·ªãnh c√°c h·ªë ƒëen quay ph√°t ra c√°c h·∫°t',\n",
              " 'uit_000603': 'h·∫•p d·∫´n l∆∞·ª£ng t·ª≠ v√† c∆° h·ªçc l∆∞·ª£ng t·ª≠',\n",
              " 'uit_000604': 'nh·ªØng t√≠nh to√°n ƒë∆∞·ª£c ki·ªÉm tra nhi·ªÅu l·∫ßn c·ªßa √¥ng cho ra nh·ªØng ph√°t hi·ªán m√¢u thu·∫´n v·ªõi ƒë·ªãnh lu·∫≠t c·ªßa √¥ng',\n",
              " 'uit_000605': 'h·∫•p d·∫´n l∆∞·ª£ng t·ª≠ v√† c∆° h·ªçc l∆∞·ª£ng t·ª≠',\n",
              " 'uit_000606': 'nh·ªØng t√≠nh to√°n ƒë∆∞·ª£c ki·ªÉm tra nhi·ªÅu l·∫ßn c·ªßa √¥ng cho ra nh·ªØng ph√°t hi·ªán m√¢u thu·∫´n v·ªõi ƒë·ªãnh lu·∫≠t c·ªßa √¥ng, v·ªën kh·∫≥ng ƒë·ªãnh r·∫±ng c√°c h·ªë ƒëen kh√¥ng bao gi·ªù co l·∫°i (ch·ªâ gi·ªØ nguy√™n ho·∫∑c l·ªõn l√™n), v√† ·ªßng h·ªô l·∫≠p lu·∫≠n c·ªßa Bekenstein v·ªÅ entropy c·ªßa ch√∫ng',\n",
              " 'uit_000607': 'm·ªôt nghi√™n c·ª©u sinh ho·∫∑c sinh vi√™n h·∫≠u ti·∫øn sƒ© s·ªëng v·ªõi h·ªç v√† gi√∫p chƒÉm s√≥c √¥ng',\n",
              " 'uit_000608': 'khi·∫øn cho c√°c tr√°ch nhi·ªám gia ƒë√¨nh r∆°i xu·ªëng ƒë√¥i vai ng√†y c√†ng ch·∫•t n·∫∑ng c·ªßa v·ª£ √¥ng',\n",
              " 'uit_000609': 'Bernard Carr',\n",
              " 'uit_000610': 'nghi√™n c·ª©u sinh ho·∫∑c sinh vi√™n h·∫≠u ti·∫øn sƒ© s·ªëng v·ªõi h·ªç',\n",
              " 'uit_000611': 'c√°c tr√°ch nhi·ªám gia ƒë√¨nh r∆°i xu·ªëng ƒë√¥i vai ng√†y c√†ng ch·∫•t n·∫∑ng c·ªßa v·ª£ √¥ng',\n",
              " 'uit_000612': 'Cygnus X-1 l√† m·ªôt h·ªë ƒëen',\n",
              " 'uit_000613': 'v·∫≠t ch·∫•t t·ªëi neutralino',\n",
              " 'uit_000614': 'm·ªôt th√°ng',\n",
              " 'uit_000615': 'sao t·ªëi (sao ch·ª©a m·ªôt h√†m l∆∞·ª£ng l·ªõn v·∫≠t ch·∫•t t·ªëi neutralino) Cygnus X-1 l√† m·ªôt h·ªë ƒëen',\n",
              " 'uit_000616': 'h·ªë ƒëen v√† nh·ªØng nh√† v·∫≠t l√Ω nghi√™n c·ª©u ƒë·ªÅ t√†i n√†y',\n",
              " 'uit_000617': 'Ph√≥ Gi√°o s∆∞',\n",
              " 'uit_000618': 'Hawking th∆∞·ªùng xuy√™n ƒë∆∞·ª£c b√°o ch√≠ v√† truy·ªÅn h√¨nh m·ªùi ph·ªèng v·∫•n',\n",
              " 'uit_000619': 'c√¥ng ch√∫ng c√≥ s·ª± quan t√¢m ng√†y c√†ng tƒÉng t·ªõi h·ªë ƒëen',\n",
              " 'uit_000620': 'Ph√≥ Gi√°o s∆∞',\n",
              " 'uit_000621': 'th∆∞·ªùng xuy√™n ƒë∆∞·ª£c b√°o ch√≠ v√† truy·ªÅn h√¨nh m·ªùi ph·ªèng v·∫•n',\n",
              " 'uit_000622': 'B·ªã kh√≠ch ƒë·ªông t·ª´ m·ªôt cu·ªôc tranh lu·∫≠n v·ªõi ƒê·∫°i h·ªçc v·ªÅ vi·ªác ai s·∫Ω tr·∫£ ti·ªÅn cho c√°c b·ªù d·ªëc tho·∫£i ƒë·ªÉ √¥ng c√≥ th·ªÉ ƒëi xe lƒÉn t·ªõi ch·ªó l√†m',\n",
              " 'uit_000623': 'cu·ªëi nh·ªØng nƒÉm 1970',\n",
              " 'uit_000624': 'v·∫≠n ƒë·ªông cho vi·ªác c·∫£i thi·ªán c√°c l·ªëi ƒëi v√†o h·ªó tr·ª£ cho nh·ªØng ng∆∞·ªùi b·ªã t·∫≠t nguy·ªÅn ·ªü Cambridge, bao g·ªìm vi·ªác nu√¥i c√°c sinh vi√™n t√†n t·∫≠t trong tr∆∞·ªùng',\n",
              " 'uit_000625': 'ch·ªâ c√≤n gia ƒë√¨nh v√† nh·ªØng ng∆∞·ªùi b·∫°n th√¢n nh·∫•t hi·ªÉu ƒë∆∞·ª£c √¥ng',\n",
              " 'uit_000626': 'ƒê·ªÉ giao ti·∫øp v·ªõi nh·ªØng ng∆∞·ªùi kh√°c, ai ƒë√≥ hi·ªÉu r√µ s·∫Ω d·ªãch l·ªùi √¥ng cho ng∆∞·ªùi kia',\n",
              " 'uit_000627': 'B·ªã kh√≠ch ƒë·ªông t·ª´ m·ªôt cu·ªôc tranh lu·∫≠n v·ªõi ƒê·∫°i h·ªçc v·ªÅ vi·ªác ai s·∫Ω tr·∫£ ti·ªÅn cho c√°c b·ªù d·ªëc tho·∫£i ƒë·ªÉ √¥ng c√≥ th·ªÉ ƒëi xe lƒÉn t·ªõi ch·ªó l√†m',\n",
              " 'uit_000628': 'trong khi mu·ªën gi√∫p ƒë·ª° ng∆∞·ªùi kh√°c, √¥ng t√¨m c√°ch t√°ch b·∫£n th√¢n ra kh·ªèi chuy·ªán b·ªánh t·∫≠t v√† c√°c kh√≥ khƒÉn c·ªßa n√≥',\n",
              " 'uit_000629': 'vai tr√≤ c·ªßa m√¨nh nh∆∞ m·ªôt ng∆∞·ªùi b√™nh v·ª±c cho quy·ªÅn c·ªßa ng∆∞·ªùi t√†n t·∫≠t',\n",
              " 'uit_000630': 'vai tr√≤ c·ªßa m√¨nh nh∆∞ m·ªôt ng∆∞·ªùi b√™nh v·ª±c cho quy·ªÅn c·ªßa ng∆∞·ªùi t√†n t·∫≠t',\n",
              " 'uit_000631': 'gh·∫ø Gi√°o s∆∞ To√°n h·ªçc Lucas',\n",
              " 'uit_000632': 'm·ªôt v·ªã tr√≠ danh ti·∫øng h√†ng ƒë·∫ßu ·ªü ƒê·∫°i h·ªçc Cambridge c≈©ng nh∆∞ tr√™n th·∫ø gi·ªõi',\n",
              " 'uit_000633': 't·ª´ng l√† v·ªã tr√≠ c·ªßa Isaac Newton v√† Paul Dirac',\n",
              " 'uit_000634': 'Vi·ªác √¥ng thi·∫øu d·∫•n th√¢n v√†o cu·ªôc ƒë·∫•u tranh',\n",
              " 'uit_000635': 'trong khi mu·ªën gi√∫p ƒë·ª° ng∆∞·ªùi kh√°c, √¥ng t√¨m c√°ch t√°ch b·∫£n th√¢n ra kh·ªèi chuy·ªán b·ªánh t·∫≠t v√† c√°c kh√≥ khƒÉn c·ªßa n√≥',\n",
              " 'uit_000636': 'suy nghƒ© theo tr·ª±c gi√°c v√† ∆∞·ªõc ƒëo√°n h∆°n l√† nh·∫•n m·∫°nh v√†o c√°c ph√©p ch·ª©ng minh to√°n h·ªçc',\n",
              " 'uit_000637': 'suy nghƒ© theo tr·ª±c gi√°c v√† ∆∞·ªõc ƒëo√°n h∆°n l√† nh·∫•n m·∫°nh v√†o c√°c ph√©p ch·ª©ng minh to√°n h·ªçc',\n",
              " 'uit_000638': 'Is the end in sight for Theoretical Physics',\n",
              " 'uit_000639': 'd√π r·∫•t mi·ªÖn c∆∞·ª°ng, m·ªôt v√†i d·ªãch v·ª• ƒëi·ªÅu d∆∞·ª°ng t·∫°i gia',\n",
              " 'uit_000640': 'Si√™u h·∫•p d·∫´n N=8 nh∆∞ l√Ω thuy·∫øt h√†ng ƒë·∫ßu nh·∫±m gi·∫£i quy·∫øt nhi·ªÅu b√†i to√°n n·ªïi b·∫≠t m√† c√°c nh√† v·∫≠t l√Ω ƒëang nghi√™n c·ª©u',\n",
              " 'uit_000641': 'm·ªôt v√†i d·ªãch v·ª• ƒëi·ªÅu d∆∞·ª°ng t·∫°i gia',\n",
              " 'uit_000642': 'th√¥ng tin c·ªßa m·ªôt h·ªë ƒëen b·ªã m·∫•t kh√¥ng th·ªÉ ph·ª•c h·ªìi khi m·ªôt h·ªë ƒëen b·ªëc h∆°i',\n",
              " 'uit_000643': 'Ngh·ªãch l√Ω th√¥ng tin h·ªë ƒëen n√†y vi ph·∫°m nguy√™n l√Ω c∆° b·∫£n c·ªßa c∆° h·ªçc l∆∞·ª£ng t·ª≠',\n",
              " 'uit_000644': 'Th√°ng 12 nƒÉm 1977',\n",
              " 'uit_000645': 'nh·ªØng nƒÉm 1980',\n",
              " 'uit_000646': 'khi h√°t t·∫°i m·ªôt d√†n nh·∫°c nh√† th·ªù',\n",
              " 'uit_000647': 'Jane v√† Hellyer Jones quy·∫øt ƒë·ªãnh kh√¥ng ph√° v·ª° gia ƒë√¨nh v√† m·ªëi quan h·ªá c·ªßa h·ªç v·∫´n gi·ªØ trong s√°ng trong m·ªôt th·ªùi gian d√†i',\n",
              " 'uit_000648': 'gi·ªØa nh·ªØng nƒÉm 1980',\n",
              " 'uit_000649': 'n·∫£y n·ªü t√¨nh c·∫£m l√£ng m·∫°n v·ªõi nhau',\n",
              " 'uit_000650': 'nghi√™n c·ª©u l√Ω thuy·∫øt l∆∞·ª£ng t·ª≠ m·ªõi',\n",
              " 'uit_000651': 'theo sau V·ª• N·ªï L·ªõn v≈© tr·ª• ban ƒë·∫ßu m·ªü r·ªông c·ª±c k·ª≥ nhanh ch√≥ng tr∆∞·ªõc khi gi·∫£m t·ªëc ƒë·ªô th√†nh m·ªôt s·ª± gi√£n n·ªü ch·∫≠m h∆°n',\n",
              " 'uit_000652': 'v≈© tr·ª• c√≥ th·ªÉ kh√¥ng c√≥ bi√™n-kh√¥ng c√≥ ƒëi·ªÉm ƒë·∫ßu hay ƒëi·ªÉm cu·ªëi',\n",
              " 'uit_000653': 'theo sau V·ª• N·ªï L·ªõn v≈© tr·ª• ban ƒë·∫ßu m·ªü r·ªông c·ª±c k·ª≥ nhanh ch√≥ng tr∆∞·ªõc khi gi·∫£m t·ªëc ƒë·ªô th√†nh m·ªôt s·ª± gi√£n n·ªü ch·∫≠m h∆°n',\n",
              " 'uit_000654': 'nghi√™n c·ª©u l√Ω thuy·∫øt l∆∞·ª£ng t·ª≠',\n",
              " 'uit_000655': 'v≈© tr·ª• c√≥ th·ªÉ kh√¥ng c√≥ bi√™n-kh√¥ng c√≥ ƒëi·ªÉm ƒë·∫ßu hay ƒëi·ªÉm cu·ªëi',\n",
              " 'uit_000656': 'V≈© tr·ª• Nguy√™n th·ªßy',\n",
              " 'uit_000657': 'N·∫øu v≈© tr·ª• kh√¥ng c√≥ bi√™n m√† t·ª± bao b·ªçc... th√¨ Ch√∫a s·∫Ω kh√¥ng c√≥ b·∫•t k·ª≥ t·ª± do l·ª±a ch·ªçn n√†o v·ªÅ vi·ªác v≈© tr·ª• b·∫Øt ƒë·∫ßu ra sao',\n",
              " 'uit_000658': 'ƒë√≥ l√† ƒëi·ªÉm m√† t·∫•t c·∫£ c√°c ƒë∆∞·ªùng kinh tuy·∫øn h∆∞·ªõng v·ªÅ ph√≠a b·∫Øc g·∫∑p nhau v√† k·∫øt th√∫c',\n",
              " 'uit_000659': 'ƒë∆∞·ª£c thay th·∫ø b·∫±ng m·ªôt v√πng t∆∞∆°ng t·ª± nh∆∞ B·∫Øc C·ª±c',\n",
              " 'uit_000660': 'James Hartle',\n",
              " 'uit_000661': 'kh√¥ng c√≥ bi√™n trong kh√¥ng-th·ªùi gian',\n",
              " 'uit_000662': 'ƒë√≥ l√† ƒëi·ªÉm m√† t·∫•t c·∫£ c√°c ƒë∆∞·ªùng kinh tuy·∫øn h∆∞·ªõng v·ªÅ ph√≠a b·∫Øc g·∫∑p nhau v√† k·∫øt th√∫c',\n",
              " 'uit_000663': 'James Hartle',\n",
              " 'uit_000664': 'kh√¥ng c·∫ßn thi·∫øt ƒë·ªÉ gi·∫£i th√≠ch ngu·ªìn g·ªëc c·ªßa v≈© tr·ª•',\n",
              " 'uit_000665': 'm·ªôt v≈© tr·ª• m·ªü',\n",
              " 'uit_000666': 's·ª± t·ªìn t·∫°i c·ªßa m·ªôt ƒê·∫•ng S√°ng Th·∫ø',\n",
              " 'uit_000667': 'n√≥ c≈©ng t∆∞∆°ng th√≠ch v·ªõi m·ªôt v≈© tr·ª• m·ªü',\n",
              " 'uit_000668': 'nƒÉm 1981 √¥ng nh·∫≠n Huy ch∆∞∆°ng Franklin, v√† nƒÉm 1982 nh·∫≠n t∆∞·ªõc CBE (m·ªôt t∆∞·ªõc b·∫≠c hi·ªáp sƒ© h·∫°ng th·∫•p c·ªßa ƒê·∫ø qu·ªëc Anh)',\n",
              " 'uit_000669': 't∆∞·ªõc CBE',\n",
              " 'uit_000670': 'v√†o l√∫c v≈© tr·ª• ng·ª´ng d√£n n·ªü v√† cu·ªëi c√πng suy s·ª•p, th·ªùi gian s·∫Ω ch·∫°y theo h∆∞·ªõng ng∆∞·ª£c l·∫°i',\n",
              " 'uit_000671': 'c√¥ng b·ªë c·ªßa Don Page v√† Raymond Laflamme',\n",
              " 'uit_000672': 'v√†o l√∫c v≈© tr·ª• ng·ª´ng d√£n n·ªü v√† cu·ªëi c√πng suy s·ª•p, th·ªùi gian s·∫Ω ch·∫°y theo h∆∞·ªõng ng∆∞·ª£c l·∫°i',\n",
              " 'uit_000673': 'A Brief History of Time',\n",
              " 'uit_000674': 'Bantam Books',\n",
              " 'uit_000675': 'nh·∫≠n m·ªôt kho·∫£n ti·ªÅn ƒë·∫∑t c·ªçc l·ªõn cho t√°c ph·∫©m',\n",
              " 'uit_000676': 'Bantam Books',\n",
              " 'uit_000677': 'A Brief History of Time',\n",
              " 'uit_000678': 'thanh to√°n h√≥a ƒë∆°n, n√™n d∆∞·ªõi nhu c·∫ßu trang tr·∫£i chi ph√≠ vi·ªác h·ªçc h√†nh c·ªßa con c√°i v√† sinh ho·∫°t gia ƒë√¨nh',\n",
              " 'uit_000679': 'C√°c y t√° ƒë∆∞·ª£c thu√™ su·ªët ba ca ƒë·ªÉ chƒÉm s√≥c √¥ng hai m∆∞∆°i b·ªën ti·∫øng ƒë·ªìng h·ªì m·ªói ng√†y',\n",
              " 'uit_000680': 'c√≥ th·ªÉ ƒëe d·ªça t√≠nh m·∫°ng',\n",
              " 'uit_000681': 'C∆° quan ChƒÉm s√≥c S·ª©c kh·ªèe Anh',\n",
              " 'uit_000682': 'chƒÉm s√≥c ƒëi·ªÅu d∆∞·ª°ng su·ªët ng√†y ƒë√™m v√† lo·∫°i b·ªè nƒÉng l·ª±c ph√°t √¢m √≠t ·ªèi c√≤n l·∫°i c·ªßa √¥ng',\n",
              " 'uit_000683': 'chƒÉm s√≥c √¥ng hai m∆∞∆°i b·ªën ti·∫øng ƒë·ªìng h·ªì m·ªói ng√†y',\n",
              " 'uit_000684': 'm·ªôt qu·ªπ ·ªü Hoa K·ª≥',\n",
              " 'uit_000685': 's·ª≠ d·ª•ng m·ªôt c√¥ng t·∫Øc √¥ng ch·ªçn c√°c c·ª•m t·ª´, t·ª´, ho·∫∑c ch·ªØ c√°i t·ª´ m·ªôt b·ªô nh·ªõ ch·ª©a kho·∫£ng 2500-3000 l·ª±a ch·ªçn ƒë∆∞·ª£c qu√©t qua b·ªüi m√°y',\n",
              " 'uit_000686': 'Gi·ªù t√¥i ƒë√¢m ra giao ti·∫øp t·ªët h∆°n l√† tr∆∞·ªõc khi t√¥i m·∫•t gi·ªçng n√≥i',\n",
              " 'uit_000687': 'Equalizer',\n",
              " 'uit_000688': 'Gi·ªù t√¥i ƒë√¢m ra giao ti·∫øp t·ªët h∆°n l√† tr∆∞·ªõc khi t√¥i m·∫•t gi·ªçng n√≥i',\n",
              " 'uit_000689': 's·ª≠ d·ª•ng m·ªôt c√¥ng t·∫Øc √¥ng ch·ªçn c√°c c·ª•m t·ª´, t·ª´, ho·∫∑c ch·ªØ c√°i t·ª´ m·ªôt b·ªô nh·ªõ ch·ª©a kho·∫£ng 2500-3000 l·ª±a ch·ªçn ƒë∆∞·ª£c qu√©t qua b·ªüi m√°y',\n",
              " 'uit_000690': 'th√∫c ƒë·∫©y √¥ng ph·∫£i gi·∫£i th√≠ch c√°c √Ω t∆∞·ªüng m·ªôt c√°ch r√µ r√†ng trong ng√¥n ng·ªØ kh√¥ng mang t√≠nh k·ªπ thu·∫≠t',\n",
              " 'uit_000691': 'y√™u c·∫ßu tr·ª£ l√Ω gi√∫p √¥ng ho√†n th√†nh vi·ªác vi·∫øt \"L∆∞·ª£c s·ª≠ Th·ªùi gian\"',\n",
              " 'uit_000692': 'tr·ªü th√†nh m·ªôt th√†nh c√¥ng phi th∆∞·ªùng, nhanh ch√≥ng v∆∞∆°n l√™n ƒë·∫ßu c√°c danh s√°ch b√°n ch·∫°y nh·∫•t ·ªü c·∫£ hai qu·ªëc gia v√† duy tr√¨ v·ªã tr√≠ kh√¥ng ch·ªâ nhi·ªÅu tu·∫ßn m√† nhi·ªÅu nƒÉm li√™n t·ª•c',\n",
              " 'uit_000693': 'tr·ªü th√†nh m·ªôt th√†nh c√¥ng phi th∆∞·ªùng, nhanh ch√≥ng v∆∞∆°n l√™n ƒë·∫ßu c√°c danh s√°ch b√°n ch·∫°y nh·∫•t ·ªü c·∫£ hai qu·ªëc gia v√† duy tr√¨ v·ªã tr√≠ kh√¥ng ch·ªâ nhi·ªÅu tu·∫ßn m√† nhi·ªÅu nƒÉm li√™n t·ª•c',\n",
              " 'uit_000694': 'y√™u c·∫ßu tr·ª£ l√Ω gi√∫p √¥ng ho√†n th√†nh vi·ªác vi·∫øt \"L∆∞·ª£c s·ª≠ Th·ªùi gian\"',\n",
              " 'uit_000695': 'ƒë∆∞·ª£c d·ªãch sang nhi·ªÅu th·ª© ti·∫øng, v√† t·ªõi nƒÉm 2009 b√°n ƒë∆∞·ª£c √≠t nh·∫•t 9 tri·ªáu b·∫£n',\n",
              " 'uit_000696': 'Master of the Universe',\n",
              " 'uit_000697': '\"L∆∞·ª£c s·ª≠ Th·ªùi gian\" ƒë∆∞·ª£c d·ªãch sang nhi·ªÅu th·ª© ti·∫øng, v√† t·ªõi nƒÉm 2009 b√°n ƒë∆∞·ª£c √≠t nh·∫•t 9 tri·ªáu b·∫£n',\n",
              " 'uit_000698': 'Hawking ƒë√£ du h√†nh li√™n t·ª•c ƒë·ªÉ qu·∫£ng b√° c√¥ng tr√¨nh c·ªßa m√¨nh, v√† tham gia ti·ªác t√πng v√† khi√™u v≈© t·ªõi t·∫≠n ƒë√™m khuya',\n",
              " 'uit_000699': 'trong vai tr√≤ ng∆∞·ªùi n·ªïi ti·∫øng',\n",
              " 'uit_000700': 'nƒÉm b·∫±ng ti·∫øn sƒ© danh d·ª±, Huy ch∆∞∆°ng V√†ng c·ªßa H·ªôi Thi√™n vƒÉn h·ªçc Ho√†ng gia (1985), Huy ch∆∞∆°ng Paul Dirac (1987) v√†, c√πng v·ªõi Penrose, Gi·∫£i Wolf danh ti·∫øng (1988). NƒÉm 1989, √¥ng ƒë∆∞·ª£c N·ªØ ho√†ng Elizabeth II phong t∆∞·ªõc CH',\n",
              " 'uit_000701': '√≠t c√≥ th·ªùi gian d√†nh cho c√¥ng vi·ªác v√† c√°c h·ªçc tr√≤',\n",
              " 'uit_000702': 't∆∞·ªõc hi·ªáu d√¢n s·ª± cao th·ª© hai m√† m·ªôt b√¨nh d√¢n Anh c√≥ th·ªÉ ƒë·∫°t ƒë∆∞·ª£c, th·∫•p h∆°n Hu√¢n ch∆∞∆°ng C√¥ng tr·∫°ng-OM',\n",
              " 'uit_000703': 'khi·∫øn √¥ng √≠t c√≥ th·ªùi gian d√†nh cho c√¥ng vi·ªác v√† c√°c h·ªçc tr√≤',\n",
              " 'uit_000704': 'ch·ªß y·∫øu l√† do s·ª± t√†n t·∫≠t c·ªßa √¥ng',\n",
              " 'uit_000705': 's·ª± t√†n t·∫≠t c·ªßa √¥ng',\n",
              " 'uit_000706': 'Quan ƒëi·ªÉm b·∫•t kh·∫£ tri v·ªÅ t√¥n gi√°o c·ªßa Hawking c≈©ng t∆∞∆°ng ph·∫£n v·ªõi ƒë·ª©c tin Ki t√¥ gi√°o m·∫°nh m·∫Ω c·ªßa ng∆∞·ªùi v·ª£',\n",
              " 'uit_000707': 'Cu·ªôc h√¥n nh√¢n gi·ªØa Jane v√† Stephen Hawking',\n",
              " 'uit_000708': 'g√¢y th√°ch th·ª©c cho c√°c ƒë·ªìng nghi·ªáp v√† th√†nh vi√™n gia ƒë√¨nh',\n",
              " 'uit_000709': 'g√¢y th√°ch th·ª©c cho c√°c ƒë·ªìng nghi·ªáp v√† th√†nh vi√™n gia ƒë√¨nh',\n",
              " 'uit_000710': 'ƒë·ªÉ thƒÉm Nh√†n',\n",
              " 'uit_000711': 'nƒÉm 1990',\n",
              " 'uit_000712': 'Hawking tr·ªü n√™n ng√†y c√†ng g·∫ßn g≈©i v·ªõi m·ªôt trong s·ªë c√°c y t√° c·ªßa √¥ng, Elaine Mason',\n",
              " 'uit_000713': 'nh·∫≠n m·ªôt c√¥ g√°i Vi·ªát Nam s·ªëng ·ªü L√†ng tr·∫ª em SOS t√™n l√† Nguy·ªÖn Th·ªã Thu Nh√†n l√†m con nu√¥i',\n",
              " 'uit_000714': 'ƒë·ªÅ xu·∫•t c·ªßa Penrose v·ªÅ m·ªôt \"ph·ªèng ƒëo√°n ki·ªÉm duy·ªát v≈© tr·ª•\"',\n",
              " 'uit_000715': 'ƒë·ªÅ xu·∫•t c·ªßa Penrose v·ªÅ m·ªôt \"ph·ªèng ƒëo√°n ki·ªÉm duy·ªát v≈© tr·ª•\"-r·∫±ng kh√¥ng th·ªÉ n√†o c√≥ \"k√¨ d·ªã tr·∫ßn tru·ªìng\" kh√¥ng che b·ªüi m·ªôt ch√¢n tr·ªùi-l√† ƒë√∫ng',\n",
              " 'uit_000716': 'Gary Gibbons',\n",
              " 'uit_000717': 'Gary Gibbons',\n",
              " 'uit_000718': 'h·ªë ƒëen v√† V·ª• N·ªï L·ªõn',\n",
              " 'uit_000719': 'quan ni·ªám v·ªÅ h·ªë ƒëen cho b·ªüi thuy·∫øt t∆∞∆°ng ƒë·ªëi t·ªïng qu√°t',\n",
              " 'uit_000720': 'theo h∆∞·ªõng ng∆∞·ª£c l·∫°i, r·∫±ng c∆° h·ªçc l∆∞·ª£ng t·ª≠ ƒë·ªÅ xu·∫•t r·∫±ng th√¥ng tin ph√°t ra b·ªüi m·ªôt h·ªë ƒëen li√™n quan t·ªõi th√¥ng tin r∆°i v√†o n√≥ ·ªü m·ªôt th·ªùi ƒëi·ªÉm tr∆∞·ªõc ƒë·∫•y, quan ni·ªám v·ªÅ h·ªë ƒëen cho b·ªüi thuy·∫øt t∆∞∆°ng ƒë·ªëi t·ªïng qu√°t ph·∫£i ƒë∆∞·ª£c hi·ªáu ch·ªânh theo m·ªôt c√°ch n√†o ƒë√≥',\n",
              " 'uit_000721': 'c∆° h·ªçc l∆∞·ª£ng t·ª≠ ƒë·ªÅ xu·∫•t r·∫±ng th√¥ng tin ph√°t ra b·ªüi m·ªôt h·ªë ƒëen li√™n quan t·ªõi th√¥ng tin r∆°i v√†o n√≥ ·ªü m·ªôt th·ªùi ƒëi·ªÉm tr∆∞·ªõc ƒë·∫•y',\n",
              " 'uit_000722': 'ngh·ªãch l√Ω th√¥ng tin h·ªë ƒëen',\n",
              " 'uit_000723': 'ngh·ªãch l√Ω th√¥ng tin h·ªë ƒëen',\n",
              " 'uit_000724': 'ƒëem khoa h·ªçc t·ªõi m·ªôt l·ªõp c√¥ng ch√∫ng r·ªông r√£i h∆°n',\n",
              " 'uit_000725': 'khoa h·ªçc',\n",
              " 'uit_000726': 'H·ªë ƒêen v√† c√°c V≈© tr·ª• S∆° sinh v√† nh·ªØng Ti·ªÉu lu·∫≠n kh√°c',\n",
              " 'uit_000727': 'khoa h·ªçc',\n",
              " 'uit_000728': 'Leonard Nimoy, ng∆∞·ªùi ƒë√≥ng vai Spock trong Star Trek, bi·∫øt ƒë∆∞·ª£c r·∫±ng Hawking h·ª©ng th√∫ v·ªõi vi·ªác xu·∫•t hi·ªán trong ch∆∞∆°ng tr√¨nh',\n",
              " 'uit_000729': 'bi·∫øt ƒë∆∞·ª£c r·∫±ng Hawking h·ª©ng th√∫ v·ªõi vi·ªác xu·∫•t hi·ªán trong ch∆∞∆°ng tr√¨nh',\n",
              " 'uit_000730': 'Keep Talking',\n",
              " 'uit_000731': 's√™-ri h√†i k·ªãch t√¨nh hu·ªëng The Simpsons',\n",
              " 'uit_000732': 'xu·∫•t hi·ªán nƒÉm 1999 trong s√™-ri h√†i k·ªãch t√¨nh hu·ªëng The Simpsons',\n",
              " 'uit_000733': 'm·ªôt v≈© tr·ª• h·ªçc \"tr√™n-xu·ªëng\", ph√°t bi·ªÉu r·∫±ng v≈© tr·ª• kh√¥ng ph·∫£i c√≥ m·ªôt tr·∫°ng th√°i ban ƒë·∫ßu duy nh·∫•t m√† l√† nhi·ªÅu tr·∫°ng th√°i, v√† do ƒë√≥ l√† kh√¥ng th√≠ch h·ª£p ƒë·ªÉ h√¨nh th√†nh m·ªôt l√Ω thuy·∫øt ti√™n ƒëo√°n h√¨nh d·∫°ng hi·ªán t·∫°i c·ªßa v≈© tr·ª• t·ª´ m·ªôt tr·∫°ng th√°i ban ƒë·∫ßu ƒë·∫∑c bi·ªát n√†o',\n",
              " 'uit_000734': 'ph√°t tri·ªÉn m·ªôt v≈© tr·ª• h·ªçc \"tr√™n-xu·ªëng\"',\n",
              " 'uit_000735': 'V≈© tr·ª• trong v·ªè h·∫°t d·∫ª',\n",
              " 'uit_000736': 'ti√™n ƒëo√°n h√¨nh d·∫°ng hi·ªán t·∫°i c·ªßa v≈© tr·ª• t·ª´ m·ªôt tr·∫°ng th√°i ban ƒë·∫ßu ƒë·∫∑c bi·ªát n√†o',\n",
              " 'uit_000737': 'thi·∫øu nhi',\n",
              " 'uit_000738': 'nƒÉm 2006',\n",
              " 'uit_000739': 'thi·∫øu nhi',\n",
              " 'uit_000740': 'M·ªôt phi√™n b·∫£n hi·ªáu ch·ªânh c·ªßa cu·ªën s√°ch tr∆∞·ªõc ƒë√¢y c·ªßa Jane, nay mang t√™n m·ªõi \"H√†nh tr√¨nh t·ªõi V√¥ h·∫°n, Cu·ªôc ƒë·ªùi t√¥i v·ªõi Stephen\", xu·∫•t hi·ªán nƒÉm 2007',\n",
              " 'uit_000741': 'tr√¨nh b√†y v·∫≠t l√Ω l√Ω thuy·∫øt theo c√°ch d·ªÖ hi·ªÉu v√† m√¥ t·∫£ c√°c nh√¢n v·∫≠t t∆∞∆°ng t·ª± c√°c th√†nh vi√™n gia ƒë√¨nh Hawking',\n",
              " 'uit_000742': 'nƒÉm 2006',\n",
              " 'uit_000743': 'phi√™n b·∫£n hi·ªáu ch·ªânh c·ªßa cu·ªën s√°ch tr∆∞·ªõc ƒë√¢y c·ªßa Jane, nay mang t√™n m·ªõi \"H√†nh tr√¨nh t·ªõi V√¥ h·∫°n, Cu·ªôc ƒë·ªùi t√¥i v·ªõi Stephen\"',\n",
              " 'uit_000744': 'phim t√†i li·ªáu c√≥ t√™n The Real Stephen Hawking: (2001) v√† \"Stephen Hawking: Profile\" (2002), m·ªôt phim truy·ªÅn h√¨nh Hawking v·ªÅ giai ƒëo·∫°n b·∫Øt ƒë·∫ßu cƒÉn b·ªánh c·ªßa Hawking (2004), c√πng m·ªôt s√™-ri phim t√†i li·ªáu Stephen Hawking, Master of the Universe (2008)',\n",
              " 'uit_000745': 'm√°y bay ph·∫£n l·ª±c c√° nh√¢n',\n",
              " 'uit_000746': 'Chile, ƒê·∫£o Ph·ª•c Sinh, Nam Phi, r·ªìi T√¢y Ban Nha (ƒë·ªÉ nh·∫≠n Gi·∫£i Fonseca nƒÉm 2008), Canada v√† nhi·ªÅu chuy·∫øn ƒëi t·ªõi Hoa K·ª≥',\n",
              " 'uit_000747': 'Chile, ƒê·∫£o Ph·ª•c Sinh, Nam Phi, r·ªìi T√¢y Ban Nha (ƒë·ªÉ nh·∫≠n Gi·∫£i Fonseca nƒÉm 2008), Canada v√† nhi·ªÅu chuy·∫øn ƒëi t·ªõi Hoa K·ª≥',\n",
              " 'uit_000748': 'li√™n quan t·ªõi s·ª± t√†n t·∫≠t c·ªßa √¥ng',\n",
              " 'uit_000749': 'm·ªôt lo·∫°t tuy√™n b·ªë g√¢y ch√∫ √Ω v√† th∆∞·ªùng g√¢y tranh c√£i',\n",
              " 'uit_000750': 's·ª± s·ªëng tr√™n Tr√°i ƒê·∫•t b·ªã ƒëe d·ªça do \"m·ªôt cu·ªôc chi·∫øn tranh h·∫°t nh√¢n ƒë·ªôt ng·ªôt, m·ªôt virus ƒë∆∞·ª£c l·∫≠p tr√¨nh gien hay c√°c m·ªëi hi·ªÉm h·ªça m√† ch√∫ng ta c√≤n ch∆∞a nghƒ© t·ªõi\"',\n",
              " 'uit_000751': 'm·ªôt cu·ªôc chi·∫øn tranh h·∫°t nh√¢n ƒë·ªôt ng·ªôt, m·ªôt virus ƒë∆∞·ª£c l·∫≠p tr√¨nh gien hay c√°c m·ªëi hi·ªÉm h·ªça m√† ch√∫ng ta c√≤n ch∆∞a nghƒ© t·ªõi',\n",
              " 'uit_000752': '√¥ng t·ª´ng kh·∫≥ng ƒë·ªãnh r·∫±ng virus m√°y t√≠nh l√† m·ªôt d·∫°ng s·ª± s·ªëng, r·∫±ng con ng∆∞·ªùi n√™n s·ª≠ d·ª•ng kƒ© thu·∫≠t di truy·ªÅn ƒë·ªÉ tr√°nh kh·ªèi b·ªã v∆∞·ª£t m·∫∑t b·ªüi m√°y t√≠nh, v√† r·∫±ng ng∆∞·ªùi ngo√†i h√†nh tinh c√≥ l·∫Ω t·ªìn t·∫°i v√† c·∫ßn tr√°nh giao ti·∫øp v·ªõi h·ªç v√¨ h·ªç c√≥ th·ªÉ s·∫Ω chinh ph·∫°t con ng∆∞·ªùi',\n",
              " 'uit_000753': 'Mong mu·ªën tƒÉng c∆∞·ªùng m·ªëi quan t√¢m c·ªßa c√¥ng ch√∫ng t·ªõi c√°c chuy·∫øn bay ra ngo√†i kh√¥ng gian v√† th·ªÉ hi·ªán ti·ªÅm nƒÉng c·ªßa nh·ªØng ng∆∞·ªùi t√†n t·∫≠t',\n",
              " 'uit_000754': 'c√°c chuy·∫øn bay ra ngo√†i kh√¥ng gian v√† th·ªÉ hi·ªán ti·ªÅm nƒÉng c·ªßa nh·ªØng ng∆∞·ªùi t√†n t·∫≠t',\n",
              " 'uit_000755': '√¥ng t·ª´ng kh·∫≥ng ƒë·ªãnh r·∫±ng virus m√°y t√≠nh l√† m·ªôt d·∫°ng s·ª± s·ªëng, r·∫±ng con ng∆∞·ªùi n√™n s·ª≠ d·ª•ng kƒ© thu·∫≠t di truy·ªÅn ƒë·ªÉ tr√°nh kh·ªèi b·ªã v∆∞·ª£t m·∫∑t b·ªüi m√°y t√≠nh, v√† r·∫±ng ng∆∞·ªùi ngo√†i h√†nh tinh c√≥ l·∫Ω t·ªìn t·∫°i v√† c·∫ßn tr√°nh giao ti·∫øp v·ªõi h·ªç v√¨ h·ªç c√≥ th·ªÉ s·∫Ω chinh ph·∫°t con ng∆∞·ªùi',\n",
              " 'uit_000756': 'c√°c chuy·∫øn bay kh√¥ng gian v√† vi·ªác l·∫≠p thu·ªôc ƒë·ªãa ngo√†i v≈© tr·ª•',\n",
              " 'uit_000757': 'c√°c chuy·∫øn bay kh√¥ng gian v√† vi·ªác l·∫≠p thu·ªôc ƒë·ªãa ngo√†i v≈© tr·ª•',\n",
              " 'uit_000758': 'b√†y t·ªè s·ª± ·ªßng h·ªô v·ªõi ·ª©ng c·ª≠ vi√™n D√¢n ch·ªß Al Gore trong cu·ªôc b·∫ßu c·ª≠ T·ªïng th·ªëng Hoa K·ª≥ nƒÉm 2000, g·ªçi Cu·ªôc t·∫•n c√¥ng Iraq 2003 l√† m·ªôt \"t·ªôi √°c chi·∫øn tranh\", t·∫©y chay m·ªôt h·ªôi th·∫£o ·ªü Israel do lo ng·∫°i v·ªÅ ch√≠nh s√°ch c·ªßa Israel ƒë·ªëi v·ªõi ng∆∞·ªùi Palestine, duy tr√¨ chi·∫øn d·ªãch l√¢u d√†i c·ªßa √¥ng v·∫≠n ƒë·ªông gi·∫£i tr·ª´ v≈© kh√≠ h·∫°t nh√¢n, v√† ·ªßng h·ªô nghi√™n c·ª©u t·∫ø b√†o g·ªëc, h·ªá th·ªëng y t·∫ø to√†n c·∫ßu, v√† h√†nh ƒë·ªông ngƒÉn ch·∫∑n bi·∫øn ƒë·ªïi kh√≠ h·∫≠u',\n",
              " 'uit_000759': 'b√†y t·ªè s·ª± ·ªßng h·ªô v·ªõi ·ª©ng c·ª≠ vi√™n D√¢n ch·ªß Al Gore trong cu·ªôc b·∫ßu c·ª≠ T·ªïng th·ªëng Hoa K·ª≥ nƒÉm 2000, g·ªçi Cu·ªôc t·∫•n c√¥ng Iraq 2003 l√† m·ªôt \"t·ªôi √°c chi·∫øn tranh\", t·∫©y chay m·ªôt h·ªôi th·∫£o ·ªü Israel do lo ng·∫°i v·ªÅ ch√≠nh s√°ch c·ªßa Israel ƒë·ªëi v·ªõi ng∆∞·ªùi Palestine, duy tr√¨ chi·∫øn d·ªãch l√¢u d√†i c·ªßa √¥ng v·∫≠n ƒë·ªông gi·∫£i tr·ª´ v≈© kh√≠ h·∫°t nh√¢n, v√† ·ªßng h·ªô nghi√™n c·ª©u t·∫ø b√†o g·ªëc, h·ªá th·ªëng y t·∫ø to√†n c·∫ßu, v√† h√†nh ƒë·ªông ngƒÉn ch·∫∑n bi·∫øn ƒë·ªïi kh√≠ h·∫≠u',\n",
              " 'uit_000760': 'm·ªôt chi·∫øc xe lƒÉn, National Savings, British Telecom, Specsavers, Egg Banking, v√† Go Compare',\n",
              " 'uit_000761': 'ƒê·∫£ng Lao ƒë·ªông',\n",
              " 'uit_000762': 'ƒê·∫£ng Lao ƒë·ªông',\n",
              " 'uit_000763': 'National Savings, British Telecom, Specsavers, Egg Banking, v√† Go Compare',\n",
              " 'uit_000764': 's·ª± m·∫•t m√°t th√¥ng tin c·ªßa m·ªôt h·ªë ƒëen',\n",
              " 'uit_000765': 'c√°c h·ªë ƒëen c√≥ nhi·ªÅu h∆°n m·ªôt t√¥ p√¥',\n",
              " 'uit_000766': '√¥ng l·∫≠p lu·∫≠n r·∫±ng ngh·ªãch l√Ω th√¥ng tin ƒë∆∞·ª£c gi·∫£i th√≠ch b·∫±ng c√°ch ki·ªÉm tra t·∫•t c·∫£ nh·ªØng l·ªãch s·ª≠ t∆∞∆°ng ƒë∆∞∆°ng c·ªßa v≈© tr·ª•, v·ªõi m·∫•t m√°t th√¥ng tin trong nh·ªØng v≈© tr·ª• c√≥ h·ªë ƒëen s·∫Ω ƒë∆∞·ª£c tri·ªát ti√™u b·ªüi nh·ªØng v≈© tr·ª• kh√¥ng c√≥',\n",
              " 'uit_000767': 'ngh·ªãch l√Ω th√¥ng tin ƒë∆∞·ª£c gi·∫£i th√≠ch b·∫±ng c√°ch ki·ªÉm tra t·∫•t c·∫£ nh·ªØng l·ªãch s·ª≠ t∆∞∆°ng ƒë∆∞∆°ng c·ªßa v≈© tr·ª•, v·ªõi m·∫•t m√°t th√¥ng tin trong nh·ªØng v≈© tr·ª• c√≥ h·ªë ƒëen s·∫Ω ƒë∆∞·ª£c tri·ªát ti√™u b·ªüi nh·ªØng v≈© tr·ª• kh√¥ng c√≥',\n",
              " 'uit_000768': 'Hawking nhanh ch√≥ng th·ª´a nh·∫≠n thua cu·ªôc v√† n√≥i r·∫±ng Higgs n√™n nh·∫≠n ƒë∆∞·ª£c Gi·∫£i Nobel V·∫≠t l√Ω',\n",
              " 'uit_000769': 'v·ªã th·∫ø n·ªïi ti·∫øng c·ªßa Hawking ƒëem l·∫°i cho √¥ng ta s·ª± tin c·∫≠y m√† ng∆∞·ªùi kh√°c kh√¥ng c√≥',\n",
              " 'uit_000770': 'v·ªã th·∫ø n·ªïi ti·∫øng c·ªßa Hawking ƒëem l·∫°i cho √¥ng ta s·ª± tin c·∫≠y m√† ng∆∞·ªùi kh√°c kh√¥ng c√≥',\n",
              " 'uit_000771': 'th√°ng 7 nƒÉm 2012',\n",
              " 'uit_000772': 'c√¥ng khai v·ªÅ v·∫•n ƒë·ªÅ n√†y nƒÉm 2002 v√† ti·∫øp t·ª•c nƒÉm 2008, trong ƒë√≥ Higgs ch·ªâ tr√≠ch c√¥ng tr√¨nh c·ªßa Hawking',\n",
              " 'uit_000773': 'Hawking ƒë√£ kh·∫≥ng ƒë·ªãnh d·ª©t kho√°t, v√† ƒë√°nh c∆∞·ª£c, r·∫±ng s·∫Ω kh√¥ng bao gi·ªù t√¨m th·∫•y ƒë∆∞·ª£c Boson Higgs',\n",
              " 'uit_000774': 'Hawking ƒë√£ kh·∫≥ng ƒë·ªãnh d·ª©t kho√°t, v√† ƒë√°nh c∆∞·ª£c, r·∫±ng s·∫Ω kh√¥ng bao gi·ªù t√¨m th·∫•y ƒë∆∞·ª£c Boson Higgs',\n",
              " 'uit_000775': 't√¨nh tr·∫°ng v·∫´n t·ªânh t√°o nh∆∞ng kh√¥ng th·ªÉ c·ª≠ ƒë·ªông b·∫•t c·ª© b·ªô ph·∫≠n n√†o ngo√†i m·∫Øt',\n",
              " 'uit_000776': '√¥ng ƒëang h·ª£p t√°c v·ªõi c√°c nh√† nghi√™n c·ª©u v·ªÅ c√°c h·ªá th·ªëng c√≥ th·ªÉ di·ªÖn d·ªãch c√°c h√¨nh ·∫£nh n√£o b·ªô ho·∫∑c bi·ªÉu di·ªÖn n√©t m·∫∑t th√†nh ph∆∞∆°ng th·ª©c k√≠ch ho·∫°t c√¥ng t·∫Øc',\n",
              " 'uit_000777': '√¥ng b·∫Øt ƒë·∫ßu ph·∫£i ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã giao ti·∫øp b·∫±ng c·ª≠ ƒë·ªông c·ªßa c∆° m√° do kh√¥ng th·ªÉ s·ª≠ d·ª•ng tay n·ªØa, v·ªõi t·ªëc ƒë·ªô ch·ªâ m·ªôt t·ª´ m·ªói ph√∫t',\n",
              " 'uit_000778': 'Huy ch∆∞∆°ng Copley t·ª´ H·ªôi Ho√†ng gia (2006), vinh d·ª± d√¢n s·ª± cao nh·∫•t c·ªßa Hoa K·ª≥-Hu√¢n ch∆∞∆°ng T·ª± do T·ªïng th·ªëng (2009), v√† Gi·∫£i th∆∞·ªüng V·∫≠t l√Ω C∆° b·∫£n Nga (2012)',\n",
              " 'uit_000779': 'ph·∫£i ƒëi·ªÅu khi·ªÉn thi·∫øt b·ªã giao ti·∫øp b·∫±ng c·ª≠ ƒë·ªông c·ªßa c∆° m√° do kh√¥ng th·ªÉ s·ª≠ d·ª•ng tay n·ªØa, v·ªõi t·ªëc ƒë·ªô ch·ªâ m·ªôt t·ª´ m·ªói ph√∫t',\n",
              " 'uit_000780': 'Huy ch∆∞∆°ng Copley',\n",
              " 'uit_000781': 'h·ª£p t√°c v·ªõi c√°c nh√† nghi√™n c·ª©u v·ªÅ c√°c h·ªá th·ªëng c√≥ th·ªÉ di·ªÖn d·ªãch c√°c h√¨nh ·∫£nh n√£o b·ªô ho·∫∑c bi·ªÉu di·ªÖn n√©t m·∫∑t th√†nh ph∆∞∆°ng th·ª©c k√≠ch ho·∫°t c√¥ng t·∫Øc',\n",
              " 'uit_000782': 'v≈© tr·ª• ƒë∆∞·ª£c v·∫≠n h√†nh b·∫±ng c√°c ƒë·ªãnh lu·∫≠t khoa h·ªçc. C√°c ƒë·ªãnh lu·∫≠t ƒë√≥ c√≥ th·ªÉ ƒë∆∞·ª£c Ch√∫a Tr·ªùi ban b·ªë, nh∆∞ng Ch√∫a kh√¥ng can thi·ªáp ƒë·ªÉ ph√° v·ª° ch√∫ng',\n",
              " 'uit_000783': 'Curiosity tr√™n Discovery Channel',\n",
              " 'uit_000784': 'v≈© tr·ª• ƒë∆∞·ª£c v·∫≠n h√†nh b·∫±ng c√°c ƒë·ªãnh lu·∫≠t khoa h·ªçc',\n",
              " 'uit_000785': 'Thi√™n ƒë∆∞·ªùng l√† m·ªôt huy·ªÅn tho·∫°i, tin r·∫±ng \"kh√¥ng c√≥ thi√™n ƒë∆∞·ªùng hay th·∫ø gi·ªõi b√™n kia\"',\n",
              " 'uit_000786': 'Thi√™n ƒë∆∞·ªùng l√† m·ªôt huy·ªÅn tho·∫°i, tin r·∫±ng \"kh√¥ng c√≥ thi√™n ƒë∆∞·ªùng hay th·∫ø gi·ªõi b√™n kia\" v√† r·∫±ng m·ªôt kh√°i ni·ªám nh∆∞ th·∫ø l√† \"m·ªôt truy·ªán c·ªï t√≠ch d√†nh cho nh·ªØng ng∆∞·ªùi s·ª£ b√≥ng t·ªëi.\"',\n",
              " 'uit_000787': 'Curiosity tr√™n Discovery Channel',\n",
              " 'uit_000788': 'c√°c nh√† khoa h·ªçc \"ƒë√£ tr·ªü th√†nh ng∆∞·ªùi mang ng·ªçn ƒëu·ªëc kh√°m ph√° trong cu·ªôc truy t·∫ßm tri th·ª©c c·ªßa ch√∫ng ta.\"',\n",
              " 'uit_000789': 'c√°c tri·∫øt gia \"kh√¥ng b·∫Øt k·ªãp v·ªõi nh·ªØng ti·∫øn b·ªô khoa h·ªçc hi·ªán ƒë·∫°i\"',\n",
              " 'uit_000790': 'Khai s√°ng',\n",
              " 'uit_000791': 'c√°c v·∫•n ƒë·ªÅ tri·∫øt h·ªçc c√≥ th·ªÉ ƒë∆∞·ª£c khoa h·ªçc tr·∫£ l·ªùi, ƒë·∫∑c bi·ªát l√† nh·ªØng l√Ω thuy·∫øt khoa h·ªçc m·ªõi \"d·∫´n ch√∫ng ta t·ªõi m·ªôt b·ª©c tranh m·ªõi v√† h·∫øt s·ª©c kh√°c bi·ªát v·ªÅ v≈© tr·ª• v√† v·ªã tr√≠ c·ªßa ch√∫ng ta trong n√≥\"',\n",
              " 'uit_000792': 'S·ª± d≈©ng c·∫£m, can tr∆∞·ªùng c·ªông v·ªõi tr√≠ tu·ªá, khi·∫øu h√†i h∆∞·ªõc',\n",
              " 'uit_000793': '√îng l√† m·ªôt nh√† khoa h·ªçc v√† l√† m·ªôt ng∆∞·ªùi ƒë√†n √¥ng tuy·ªát v·ªùi, ng∆∞·ªùi m√† nh·ªØng c·ªëng hi·∫øn v√† di s·∫£n c·ªßa m√¨nh s·∫Ω s·ªëng m√£i nhi·ªÅu nƒÉm n·ªØa',\n",
              " 'uit_000794': '√îng l√† m·ªôt nh√† khoa h·ªçc v√† l√† m·ªôt ng∆∞·ªùi ƒë√†n √¥ng tuy·ªát v·ªùi, ng∆∞·ªùi m√† nh·ªØng c·ªëng hi·∫øn v√† di s·∫£n c·ªßa m√¨nh s·∫Ω s·ªëng m√£i nhi·ªÅu nƒÉm n·ªØa',\n",
              " 'uit_000795': 'V≈© tr·ª• s·∫Ω ch·∫≥ng c√≥ nhi·ªÅu √Ω nghƒ©a n·∫øu nh∆∞ ƒë√≥ kh√¥ng ph·∫£i l√† m√°i nh√† ch·ªü che cho nh·ªØng ng∆∞·ªùi b·∫°n y√™u th∆∞∆°ng',\n",
              " 'uit_000796': 'kho·∫£ng c√°ch ƒëi b·ªô c·ªßa Hawking t·ªõi B·ªô m√¥n To√°n h·ªçc ·ª®ng d·ª•ng v√† V·∫≠t l√Ω L√Ω thuy·∫øt',\n",
              " 'uit_000797': 'th√°ng 4 nƒÉm 1979',\n",
              " 'uit_000798': 'd·ª± c√°c h·ªôi ngh·ªã v√† li√™n quan ƒë·∫øn v·∫≠t l√Ω',\n",
              " 'uit_000799': 'th√°ng 4 nƒÉm 1979',\n",
              " 'uit_000800': 'd·ª± c√°c h·ªôi ngh·ªã v√† li√™n quan ƒë·∫øn v·∫≠t l√Ω',\n",
              " 'uit_000801': 'b·ªánh t·∫≠t v√† nh·ªØng th√°ch th·ª©c v·ªÅ c∆° th·ªÉ c·ªßa m√¨nh',\n",
              " 'uit_000802': 'S·ª± t√†n t·∫≠t c·ªßa √¥ng c√≥ nghƒ©a l√† tr√°ch nhi·ªám c·ªßa gia ƒë√¨nh ƒë·∫∑t tr√™n to√†n b·ªô ƒë√¥i vai ng∆∞·ªùi v·ª£ ng√†y c√†ng c·∫£m th·∫•y qu√° t·∫£i c·ªßa √¥ng',\n",
              " 'uit_000803': 'b·ªánh t·∫≠t v√† nh·ªØng th√°ch th·ª©c v·ªÅ c∆° th·ªÉ c·ªßa m√¨nh, th·∫≠m ch√≠ - trong m·ªôt ti·ªÅn l·ªá ƒë∆∞·ª£c ƒë·∫∑t ra trong th·ªùi gian t√°n t·ªânh Jane',\n",
              " 'uit_000804': 'm·ªôt sinh vi√™n sau ƒë·∫°i h·ªçc ho·∫∑c sau ti·∫øn sƒ© ƒë·∫øn s·ªëng v·ªõi h·ªç v√† gi√∫p ƒë·ª° chƒÉm s√≥c Hawking',\n",
              " 'uit_001209': 'K√∂niggr√§tz',\n",
              " 'uit_001210': 'th·ªëng nh·∫•t n∆∞·ªõc ƒê·ª©c',\n",
              " 'uit_001211': 'nƒÉm 1870',\n",
              " 'uit_001212': 'ng√†y 18 th√°ng 1 nƒÉm 1871',\n",
              " 'uit_001213': 'Th·ªß t∆∞·ªõng Otto von Bismarck',\n",
              " 'uit_001214': 'Wilhelm I',\n",
              " 'uit_001215': 'th·ªëng nh·∫•t n∆∞·ªõc ƒê·ª©c',\n",
              " 'uit_001216': 'Khoa h·ªçc, c√¥ng ngh·ªá, gi√°o d·ª•c',\n",
              " 'uit_001217': 'qu√¢n s·ª± v√† kinh t·∫ø',\n",
              " 'uit_001218': '65 tri·ªáu ng∆∞·ªùi',\n",
              " 'uit_001219': '65 tri·ªáu ng∆∞·ªùi',\n",
              " 'uit_001220': 'b·∫°i tr·∫≠n trong cu·ªôc Chi·∫øn tranh th·∫ø gi·ªõi l·∫ßn th·ª© nh·∫•t',\n",
              " 'uit_001221': 'qu√¢n s·ª± v√† kinh t·∫ø',\n",
              " 'uit_001222': 'm·∫´u m·ª±c',\n",
              " 'uit_001223': '65 tri·ªáu ng∆∞·ªùi',\n",
              " 'uit_001224': 't·∫°o d·ª±ng m·ªôt gu·ªìng m√°y',\n",
              " 'uit_001225': 'm·ªôt v∆∞∆°ng qu·ªëc ƒë·ªôc l·∫≠p, ƒëo√†n k·∫øt v√† h√πng m·∫°nh',\n",
              " 'uit_001226': 'Tuy·ªÉn h·∫ßu t∆∞·ªõc (Elector) Friedrich Wilhelm I',\n",
              " 'uit_001227': '30.000 ng∆∞·ªùi',\n",
              " 'uit_001228': 'Friedrich Wilhelm I',\n",
              " 'uit_001229': '48 nƒÉm',\n",
              " 'uit_001230': '48 nƒÉm',\n",
              " 'uit_001231': 'Friedrich III',\n",
              " 'uit_001232': 'Ng√†y 18/1/1701',\n",
              " 'uit_001233': 'Friedrich II',\n",
              " 'uit_001234': 'n·∫øu phong Friedrich l√†m vua th√¨ c√°c Tuy·ªÉn h·∫ßu t∆∞·ªõc c·ªßa Hannover, Bayern v√† Sachsen c≈©ng s·∫Ω mu·ªën l√†m vua',\n",
              " 'uit_001235': 'Ho√†ng ƒë·∫ø c·ªßa ƒê·∫ø qu·ªëc La M√£ Th·∫ßn th√°nh',\n",
              " 'uit_001236': 'Ng√†y 18/1/1701',\n",
              " 'uit_001237': 'n·∫øu phong Friedrich l√†m vua th√¨ c√°c Tuy·ªÉn h·∫ßu t∆∞·ªõc c·ªßa Hannover, Bayern v√† Sachsen c≈©ng s·∫Ω mu·ªën l√†m vua',\n",
              " 'uit_001238': 'Friedrich Wilhelm I',\n",
              " 'uit_001239': 'c√°c c√¥ng tr√¨nh ki·∫øn tr√∫c xa hoa phung ph√≠',\n",
              " 'uit_001240': 'c√°c c√¥ng tr√¨nh ki·∫øn tr√∫c xa hoa phung ph√≠',\n",
              " 'uit_001241': 'nh√† m√°y l√†m thu·ªëc s√∫ng, l√≤ ƒë√∫c ƒë·∫°i b√°c, kho v≈© kh√≠, doanh tr·∫°i qu√¢n ƒë·ªôi',\n",
              " 'uit_001242': 'm·ª•c ti√™u c·ªßa Ph·ªï l√† tr·ªü n√™n m·ªôt c∆∞·ªùng qu·ªëc qu√¢n s·ª±',\n",
              " 'uit_001243': 'Vua Friedrich I',\n",
              " 'uit_001244': 'Friedrich Wilhelm I',\n",
              " 'uit_001245': 'nh√† m√°y l√†m thu·ªëc s√∫ng, l√≤ ƒë√∫c ƒë·∫°i b√°c, kho v≈© kh√≠, doanh tr·∫°i qu√¢n ƒë·ªôi',\n",
              " 'uit_001246': 'm·ª•c ti√™u c·ªßa Ph·ªï l√† tr·ªü n√™n m·ªôt c∆∞·ªùng qu·ªëc qu√¢n s·ª±',\n",
              " 'uit_001247': 'x√¢y d·ª±ng ch·ªâ v·ªõi m·ª•c ƒë√≠ch qu√¢n s·ª±',\n",
              " 'uit_001248': 'Friedrich Wilhelm I',\n",
              " 'uit_001249': 't·ªânh Silesia',\n",
              " 'uit_001250': 'nƒÉm 1740',\n",
              " 'uit_001251': 'v√†i th√°ng',\n",
              " 'uit_001252': 'ƒê·∫ø qu·ªëc La M√£ Th·∫ßn th√°nh',\n",
              " 'uit_001253': 'Friedrich ƒê·∫°i ƒë·∫ø',\n",
              " 'uit_001254': 'v√†i th√°ng',\n",
              " 'uit_001255': 't·ªânh Silesia',\n",
              " 'uit_001256': 'ƒê·∫ø qu·ªëc La M√£ Th·∫ßn th√°nh',\n",
              " 'uit_001257': 'Friedrich ƒê·∫°i ƒë·∫ø',\n",
              " 'uit_001258': 'ti·ªÅm l·ª±c c·ªët l√µi',\n",
              " 'uit_001259': 'v∆∞∆°ng qu·ªëc qu√¢n s·ª± kh·∫Øc kh·ªï',\n",
              " 'uit_001260': 'ch√≠nh s√°ch ngo·∫°i giao m·ªÅm d·∫ªo, s·∫µn s√†ng li√™n minh v·ªõi b·∫•t k·ª≥ th·∫ø l·ª±c n√†o',\n",
              " 'uit_001261': 'ngh√®o t√∫ng',\n",
              " 'uit_001262': 'kh√¥ng c√≥ ƒë·∫•t canh t√°c, s·ªëng cu·ªôc ƒë·ªùi v√¥ c√πng c·ª±c kh·ªï',\n",
              " 'uit_001263': 'ti·ªÅm l·ª±c c·ªët l√µi',\n",
              " 'uit_001264': 'ƒê·∫•t kh√¥ c·∫±n, kh√¥ng c√≥ kho√°ng s·∫£n',\n",
              " 'uit_001265': 'ch√≠nh s√°ch ngo·∫°i giao m·ªÅm d·∫ªo, s·∫µn s√†ng li√™n minh v·ªõi b·∫•t k·ª≥ th·∫ø l·ª±c n√†o',\n",
              " 'uit_001266': 'ph·ª•c t√πng, l√†m vi·ªác v√† hy sinh',\n",
              " 'uit_001267': 'm·ªôt ƒë·ªôi qu√¢n c√≥ qu·ªëc gia',\n",
              " 'uit_001268': 'ph·ª•c t√πng, l√†m vi·ªác v√† hy sinh',\n",
              " 'uit_001269': 'qu√¢n v∆∞∆°ng',\n",
              " 'uit_001270': 'ƒë·∫ßu √≥c h·∫πp h√≤i v√† qua qu√¢n ƒë·ªôi c√≥ k·ª∑ lu·∫≠t m·ªôt c√°ch t√†n b·∫°o',\n",
              " 'uit_001271': 'Hai ph·∫ßn ba, v√† c√≥ l√∫c nƒÉm ph·∫ßn s√°u',\n",
              " 'uit_001272': 'nƒÉm ph·∫ßn s√°u',\n",
              " 'uit_001273': 'c√¥ng qu·ªëc Schleswig v√† Holstein',\n",
              " 'uit_001274': 'tr·∫≠n Sedan',\n",
              " 'uit_001275': 'c√¥ng qu·ªëc Schleswig v√† Holstein',\n",
              " 'uit_001276': 'gi·∫£i t√°n ngh·ªã vi·ªán r·ªìi t·ª± huy ƒë·ªông ngu·ªìn kinh ph√≠',\n",
              " 'uit_001277': '√Åo',\n",
              " 'uit_001278': 'ƒêan M·∫°ch',\n",
              " 'uit_001279': 'gi·∫£i t√°n ngh·ªã vi·ªán r·ªìi t·ª± huy ƒë·ªông ngu·ªìn kinh ph√≠',\n",
              " 'uit_001280': 'ph√°t ƒë·ªông ba cu·ªôc chi·∫øn',\n",
              " 'uit_001281': '√Åo',\n",
              " 'uit_001282': 'tr·∫≠n Sedan',\n",
              " 'uit_001283': 'Hanover, Hesse, Nassua, Frankfurt v√† Elbe',\n",
              " 'uit_001284': 'ch∆∞a ƒë·∫ßy 2 th√°ng',\n",
              " 'uit_001285': 'v∆∞∆°ng qu·ªëc Bayern',\n",
              " 'uit_001286': 'Napol√©on III',\n",
              " 'uit_001287': 'Hanover, Hesse, Nassua, Frankfurt v√† Elbe',\n",
              " 'uit_001288': 'nƒÉm 1870',\n",
              " 'uit_001289': 'v≈© trang t·ªët h∆°n, t·ªï ch·ª©c cao h∆°n v√† chi·∫øn ƒë·∫•u t·ªët h∆°n',\n",
              " 'uit_001290': 'ch·ªâ do Th∆∞·ª£ng ƒë·∫ø trao cho, ch·ª© kh√¥ng ph·∫£i t·ª´ ngh·ªã vi·ªán ho·∫∑c qua d√¢n ch√∫ng',\n",
              " 'uit_001291': 'ch·∫ø ƒë·ªô chuy√™n ch·∫ø qu√¢n phi·ªát',\n",
              " 'uit_001292': 'ƒë·∫°i di·ªán nh√¢n d√¢n b√†n c√£i cho h·∫£ d·∫° ho·∫∑c m·∫∑c c·∫£ quy·ªÅn l·ª£i nh·ªè nhoi cho giai c·∫•p m√† h·ªç l√†m ƒë·∫°i di·ªán',\n",
              " 'uit_001293': 'Ho√†ng ƒë·∫ø Wilhelm II',\n",
              " 'uit_001294': 'n∆∞·ªõc Ph·ªï',\n",
              " 'uit_001295': 'ch·ªâ do Th∆∞·ª£ng ƒë·∫ø trao cho, ch·ª© kh√¥ng ph·∫£i t·ª´ ngh·ªã vi·ªán ho·∫∑c qua d√¢n ch√∫ng',\n",
              " 'uit_001296': 'ƒë·∫°i di·ªán nh√¢n d√¢n b√†n c√£i cho h·∫£ d·∫° ho·∫∑c m·∫∑c c·∫£ quy·ªÅn l·ª£i nh·ªè nhoi cho giai c·∫•p m√† h·ªç l√†m ƒë·∫°i di·ªán',\n",
              " 'uit_001297': 'ch·∫ø ƒë·ªô chuy√™n ch·∫ø qu√¢n phi·ªát',\n",
              " 'uit_001298': 'cu·ªôc c√°ch m·∫°ng c√¥ng nghi·ªáp',\n",
              " 'uit_001299': 'ch·∫ø ƒë·ªô qu√¢n phi·ªát',\n",
              " 'uit_001300': 'ch·ªãu tr√°ch nhi·ªám v·ªõi ch√≠nh √¥ng',\n",
              " 'uit_001301': 'ch·∫ø ƒë·ªô chuy√™n ch·∫ø c·ªßa v∆∞∆°ng tri·ªÅu Hohenzollern',\n",
              " 'uit_001302': 'ch·∫ø ƒë·ªô qu√¢n ch·ªß ngh·ªã vi·ªán',\n",
              " 'uit_001303': 'ƒë∆∞·ª£c h∆∞·ªüng l·ª£i t·ª´ cu·ªôc c√°ch m·∫°ng c√¥ng nghi·ªáp',\n",
              " 'uit_001304': 'ch·∫ø ƒë·ªô chuy√™n ch·∫ø c·ªßa v∆∞∆°ng tri·ªÅu Hohenzollern',\n",
              " 'uit_001305': 'ch·ªãu tr√°ch nhi·ªám v·ªõi ch√≠nh √¥ng',\n",
              " 'uit_001306': 't·ª´ ch·ªß v√† th·ª£',\n",
              " 'uit_001307': 'T√¥i ƒë√£ nghi√™n c·ª©u ph√°p ch·∫ø ch·ªß nghƒ©a x√£ h·ªôi c·ªßa Bismarck',\n",
              " 'uit_001308': 'T√¥i ƒë√£ nghi√™n c·ª©u ph√°p ch·∫ø ch·ªß nghƒ©a x√£ h·ªôi c·ªßa Bismarck',\n",
              " 'uit_001309': 'Hitler',\n",
              " 'uit_001310': 'thi·∫øt l·∫≠p ch∆∞∆°ng tr√¨nh an ninh x√£ h·ªôi r·ªông r√£i',\n",
              " 'uit_001311': 'ƒë√°nh gi√° cao',\n",
              " 'uit_001312': 't∆∞·ªõng Paul von Hindenburg',\n",
              " 'uit_001313': 'ƒê·ª©c ho√†ng Wilhelm II',\n",
              " 'uit_001314': 'ƒê·∫ø qu·ªëc √Åo-Hung v√† ƒê·∫ø qu·ªëc Nga',\n",
              " 'uit_001315': 'nƒÉm 1914',\n",
              " 'uit_001316': 't∆∞·ªõng Paul von Hindenburg',\n",
              " 'uit_001317': 'tr·∫≠n Grunwald',\n",
              " 'uit_001318': 'tr·∫≠n Grunwald',\n",
              " 'uit_001319': 'ƒê·∫ø qu·ªëc √Åo-Hung v√† ƒê·∫ø qu·ªëc Nga',\n",
              " 'uit_001320': 'ph√≠a T√¢y l√† m·ªëi ƒëe d·ªça l·ªõn nh·∫•t cho ƒê·∫ø qu·ªëc ƒê·ª©c',\n",
              " 'uit_001321': 'Bismarck khai m·∫°c Ngh·ªã vi·ªán ƒë·∫ßu ti√™n c·ªßa ƒê·∫ø ch·∫ø th·ª© hai v√†o nƒÉm 1871',\n",
              " 'uit_001322': 'c√°c ho√†ng ƒë·∫ø v∆∞∆°ng tri·ªÅu Hohenzollern',\n",
              " 'uit_001323': 'c√°c ho√†ng ƒë·∫ø v∆∞∆°ng tri·ªÅu Hohenzollern',\n",
              " 'uit_001324': 'Cung ƒëi·ªán Sanssouci',\n",
              " 'uit_001325': 'vinh quang c·ªßa ƒê·∫ø qu·ªëc ƒê·ª©c tr∆∞·ªõc ƒë√≥',\n",
              " 'uit_001326': 'T√≤a nh√† Ngh·ªã vi·ªán b·ªã ch√°y',\n",
              " 'uit_001327': 'vinh quang c·ªßa ƒê·∫ø qu·ªëc ƒê·ª©c tr∆∞·ªõc ƒë√≥',\n",
              " 'uit_001328': 'T√≤a nh√† Ngh·ªã vi·ªán b·ªã ch√°y',\n",
              " 'uit_001329': 'Cung ƒëi·ªán Sanssouci',\n",
              " 'uit_001330': 'Nh√† th·ªù Doanh tr·∫°i Potsdam',\n",
              " 'uit_001331': 'Bismarck khai m·∫°c Ngh·ªã vi·ªán ƒë·∫ßu ti√™n c·ªßa ƒê·∫ø ch·∫ø th·ª© hai v√†o nƒÉm 1871',\n",
              " 'uit_001332': 'th√†nh t·ª±u s√°ng ch√≥i m√† ng∆∞·ªùi ƒê·ª©c ƒë·∫°t ƒë∆∞·ª£c',\n",
              " 'uit_001333': 'vi·ªác dung d∆∞·ª°ng ng∆∞·ªùi Do Th√°i v√† ng∆∞·ªùi theo M√°c-x√≠t, t∆∞ t∆∞·ªüng tr·ªçng v·∫≠t ch·∫•t v√† √≠ch k·ª∑ c·ªßa gi·ªõi trung l∆∞u, ·∫£nh h∆∞·ªüng b·∫•t ch√≠nh c·ªßa nh·ªØng k·∫ª \"lu·ªìn c√∫i v√† xu n·ªãnh\" quanh ngai v√†ng Hohenzollern, \"ch√≠nh s√°ch li√™n minh tai h·∫°i\" v·ªõi V∆∞∆°ng tri·ªÅu Habsburg suy ƒë·ªìi v√† ng∆∞·ªùi √ù kh√¥ng ƒë√°ng tin thay v√¨ v·ªõi Anh, thi·∫øu ch√≠nh s√°ch v·ªÅ ch·ªßng t·ªôc v√† x√£ h·ªôi c∆° b·∫£n',\n",
              " 'uit_001334': '\"lu·ªìn c√∫i v√† xu n·ªãnh\" quanh ngai v√†ng Hohenzollern',\n",
              " 'uit_001335': 's·∫Ω kh·∫Øc ph·ª•c',\n",
              " 'uit_001336': 'ƒê·ª©c Qu·ªëc x√£ s·∫Ω kh·∫Øc ph·ª•c',\n",
              " 'uit_001337': 'vi·ªác dung d∆∞·ª°ng ng∆∞·ªùi Do Th√°i v√† ng∆∞·ªùi theo M√°c-x√≠t, t∆∞ t∆∞·ªüng tr·ªçng v·∫≠t ch·∫•t v√† √≠ch k·ª∑ c·ªßa gi·ªõi trung l∆∞u, ·∫£nh h∆∞·ªüng b·∫•t ch√≠nh c·ªßa nh·ªØng k·∫ª \"lu·ªìn c√∫i v√† xu n·ªãnh\" quanh ngai v√†ng Hohenzollern, \"ch√≠nh s√°ch li√™n minh tai h·∫°i\" v·ªõi V∆∞∆°ng tri·ªÅu Habsburg suy ƒë·ªìi v√† ng∆∞·ªùi √ù kh√¥ng ƒë√°ng tin thay v√¨ v·ªõi Anh, thi·∫øu ch√≠nh s√°ch v·ªÅ ch·ªßng t·ªôc v√† x√£ h·ªôi c∆° b·∫£n',\n",
              " 'uit_001665': 'vi·ªác quan s√°t qu·ªπ ƒë·∫°o c·ªßa ch√∫ng s·∫Ω gi√∫p cho vi·ªác x√°c ƒë·ªãnh kh·ªëi l∆∞·ª£ng c·ªßa ch√∫ng',\n",
              " 'uit_001666': 'b·∫°n ƒë·ªìng h√†nh',\n",
              " 'uit_001667': 'ngo·∫°i suy t·ª´ nh·ªØng sao ƒë√¥i',\n",
              " 'uit_001668': 'vi·ªác quan s√°t qu·ªπ ƒë·∫°o c·ªßa ch√∫ng s·∫Ω gi√∫p cho vi·ªác x√°c ƒë·ªãnh kh·ªëi l∆∞·ª£ng c·ªßa ch√∫ng',\n",
              " 'uit_001669': 'ngo·∫°i suy t·ª´ nh·ªØng sao ƒë√¥i',\n",
              " 'uit_001670': 'm·ªôt h·ªá th·ªëng g·ªìm hai ng√¥i sao chuy·ªÉn ƒë·ªông tr√™n qu·ªπ ƒë·∫°o c·ªßa kh·ªëi t√¢m hai ng√¥i sao',\n",
              " 'uit_001671': 'b·∫°n ƒë·ªìng h√†nh',\n",
              " 'uit_001672': 'binary',\n",
              " 'uit_001673': 'ƒê·∫°i H√πng',\n",
              " 'uit_001674': 'h·ªá sao ƒë√¥i',\n",
              " 'uit_001675': 'binary',\n",
              " 'uit_001676': 'ƒë·ªãnh lu·∫≠t h·∫•p d·∫´n',\n",
              " 'uit_001677': 'kho·∫£ng 50 c·∫∑p',\n",
              " 'uit_001678': 'Sir William Herschel',\n",
              " 'uit_001679': 'hi·ªáu ·ª©ng Doppler c·ªßa √°nh s√°ng ph√°t ra',\n",
              " 'uit_001680': 'hi·ªáu ·ª©ng Doppler c·ªßa √°nh s√°ng ph√°t ra',\n",
              " 'uit_001681': 'ƒë∆∞·ª£c ph√¢n bi·ªát b·∫±ng m·ªôt k√≠nh vi·ªÖn v·ªçng ƒë·ªß m·∫°nh',\n",
              " 'uit_001682': 'ƒë∆∞·ª£c ph√¢n bi·ªát b·∫±ng m·ªôt k√≠nh vi·ªÖn v·ªçng ƒë·ªß m·∫°nh',\n",
              " 'uit_001683': 'c√°c ƒë∆∞·ªùng quang ph·ªï trong √°nh s√°ng t·ª´ m·ªói ng√¥i sao ban ƒë·∫ßu b·ªã d·ªãch chuy·ªÉn xanh, sau ƒë√≥ b·ªã d·ªãch chuy·ªÉn ƒë·ªè khi n√≥ ƒë·∫ßu ti√™n di chuy·ªÉn v·ªÅ ph√≠a ch√∫ng ta, r·ªìi l·∫°i di chuy·ªÉn ra xa ch√∫ng ta, trong khi ch√∫ng chuy·ªÉn ƒë·ªông quanh kh·ªëi t√¢m chung, v·ªõi chu k·ª≥ qu·ªπ ƒë·∫°o chung c·ªßa ch√∫ng',\n",
              " 'uit_001684': 'l·ª±c h·∫´p d·∫´n',\n",
              " 'uit_001685': 'nh·ªØng c·∫∑p sao n·∫±m g·∫ßn nhau t·ªõi m·ª©c c√°c ƒë∆∞·ªùng quang ph·ªï trong √°nh s√°ng t·ª´ m·ªói ng√¥i sao ban ƒë·∫ßu b·ªã d·ªãch chuy·ªÉn xanh, sau ƒë√≥ b·ªã d·ªãch chuy·ªÉn ƒë·ªè khi n√≥ ƒë·∫ßu ti√™n di chuy·ªÉn v·ªÅ ph√≠a ch√∫ng ta, r·ªìi l·∫°i di chuy·ªÉn ra xa ch√∫ng ta, trong khi ch√∫ng chuy·ªÉn ƒë·ªông quanh kh·ªëi t√¢m chung, v·ªõi chu k·ª≥ qu·ªπ ƒë·∫°o chung c·ªßa ch√∫ng',\n",
              " 'uit_001686': 'nhanh',\n",
              " 'uit_001687': 'b·ªüi v√¨ ch√∫ng ·ªü g·∫ßn nhau',\n",
              " 'uit_001688': 'nhanh',\n",
              " 'uit_001689': 'kh√° g·∫ßn',\n",
              " 'uit_001690': 'kh√° xa',\n",
              " 'uit_001691': 'Nh·ªØng sao ƒë√¥i th·ªã gi√°c th∆∞·ªùng c√°ch nhau kh√° xa v√† th∆∞·ªùng c√≥ nh·ªØng t·ªëc ƒë·ªô qu·ªπ ƒë·∫°o qu√° nh·ªè ƒë·ªÉ c√≥ th·ªÉ ƒëo ƒë·∫°c ƒë∆∞·ª£c b·∫±ng quang ph·ªï',\n",
              " 'uit_001692': 'kh√° g·∫ßn',\n",
              " 'uit_001693': 'r·∫•t hi·∫øm',\n",
              " 'uit_001694': 'kh√¥ng ph√°t ra b·∫•t k·ª≥ m·ªôt b·ª©c x·∫° ƒëi·ªán t·ª´ n√†o',\n",
              " 'uit_001695': 'sao neutron',\n",
              " 'uit_001696': 'sao neutron',\n",
              " 'uit_001697': 'g·∫•p kho·∫£ng m∆∞·ªùi l·∫ßn',\n",
              " 'uit_001698': 'r·∫•t t·ªëi',\n",
              " 'uit_001699': 'c√°c sao ƒë√¥i dao ƒë·ªông astrometric binaries',\n",
              " 'uit_001700': 'r·∫•t t·ªëi',\n",
              " 'uit_001701': 'h·∫ßu nh∆∞ ti·∫øp x√∫c v·ªõi nhau',\n",
              " 'uit_001702': 'ƒë·ªô l·ªách t√¢m c·ªßa qu·ªπ ƒë·∫°o',\n",
              " 'uit_001703': '100',\n",
              " 'uit_001704': 'ti·∫øp x√∫c',\n",
              " 'uit_001705': '√≠t h∆°n',\n",
              " 'uit_001706': '100',\n",
              " 'uit_001707': 'c√°c h·ªá c√≥ chu k·ª≥ qu·ªπ ƒë·∫°o ng·∫Øn th√¨ c√≥ ƒë·ªô l·ªách t√¢m √≠t h∆°n',\n",
              " 'uit_001708': 'HD 188753 Ab',\n",
              " 'uit_001709': 'm·ªôt',\n",
              " 'uit_001710': 'HD 188753 Ab',\n",
              " 'uit_001711': 'Tr√™n th·ª±c t·∫ø, ƒëa s·ªë c√°c qu·ªπ ƒë·∫°o c·ªßa ch√∫ng kh√¥ng c√¢n b·∫±ng b·ªÅn (h√†nh tinh c√≥ th·ªÉ b·ªã ƒë·∫©y kh·ªèi qu·ªπ ƒë·∫°o c·ªßa n√≥ kh√° nhanh ch√≥ng, c√≥ th·ªÉ b·ªã b·∫Øn ho√†n to√†n kh·ªèi h·ªá hay b·ªã chuy·ªÉn sang m·ªôt qu·ªπ ƒë·∫°o l·ªách v√†o trong hay l√πi ra ngo√†i h∆°n), trong khi nh·ªØng qu·ªπ ƒë·∫°o kh√°c l·∫°i th·ª±c s·ª± kh·∫Øc nghi·ªát cho vi·ªác h√¨nh th√†nh sinh quy·ªÉn b·ªüi v√¨ ch√∫ng c√≥ kh√°c bi·ªát r·∫•t l·ªõn v·ªÅ nhi·ªát ƒë·ªô b·ªÅ m·∫∑t ·ªü nh·ªØng kho·∫£ng c√°ch qu·ªπ ƒë·∫°o kh√°c nhau',\n",
              " 'uit_001712': 'th√°ng 7 nƒÉm 2005',\n",
              " 'uit_001713': 'th√°ng 7 nƒÉm 2005',\n",
              " 'uit_001714': 'c√°c h√†nh tinh c·ªßa c√°c sao ƒë√¥i hay sao ba',\n",
              " 'uit_001858': 'do s·ª± gia tƒÉng c√°c ho·∫°t ƒë·ªông t·∫°o ra c√°c ch·∫•t th·∫£i kh√≠ nh√† k√≠nh, c√°c ho·∫°t ƒë·ªông khai th√°c qu√° m·ª©c c√°c b·ªÉ h·∫•p th·ª• v√† b·ªÉ ch·ª©a kh√≠ nh√† k√≠nh nh∆∞ sinh kh·ªëi, r·ª´ng, c√°c h·ªá sinh th√°i bi·ªÉn, ven b·ªù v√† ƒë·∫•t li·ªÅn kh√°c',\n",
              " 'uit_001859': 'l√† s·ª± thay ƒë·ªïi c·ªßa h·ªá th·ªëng kh√≠ h·∫≠u g·ªìm kh√≠ quy·ªÉn, th·ªßy quy·ªÉn, sinh quy·ªÉn, th·∫°ch quy·ªÉn hi·ªán t·∫°i v√† trong t∆∞∆°ng lai b·ªüi c√°c nguy√™n nh√¢n t·ª± nhi√™n v√† nh√¢n t·∫°o trong m·ªôt giai ƒëo·∫°n nh·∫•t ƒë·ªãnh t√≠nh b·∫±ng th·∫≠p k·ª∑ hay h√†ng tri·ªáu nƒÉm',\n",
              " 'uit_001860': 'kh√≠ quy·ªÉn, th·ªßy quy·ªÉn, sinh quy·ªÉn, th·∫°ch quy·ªÉn',\n",
              " 'uit_001861': 'n√≥ng l√™n to√†n c·∫ßu',\n",
              " 'uit_001862': 'l√† s·ª± thay ƒë·ªïi c·ªßa h·ªá th·ªëng kh√≠ h·∫≠u g·ªìm kh√≠ quy·ªÉn, th·ªßy quy·ªÉn, sinh quy·ªÉn, th·∫°ch quy·ªÉn hi·ªán t·∫°i v√† trong t∆∞∆°ng lai b·ªüi c√°c nguy√™n nh√¢n t·ª± nhi√™n v√† nh√¢n t·∫°o trong m·ªôt giai ƒëo·∫°n nh·∫•t ƒë·ªãnh t√≠nh b·∫±ng th·∫≠p k·ª∑ hay h√†ng tri·ªáu nƒÉm',\n",
              " 'uit_001863': 'United Nations',\n",
              " 'uit_001864': 'C√¥ng ∆∞·ªõc Khung c·ªßa Li√™n h·ª£p Qu·ªëc v·ªÅ Bi·∫øn ƒë·ªïi Kh√≠ h·∫≠u',\n",
              " 'uit_001865': 'do t√°c ƒë·ªông c·ªßa ho·∫°t ƒë·ªông con ng∆∞·ªùi',\n",
              " 'uit_001866': '·∫•m l√™n to√†n c·∫ßu',\n",
              " 'uit_001867': 'l√† s·ª± thay ƒë·ªïi c·ªßa kh√≠ h·∫≠u m√† ho·∫∑c tr·ª±c ti·∫øp ho·∫∑c gi√°n ti·∫øp do t√°c ƒë·ªông c·ªßa ho·∫°t ƒë·ªông con ng∆∞·ªùi d·∫´n ƒë·∫øn thay ƒë·ªïi th√†nh ph·∫ßn kh√≠ quy·ªÉn to√†n c·∫ßu v√† ngo√†i ra l√† nh·ªØng bi·∫øn thi√™n t·ª± nhi√™n c·ªßa kh√≠ h·∫≠u ƒë∆∞·ª£c quan s√°t tr√™n m·ªôt chu k·ª≥ th·ªùi gian d√†i',\n",
              " 'uit_001868': 'Climate justice',\n",
              " 'uit_001869': 'm·ªôt thu·∫≠t ng·ªØ s·ª≠ d·ª•ng cho khung s·ª± n√≥ng l√™n to√†n c·∫ßu c√≥ li√™n quan t·ªõi v·∫•n ƒë·ªÅ v·ªÅ ƒë·∫°o ƒë·ª©c, v√† ch√≠nh tr·ªã, ch·ª© kh√¥ng ch·ªâ ƒë∆°n thu·∫ßn l√† ho√†n to√†n v·ªÅ m√¥i tr∆∞·ªùng, ho·∫∑c thi√™n nhi√™n ƒë∆°n thu·∫ßn',\n",
              " 'uit_001870': 'l√† m·ªôt thu·∫≠t ng·ªØ s·ª≠ d·ª•ng cho khung s·ª± n√≥ng l√™n to√†n c·∫ßu c√≥ li√™n quan t·ªõi v·∫•n ƒë·ªÅ v·ªÅ ƒë·∫°o ƒë·ª©c, v√† ch√≠nh tr·ªã, ch·ª© kh√¥ng ch·ªâ ƒë∆°n thu·∫ßn l√† ho√†n to√†n v·ªÅ m√¥i tr∆∞·ªùng, ho·∫∑c thi√™n nhi√™n ƒë∆°n thu·∫ßn',\n",
              " 'uit_001871': 'v√¨ kh·ªëi l∆∞·ª£ng l·ªõn',\n",
              " 'uit_001872': 'h√†ng th·∫ø k·ª∑ ho·∫∑c l√¢u h∆°n',\n",
              " 'uit_001873': 'thay ƒë·ªïi b·ª©c x·∫° kh√≠ quy·ªÉn, bao g·ªìm c√°c qu√° tr√¨nh nh∆∞ bi·∫øn ƒë·ªïi b·ª©c x·∫° m·∫∑t tr·ªùi, ƒë·ªô l·ªách qu·ªπ ƒë·∫°o c·ªßa Tr√°i ƒê·∫•t, qu√° tr√¨nh ki·∫øn t·∫°o n√∫i, ki·∫øn t·∫°o tr√¥i d·∫°t l·ª•c ƒë·ªãa v√† s·ª± thay ƒë·ªïi n·ªìng ƒë·ªô kh√≠ nh√† k√≠nh',\n",
              " 'uit_001874': 'c√≥ th·ªÉ m·∫•t h√†ng th·∫ø k·ª∑ ho·∫∑c l√¢u',\n",
              " 'uit_001875': 'ch·∫≠m',\n",
              " 'uit_001876': 'ƒë·∫°i d∆∞∆°ng v√† ch·ªèm bƒÉng, ph·∫£n ·ª©ng ch·∫≠m v·ªõi bi·∫øn ƒë·ªïi b·ª©c x·∫° m·∫∑t tr·ªùi v√¨ kh·ªëi l∆∞·ª£ng l·ªõn',\n",
              " 'uit_001877': 'n·ªÅn t·∫£ng',\n",
              " 'uit_001878': 'v√†i nƒÉm ƒë·∫øn v√†i th·∫≠p ni√™n',\n",
              " 'uit_001879': 'ƒë√≥ng vai tr√≤ quan tr·ªçng trong s·ª± t√°i ph√¢n b·ªë nhi·ªát trong ƒë·∫°i d∆∞∆°ng tr√™n th·∫ø gi·ªõi',\n",
              " 'uit_001880': 'v√†i nƒÉm ƒë·∫øn v√†i th·∫≠p ni√™n',\n",
              " 'uit_001881': '3 ki·ªÉu',\n",
              " 'uit_001882': 'g√¢y ra nh·ªØng thay ƒë·ªïi v·ªÅ s·ª± ph√¢n b·ªë nƒÉng l∆∞·ª£ng m·∫∑t tr·ªùi theo m√πa tr√™n b·ªÅ m·∫∑t Tr√°i ƒê·∫•t v√† c√°ch n√≥ ƒë∆∞·ª£c ph√¢n b·ªë tr√™n to√†n c·∫ßu',\n",
              " 'uit_001883': 'thay ƒë·ªïi qu·ªπ ƒë·∫°o l·ªách t√¢m c·ªßa Tr√°i ƒê·∫•t, thay ƒë·ªïi tr·ª•c quay, v√† ti·∫øn ƒë·ªông c·ªßa tr·ª•c Tr√°i ƒê·∫•t',\n",
              " 'uit_001884': '3 ki·ªÉu',\n",
              " 'uit_001885': 'c√≥ th·ªÉ g√¢y bi·∫øn ƒë·ªïi m·∫°nh m·∫Ω v·ªÅ s·ª± ph√¢n b·ªë c√°c m√πa v√† ƒë·ªãa l√Ω',\n",
              " 'uit_001886': '·∫£nh h∆∞·ªüng m·∫°nh m·∫Ω ƒë·∫øn kh√≠ h·∫≠u v√† m·ªëi t∆∞∆°ng quan c·ªßa ch√∫ng v·ªõi c√°c chu k·ª≥ bƒÉng h√† v√† gian bƒÉng, quan h·ªá c·ªßa ch√∫ng v·ªõi s·ª± ph√°t tri·ªÉn v√† tho√°i lui c·ªßa Sahara, v√† ƒë·ªëi v·ªõi s·ª± xu·∫•t hi·ªán c·ªßa ch√∫ng trong c√°c ƒë·ªãa t·∫ßng',\n",
              " 'uit_001887': 'c√≥ th·ªÉ g√¢y bi·∫øn ƒë·ªïi m·∫°nh m·∫Ω v·ªÅ s·ª± ph√¢n b·ªë c√°c m√πa v√† ƒë·ªãa l√Ω',\n",
              " 'uit_001888': 'qu·ªπ ƒë·∫°o Tr√°i ƒê·∫•t',\n",
              " 'uit_001889': 'g√¢y ra s·ª± ·∫•m l√™n to√†n c·∫ßu v√† tuy·ªát ch·ªßng h√†ng lo·∫°t',\n",
              " 'uit_001890': 'Nhi·ªát ƒë·ªô to√†n c·∫ßu gi·∫£m kho·∫£ng 0,5 ¬∞C (0.9 ¬∞F)',\n",
              " 'uit_001891': 'g√¢y ra l√†m m√°t (b·∫±ng m·ªôt ph·∫ßn ngƒÉn ch·∫∑n s·ª± l√¢y truy·ªÅn c·ªßa b·ª©c x·∫° m·∫∑t tr·ªùi ƒë·∫øn b·ªÅ m·∫∑t Tr√°i ƒê·∫•t) trong th·ªùi gian m·ªôt v√†i nƒÉm',\n",
              " 'uit_001892': 'g√¢y ra l√†m m√°t (b·∫±ng m·ªôt ph·∫ßn ngƒÉn ch·∫∑n s·ª± l√¢y truy·ªÅn c·ªßa b·ª©c x·∫° m·∫∑t tr·ªùi ƒë·∫øn b·ªÅ m·∫∑t Tr√°i ƒê·∫•t) trong th·ªùi gian m·ªôt v√†i nƒÉm',\n",
              " 'uit_001893': 'v·ª• phun tr√†o nƒÉm 1912 c·ªßa n√∫i l·ª≠a Novarupta',\n",
              " 'uit_001894': 'Novarupta',\n",
              " 'uit_001895': 'v·ª• phun tr√†o c·ªßa n√∫i l·ª≠a Pinatubo',\n",
              " 'uit_001896': 'g√¢y ra s·ª± ·∫•m l√™n to√†n c·∫ßu v√† tuy·ªát ch·ªßng h√†ng lo·∫°t',\n",
              " 'uit_001897': 's·ª± h√¨nh th√†nh eo ƒë·∫•t Panama',\n",
              " 'uit_001898': 'kho·∫£ng 300 ƒë·∫øn 365 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_001899': 'ƒê·∫°i T√¢y D∆∞∆°ng v√† Th√°i B√¨nh D∆∞∆°ng',\n",
              " 'uit_001900': '·∫£nh h∆∞·ªüng r·∫•t m·∫°nh m·∫Ω ƒë·∫øn c√°c ch·∫ø ƒë·ªô ƒë·ªông l·ª±c h·ªçc c·ªßa ƒë·∫°i d∆∞∆°ng c·ªßa h·∫£i l∆∞u Gulf Stream v√† ƒë√£ l√†m cho b·∫Øc b√°n c·∫ßu b·ªã ph·ªß bƒÉng',\n",
              " 'uit_001901': 'kho·∫£ng 300 ƒë·∫øn 365 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_001902': 'ƒë√≥ng vai tr√≤ quan tr·ªçng trong vi·ªác ki·ªÉm so√°t s·ª± truy·ªÅn nhi·ªát v√† ƒë·ªô ·∫©m tr√™n to√†n c·∫ßu v√† h√¨nh th√†nh n√™n kh√≠ h·∫≠u to√†n c·∫ßu',\n",
              " 'uit_001903': 'V·ªã tr√≠ c·ªßa c√°c bi·ªÉn ƒë√≥ng vai tr√≤ quan tr·ªçng trong vi·ªác ki·ªÉm so√°t s·ª± truy·ªÅn nhi·ªát v√† ƒë·ªô ·∫©m tr√™n to√†n c·∫ßu v√† h√¨nh th√†nh n√™n kh√≠ h·∫≠u to√†n c·∫ßu',\n",
              " 'uit_001904': '·∫£nh h∆∞·ªüng r·∫•t m·∫°nh m·∫Ω ƒë·∫øn c√°c ch·∫ø ƒë·ªô ƒë·ªông l·ª±c h·ªçc c·ªßa ƒë·∫°i d∆∞∆°ng c·ªßa h·∫£i l∆∞u Gulf Stream v√† ƒë√£ l√†m cho b·∫Øc b√°n c·∫ßu b·ªã ph·ªß bƒÉng',\n",
              " 'uit_001905': 'nhi·ªát ƒë·ªô, l∆∞·ª£ng tuy·∫øt r∆°i, l∆∞·ª£ng n∆∞·ªõc n·∫±m gi·ªØa v√† d∆∞·ªõi l·ªõp bƒÉng',\n",
              " 'uit_001906': 'm·ªôt s√¥ng bƒÉng v·ªën h√¨nh th√†nh t·ª´ nhi·ªÅu s√¥ng bƒÉng nh·ªè kh√°c nhau ph·∫£i t·ªën trung b√¨nh h√†ng th·∫ø k·ªâ ho·∫∑c th·∫≠m ch√≠ l√¢u h∆°n ƒë·ªÉ tan ra b·ªüi t√°c ƒë·ªông c·ªßa nh·ªØng bi·∫øn ƒë·ªïi ng·∫Øn h·∫°n c·ªßa v√πng',\n",
              " 'uit_001907': 'h√†ng th·∫ø k·ªâ ho·∫∑c th·∫≠m ch√≠ l√¢u h∆°n',\n",
              " 'uit_001908': 'S·ª± thay ƒë·ªïi v·ªÅ nhi·ªát ƒë·ªô, l∆∞·ª£ng tuy·∫øt r∆°i, l∆∞·ª£ng n∆∞·ªõc n·∫±m gi·ªØa v√† d∆∞·ªõi l·ªõp bƒÉng',\n",
              " 'uit_001909': 'S·ª± thay ƒë·ªïi v·ªÅ nhi·ªát ƒë·ªô, l∆∞·ª£ng tuy·∫øt r∆°i, l∆∞·ª£ng n∆∞·ªõc n·∫±m gi·ªØa v√† d∆∞·ªõi l·ªõp bƒÉng',\n",
              " 'uit_001910': 'thu h·∫πp ƒë√°ng k·ªÉ, v·ªõi s·ª± l√πi d·∫ßn m·∫°nh c·ªßa nh·ªØng s√¥ng bƒÉng trong nh·ªØng nƒÉm 1940, c√≥ ƒëi·ªÅu ki·ªán ·ªïn ƒë·ªãnh ho·∫∑c ph√°t tri·ªÉn trong nh·ªØng nƒÉm 1920 v√† 1970, v√† m·ªôt l·∫ßn n·ªØa b·∫Øt ƒë·∫ßu gi·∫£m t·ª´ gi·ªØa nh·ªØng nƒÉm 1980 ƒë·∫øn nay',\n",
              " 'uit_001911': 'gi·∫£m',\n",
              " 'uit_001912': 'kho·∫£ng 445.000 km2',\n",
              " 'uit_001913': 't·ª´ nh·ªØng nƒÉm 1970',\n",
              " 'uit_001914': 'kho·∫£ng 240.000 km2',\n",
              " 'uit_001915': 'nh·ªØng nƒÉm 1970',\n",
              " 'uit_001916': 'T·ªï ch·ª©c Gi√°m s√°t S√¥ng bƒÉng Th·∫ø gi·ªõi',\n",
              " 'uit_001917': 'kho·∫£ng 445.000 km2',\n",
              " 'uit_001918': 'thay ƒë·ªïi qu·ªπ ƒë·∫°o, nh·ªØng ph·∫£n ·ª©ng nh∆∞ tƒÉng ho·∫∑c gi·∫£m c√°c d·∫£i bƒÉng l·ª•c ƒë·ªãa v√† thay ƒë·ªïi m·ª±c n∆∞·ªõc bi·ªÉn t·∫°o n√™n kh√≠ h·∫≠u',\n",
              " 'uit_001919': 'thay ƒë·ªïi qu·ªπ ƒë·∫°o',\n",
              " 'uit_001920': 'kho·∫£ng 11.700 nƒÉm',\n",
              " 'uit_001921': 'Dansgaard-Oeschger',\n",
              " 'uit_001922': 'kho·∫£ng 11.700 nƒÉm',\n",
              " 'uit_001923': 'kho·∫£ng 3 tri·ªáu nƒÉm tr∆∞·ªõc',\n",
              " 'uit_001924': 'hi·ªán t∆∞·ª£ng sa m·∫°c ho√°',\n",
              " 'uit_001925': 'nhi·ªÅu lo√†i nhanh ch√≥ng bi·∫øn m·∫•t',\n",
              " 'uit_001926': 'kh√≠ h·∫≠u',\n",
              " 'uit_001927': 'd·∫´n ƒë·∫øn tƒÉng tr∆∞·ªüng th·ª±c v·∫≠t ƒë∆∞·ª£c c·∫£i thi·ªán v√† k√©o theo vi·ªác h·∫•p th·ª• nhi·ªÅu CO2 trong kh√¥ng kh√≠ h∆°n',\n",
              " 'uit_001928': 'nh·ªØng ch·ªâ s·ªë quan tr·ªçng v·ªÅ s·ª± thay ƒë·ªïi l∆∞·ª£ng CO2 qua h√†ng ng√†n nƒÉm, v√† ti·∫øp t·ª•c cung c·∫•p nh·ªØng th√¥ng tin c√≥ gi√° tr·ªã v·ªÅ s·ª± kh√°c nhau gi·ªØa ƒëi·ªÅu ki·ªán kh√¥ng kh√≠ c·ªï x∆∞a v√† hi·ªán ƒë·∫°i',\n",
              " 'uit_001929': 'C√°c th√¥ng tin t·ª´ vi·ªác ph√¢n t√≠ch ph·∫ßn l√µi bƒÉng khoan t·ª´ m·ªôt kh·ªëi bƒÉng nh∆∞ kh·ªëi bƒÉng Nam C·ª±c',\n",
              " 'uit_001930': 'Kh√¥ng kh√≠ b·ªã m·∫Øc k·∫πt ·ªü d·∫°ng bong b√≥ng trong bƒÉng',\n",
              " 'uit_001931': 'CO2',\n",
              " 'uit_001932': 'Kh√¥ng kh√≠ b·ªã m·∫Øc k·∫πt ·ªü d·∫°ng bong b√≥ng trong bƒÉng',\n",
              " 'uit_001933': 'C√°c th√¥ng tin t·ª´ vi·ªác ph√¢n t√≠ch ph·∫ßn l√µi bƒÉng khoan t·ª´ m·ªôt kh·ªëi bƒÉng nh∆∞ kh·ªëi bƒÉng Nam C·ª±c',\n",
              " 'uit_001934': 'l√† b·ªô m√¥n khoa h·ªçc hi·ªán ƒë·∫°i nghi√™n c·ª©u v·ªÅ lƒ©nh v·ª±c h√≥a th·∫°ch ·ªü k√≠ch th∆∞·ªõc t·∫ø b√†o, bao g·ªìm c·∫£ ph·∫•n hoa',\n",
              " 'uit_001935': 'ƒë·ªÉ suy ra s·ª± ph√¢n b·ªë ƒë·ªãa l√Ω c·ªßa c√°c lo√†i th·ª±c v·∫≠t t·ª´ng thay ƒë·ªïi theo ƒëi·ªÅu ki·ªán kh√≠ h·∫≠u kh√°c nhau',\n",
              " 'uit_001936': 'suy ra s·ª± ph√¢n b·ªë ƒë·ªãa l√Ω c·ªßa c√°c lo√†i th·ª±c v·∫≠t t·ª´ng thay ƒë·ªïi theo ƒëi·ªÅu ki·ªán kh√≠ h·∫≠u kh√°c nhau',\n",
              " 'uit_001937': 'l√† b·ªô m√¥n khoa h·ªçc hi·ªán ƒë·∫°i nghi√™n c·ª©u v·ªÅ lƒ©nh v·ª±c h√≥a th·∫°ch ·ªü k√≠ch th∆∞·ªõc t·∫ø b√†o, bao g·ªìm c·∫£ ph·∫•n hoa',\n",
              " 'uit_001938': 'l·ªõp ngo√†i c·ªßa ph·∫•n hoa ƒë∆∞·ª£c c·∫•u th√†nh t·ª´ m·ªôt l·ªõp ch·∫•t li·ªáu c√≥ t√≠nh ƒë√†n h·ªìi r·∫•t cao',\n",
              " 'uit_001939': 'c√°c thay ƒë·ªïi ·ªü th·∫ø gi·ªõi th·ª±c v·∫≠t',\n",
              " 'uit_001940': 'l·ªõp ngo√†i c·ªßa ph·∫•n hoa ƒë∆∞·ª£c c·∫•u th√†nh t·ª´ m·ªôt l·ªõp ch·∫•t li·ªáu c√≥ t√≠nh ƒë√†n h·ªìi r·∫•t cao',\n",
              " 'uit_001941': 'c·∫•u tr√∫c di truy·ªÅn kh√¥ng thay ƒë·ªïi ƒë√°ng k·ªÉ qua h√†ng ng√†n nƒÉm',\n",
              " 'uit_001942': 'nh·ªØng v√πng n∆∞·ªõc ng·ªçt v√† tr·∫ßm t√≠ch ƒë·∫•t ƒëai',\n",
              " 'uit_001943': 'vi·ªác nghi√™n c·ª©u d·ª±a tr√™n nh·ªØng lo√†i b·ªç c√°nh c·ª©ng kh√°c nhau s·∫Ω ƒëem l·∫°i ki·∫øn th·ª©c v·ªÅ ph·∫°m vi kh√≠ h·∫≠u hi·ªán t·∫°i, x√°c ƒë·ªãnh ƒë∆∞·ª£c tu·ªïi c·ªßa c√°c tr·∫ßm t√≠ch c√≤n s√≥t l·∫°i, t·ª´ ƒë√≥ c√≥ th·ªÉ suy ra ƒëi·ªÅu ki·ªán kh√≠ h·∫≠u trong qu√° kh·ª©',\n",
              " 'uit_001944': 'kh√¥ng',\n",
              " 'uit_001945': 'm√°y ƒëo ƒë·ªô cao - k·∫øt h·ª£p v·ªõi s·ª± ƒë·ªãnh v·ªã ch√≠nh x√°c c·ªßa c√°c qu·ªπ ƒë·∫°o v·ªá tinh',\n",
              " 'uit_001946': 'th√¥ng qua c√°c d·∫•u v·∫øt tr√™n nh·ªØng r·∫∑ng san h√¥, nh·ªØng l·ªõp tr·∫ßm t√≠ch ven bi·ªÉn, tr√™n th·ªÅm bi·ªÉn, h·∫°t trong ƒë√° v√¥i v√† nh·ªØng di t√≠ch kh·∫£o c·ªï c√≤n s√≥t l·∫°i g·∫ßn b·ªù bi·ªÉn',\n",
              " 'uit_001947': 'm√°y ƒëo th·ªßy tri·ªÅu',\n",
              " 'uit_001948': 's·ª≠ d·ª•ng c√°c m√°y ƒëo th·ªßy tri·ªÅu, c√°c s·ªë li·ªáu ƒëo ƒë∆∞·ª£c ƒë·ªëi chi·∫øu trong th·ªùi gian d√†i ƒë·ªÉ ƒë∆∞a ra m·ªôt m·ª±c n∆∞·ªõc trung b√¨nh d√†i h·∫°n',\n",
              " 'uit_001949': 'm√°y ƒëo ƒë·ªô cao - k·∫øt h·ª£p v·ªõi s·ª± ƒë·ªãnh v·ªã ch√≠nh x√°c c·ªßa c√°c qu·ªπ ƒë·∫°o v·ªá tinh',\n",
              " 'uit_001950': 'm√°y ƒëo ƒë·ªô cao - k·∫øt h·ª£p v·ªõi s·ª± ƒë·ªãnh v·ªã ch√≠nh x√°c c·ªßa c√°c qu·ªπ ƒë·∫°o v·ªá tinh',\n",
              " 'uit_001951': 'ph∆∞∆°ng ph√°p urani v√† cacbon ph√≥ng x·∫°',\n",
              " 'uit_001952': 's·ª≠ d·ª•ng c√°c m√°y ƒëo th·ªßy tri·ªÅu, c√°c s·ªë li·ªáu ƒëo ƒë∆∞·ª£c ƒë·ªëi chi·∫øu trong th·ªùi gian d√†i ƒë·ªÉ ƒë∆∞a ra m·ªôt m·ª±c n∆∞·ªõc trung b√¨nh d√†i h·∫°n',\n",
              " 'uit_001953': 'm√°y ƒëo th·ªßy tri·ªÅu',\n",
              " 'uit_001954': 'th√¥ng qua c√°c d·∫•u v·∫øt tr√™n nh·ªØng r·∫∑ng san h√¥, nh·ªØng l·ªõp tr·∫ßm t√≠ch ven bi·ªÉn, tr√™n th·ªÅm bi·ªÉn, h·∫°t trong ƒë√° v√¥i v√† nh·ªØng di t√≠ch kh·∫£o c·ªï c√≤n s√≥t l·∫°i g·∫ßn b·ªù bi·ªÉn',\n",
              " 'uit_001955': 'ph∆∞∆°ng ph√°p urani v√† cacbon ph√≥ng x·∫°',\n",
              " 'uit_001956': 'Edward I',\n",
              " 'uit_001957': 't·ª´ 1307 cho ƒë·∫øn khi b·ªã l·∫≠t ƒë·ªï v√†o th√°ng 1 nƒÉm 1327',\n",
              " 'uit_001958': 'con g√°i c·ªßa vua Ph√°p Philippe IV',\n",
              " 'uit_001959': 'Edward II',\n",
              " 'uit_001960': '21 th√°ng 9, 1327',\n",
              " 'uit_001961': 'Alphonso',\n",
              " 'uit_001962': 'gi√°i quy·∫øt nh·ªØng cƒÉng th·∫≥ng v∆∞∆°ng quy·ªÅn gi·ªØa Anh v√† Ph√°p',\n",
              " 'uit_001963': 'thu h·ªìi s·∫Øc l·ªánh c·∫£i c√°ch v√† tri·ªáu h·ªìi s·ªßng th·∫ßn c·ªßa m√¨nh',\n",
              " 'uit_001964': 'nƒÉm 1300',\n",
              " 'uit_001965': 'n·∫°n ƒë√≥i lan r·ªông',\n",
              " 'uit_001966': 'l∆∞u ƒë√†y √¥ng ta',\n",
              " 'uit_001967': 'C√°c nam t∆∞·ªõc ƒë∆∞·ª£c trao quy·ªÅn tr·ª•c xu·∫•t Gaveston',\n",
              " 'uit_001968': 'm·ªëi quan h·ªá g·∫ßn th√¢n thi·∫øt g√¢y nhi·ªÅu tranh c√£i v·ªõi Piers Gaveston',\n",
              " 'uit_001969': 'C√°c nam t∆∞·ªõc ƒë∆∞·ª£c trao quy·ªÅn tr·ª•c xu·∫•t Gaveston',\n",
              " 'uit_001970': 'Piers Gaveston',\n",
              " 'uit_001971': 't·ª´ nƒÉm 1300',\n",
              " 'uit_001972': 'b·ªã c√°c hi·ªáp sƒ© c·ªßa v∆∞∆°ng tri·ªÅu m·ªõi √°m s√°t',\n",
              " 'uit_001973': 'Isabella l·∫≠p li√™n minh v·ªõi Roger Mortimer ƒëang b·ªã l∆∞u ƒë√†y, v√† x√¢m l∆∞·ª£c Anh b·∫±ng m·ªôt ƒë·ªôi qu√¢n nh·ªè nƒÉm 1326',\n",
              " 'uit_001974': 'Gia ƒë√¨nh Despenser, ƒë·∫∑c bi·ªát l√† Hugh Despenser tr·∫ª',\n",
              " 'uit_001975': 'th√°ng 1 nƒÉm 1327',\n",
              " 'uit_001976': 'l·∫≠p li√™n minh v·ªõi Roger Mortimer ƒëang b·ªã l∆∞u ƒë√†y, v√† x√¢m l∆∞·ª£c Anh b·∫±ng m·ªôt ƒë·ªôi qu√¢n nh·ªè nƒÉm 1326',\n",
              " 'uit_001977': 'th√°ng 1 nƒÉm 1327',\n",
              " 'uit_001978': 'c√°c hi·ªáp sƒ© c·ªßa v∆∞∆°ng tri·ªÅu m·ªõi √°m s√°t',\n",
              " 'uit_001979': 'th√°ng 1 nƒÉm 1327',\n",
              " 'uit_001980': 'v·ªã vua l∆∞·ªùi nh√°c v√† thi·∫øu nƒÉng l·ª±c, hay ƒë∆°n gi·∫£n ch·ªâ l√† m·ªôt nh√† cai tr·ªã b·∫•t ƒë·∫Øc dƒ© v√† ho√†n to√†n th·∫•t b·∫°i',\n",
              " 'uit_001981': 'nh·ªØng ng∆∞·ªùi ƒë∆∞∆°ng th·ªùi',\n",
              " 'uit_001982': 'Christopher Marlowe',\n",
              " 'uit_001983': 's·ª± ti·∫øn tri·ªÉn tri·ªÉn c·ªßa c√°c t·ªï ch·ª©c Qu·ªëc h·ªôi trong tri·ªÅu ƒë·∫°i c·ªßa √¥ng l√† s·ª± ch·ªâ d·∫•u t√≠ch c·ª±c cho ƒë·∫•t n∆∞·ªõc Anh m√† trong th·ªùi gian d√†i ch∆∞a ƒë·∫°t ƒë∆∞·ª£c',\n",
              " 'uit_001984': 'M·ªëi quan h·ªá gi·ªØa Edward v√† Gaveston',\n",
              " 'uit_001985': 's·ª± ch·ªâ d·∫•u t√≠ch c·ª±c cho ƒë·∫•t n∆∞·ªõc Anh m√† trong th·ªùi gian d√†i ch∆∞a ƒë·∫°t ƒë∆∞·ª£c',\n",
              " 'uit_001986': 'l∆∞·ªùi nh√°c v√† thi·∫øu nƒÉng l·ª±c, hay ƒë∆°n gi·∫£n ch·ªâ l√† m·ªôt nh√† cai tr·ªã b·∫•t ƒë·∫Øc dƒ© v√† ho√†n to√†n th·∫•t b·∫°i',\n",
              " 'uit_001987': 'm·ªëi quan h·ªá ƒë·ªìng t√≠nh luy·∫øn √°i ƒë·ªìn ƒëo√°n c·ªßa hai ng∆∞·ªùi ƒë√†n √¥ng',\n",
              " 'uit_001988': 'm·ªôt nh√† l√£nh ƒë·∫°o qu√¢n s·ª± t√†i nƒÉng',\n",
              " 'uit_001989': 'm·ªôt nh√† l√£nh ƒë·∫°o qu√¢n s·ª± t√†i nƒÉng',\n",
              " 'uit_001990': 'V∆∞∆°ng qu·ªëc Castile',\n",
              " 'uit_001991': 'm·ªôt v·ªã vua ƒë√°ng s·ª£ v√† ƒë√°ng k√≠nh',\n",
              " 'uit_001992': '√¥ng th·ªÉ hi·ªán kh·∫£ nƒÉng ki·ªÉm so√°t quy·ªÅn h√†nh r·ªông l·ªõn c·ªßa c√°c b√° t∆∞·ªõc trong h√†ng ng≈© gi·ªõi qu√Ω t·ªôc Anh',\n",
              " 'uit_001993': 'Eleanor x·ª© Castile',\n",
              " 'uit_001994': 'tuy√™n b·ªë b√° quy·ªÅn ƒë·ªëi v·ªõi n∆∞·ªõc n√†y',\n",
              " 'uit_001995': 'tuy√™n b·ªë b√° quy·ªÅn ƒë·ªëi v·ªõi n∆∞·ªõc n√†y',\n",
              " 'uit_001996': 'tr√™n c∆∞∆°ng v·ªã ch∆∞ h·∫ßu c·∫ßn t·ªè l√≤ng th·∫ßn ph·ª•c h·ªç',\n",
              " 'uit_001997': 'C√°c vua Ph√°p nh·∫•n m·∫°nh r·∫±ng qu·ªëc v∆∞∆°ng Anh tr√™n c∆∞∆°ng v·ªã ch∆∞ h·∫ßu c·∫ßn t·ªè l√≤ng th·∫ßn ph·ª•c h·ªç, tuy nhi√™n √¥ng cho r·∫±ng y√™u c·∫ßu ƒë√≥ l√† m·ªôt s·ª± lƒÉng m·∫° ƒë·∫øn l√≤ng ki√™u h√£nh c·ªßa nh√† vua v√† v·∫•n ƒë·ªÅ n√†y v·∫´n ch∆∞a ƒë∆∞·ª£c gi·∫£i quy·∫øt tri·ªát ƒë·ªÉ',\n",
              " 'uit_001998': 'Vi·ªác Edward cai tr·ªã v√πng Gascony',\n",
              " 'uit_001999': 'nƒÉm 1307',\n",
              " 'uit_002000': 's·ª± th·ªëng tr·ªã c·ªßa Anh ·ªü Scotland',\n",
              " 'uit_002001': 'Vi·ªác Edward cai tr·ªã v√πng Gascony',\n",
              " 'uit_002002': '√¥ng ƒë√°nh thu·∫ø n·∫∑ng v√† y√™u c·∫ßu cung c·∫•p nhi·ªÅu ngu·ªìn l·ª±c ph·ª•c v·ª• chi·∫øn tranh',\n",
              " 'uit_002003': '√¥ng ƒë√°nh thu·∫ø n·∫∑ng v√† y√™u c·∫ßu cung c·∫•p nhi·ªÅu ngu·ªìn l·ª±c ph·ª•c v·ª• chi·∫øn tranh',\n",
              " 'uit_002004': 'nƒÉm 1307',\n",
              " 'uit_002005': 'Edward x·ª© Caernarfon',\n",
              " 'uit_002006': 'ng√†y 25 th√°ng 4 nƒÉm 1284',\n",
              " 'uit_002007': 'L√¢u ƒë√†i Caernarfon',\n",
              " 'uit_002008': 'Edward x·ª© Caernarfon',\n",
              " 'uit_002009': 'ng√†y 25 th√°ng 4 nƒÉm 1284',\n",
              " 'uit_002010': 'L√¢u ƒë√†i Caernarfon ·ªü mi·ªÅn B·∫Øc x·ª© Wales',\n",
              " 'uit_002011': 'ng√†y 25 th√°ng 4 nƒÉm 1284',\n",
              " 'uit_002012': 'n√≥ l√† m·ªôt ƒë·ªãa ƒëi·ªÉm c√≥ t√≠nh bi·ªÉu t∆∞·ª£ng quan tr·ªçng ƒë·ªëi v·ªõi ng∆∞·ªùi Wales b·∫£n ƒë·ªãa, g·∫Øn li·ªÅn v·ªõi l·ªãch s·ª≠ ƒë·∫ø qu·ªëc La M√£, v√† l√¢u ƒë√†i Caernarfon c≈©ng l√† n∆°i ƒë·∫∑t v∆∞∆°ng quy·ªÅn m·ªõi ·ªü mi·ªÅn B·∫Øc x·ª© Wales',\n",
              " 'uit_002013': 'Eleanor',\n",
              " 'uit_002014': 'Alphonso',\n",
              " 'uit_002015': 'd√πng t√™n ti·∫øng Norman v√† Castilla',\n",
              " 'uit_002016': 'Alice de Leygrave',\n",
              " 'uit_002017': 'Alice de Leygrave',\n",
              " 'uit_002018': 'T√™n Edward c√≥ xu·∫•t x·ª© t·ª´ ti·∫øng Anh, li√™n t∆∞·ªüng ƒë·∫øn Th√°nh ng∆∞·ªùi Anglo-Saxon l√† Edward x∆∞ng t·ªôi, v√† ƒë∆∞·ª£c l·ª±a ch·ªçn b·ªüi cha √¥ng thay cho truy·ªÅn th·ªëng d√πng t√™n ti·∫øng Norman v√† Castilla ƒë√£ ƒë∆∞·ª£c ƒë·∫∑t cho c√°c anh trai c·ªßa Edward',\n",
              " 'uit_002019': 'Mariota ho·∫∑c Mary Maunsel',\n",
              " 'uit_002020': 'm·ª•c s∆∞ Giles',\n",
              " 'uit_002021': 'm·ª•c s∆∞ Giles x·ª© Oudenarde',\n",
              " 'uit_002022': 'l√†m qu√¢n s∆∞ cho ri√™ng √¥ng',\n",
              " 'uit_002023': 'r√®n luy·ªán, hu·∫•n luy·ªán c∆∞·ª°i ng·ª±a v√† k·ªπ nƒÉng qu√¢n s·ª± cho Edward',\n",
              " 'uit_002024': 'chi ti√™u trong t∆∞ th·∫•t',\n",
              " 'uit_002025': 'ti·∫øng Ph√°p Anglo-Norman',\n",
              " 'uit_002026': 'c√°c chi ti√™u trong t∆∞ th·∫•t c·ªßa Edward',\n",
              " 'uit_002027': 'n·ªÅn gi√°o d·ª•c C√¥ng gi√°o t·ª´ c√°c tu sƒ© d√≤ng ƒêa Minh',\n",
              " 'uit_002028': 'William x·ª© Blyborough',\n",
              " 'uit_002029': 'Guy Ferre',\n",
              " 'uit_002030': 'm·ªôt tay c∆∞·ª°i ng·ª±a gi·ªèi',\n",
              " 'uit_002031': 'nh·∫°c Wales v√† ƒë√†n crwth m·ªõi v·ª´a ƒë∆∞·ª£c ph√°t minh, v√† phong c·∫ßm',\n",
              " 'uit_002032': 'ƒë·∫•u th∆∞∆°ng',\n",
              " 'uit_002033': 'nh·∫°c Wales v√† ƒë√†n crwth m·ªõi v·ª´a ƒë∆∞·ª£c ph√°t minh, v√† phong c·∫ßm',\n",
              " 'uit_002034': '√¥ng kh√¥ng c√≥ nƒÉng khi·∫øu ho·∫∑c l√† t·∫°i √¥ng kh√¥ng ƒë∆∞·ª£c ph√©p tham gia ƒë·ªÉ ƒë·∫£m b·∫£o an to√†n',\n",
              " 'uit_002035': 'gi√∫p duy tr√¨ n·ªÅn h√≤a b√¨nh l√¢u d√†i gi·ªØa Anh v·ªõi Ph√°p',\n",
              " 'uit_002036': 'NƒÉm 1290',\n",
              " 'uit_002037': 'Hi·ªáp ∆∞·ªõc Birgham',\n",
              " 'uit_002038': 'NƒÉm 1290',\n",
              " 'uit_002039': 'Margaret',\n",
              " 'uit_002040': 'Margaret c·ªßa Na Uy',\n",
              " 'uit_002041': 'Hi·ªáp ∆∞·ªõc Birgham',\n",
              " 'uit_002042': 'NƒÉm 1290',\n",
              " 'uit_002043': 'Margaret t·∫° th·∫ø c√πng nƒÉm',\n",
              " 'uit_002044': 'Margaret t·∫° th·∫ø',\n",
              " 'uit_002045': 'gi√∫p duy tr√¨ n·ªÅn h√≤a b√¨nh l√¢u d√†i gi·ªØa Anh v·ªõi Ph√°p',\n",
              " 'uit_002046': 'Edward',\n",
              " 'uit_002047': 'Edward',\n",
              " 'uit_002048': 'quan h·ªá t·ªët ƒë·∫πp',\n",
              " 'uit_002049': 's·ª± h·ªó tr·ª£ v·ªÅ t√†i ch√≠nh v√† c√°c ch·ª©c danh',\n",
              " 'uit_002050': '1301',\n",
              " 'uit_002051': 'Isabella',\n",
              " 'uit_002052': 'Nh√† vua tham gia chi·∫øn d·ªãch Flanders ch·ªëng l·∫°i Philippe IV',\n",
              " 'uit_002053': 'Isabella',\n",
              " 'uit_002054': 'L√£nh ƒë·ªãa B√° t∆∞·ªõc Chester v√† nh·ªØng v√πng ƒë·∫•t tr√™n kh·∫Øp B·∫Øc Wales',\n",
              " 'uit_002055': 'L√£nh ƒë·ªãa B√° t∆∞·ªõc Chester v√† nh·ªØng v√πng ƒë·∫•t tr√™n kh·∫Øp B·∫Øc Wales',\n",
              " 'uit_002056': 'ch·ªâ huy h·∫≠u qu√¢n trong cu·ªôc bao v√¢y Caerlaverock',\n",
              " 'uit_002057': 's·ª± th·∫ßn ph·ª•c',\n",
              " 'uit_002058': 's·ª± ƒë·ªôc l·∫≠p v·ªÅ t√†i ch√≠nh',\n",
              " 'uit_002059': 'ch·ªâ huy h·∫≠u qu√¢n trong cu·ªôc bao v√¢y Caerlaverock',\n",
              " 'uit_002060': '√¥ng ƒëem con trai ƒëi theo',\n",
              " 'uit_002061': '√¥ng ƒëem con trai ƒëi theo, khi·∫øn cho Edward tr·ªü th√†nh ch·ªâ huy h·∫≠u qu√¢n trong cu·ªôc bao v√¢y Caerlaverock',\n",
              " 'uit_002062': 's·ª± ƒë·ªôc l·∫≠p v·ªÅ t√†i ch√≠nh',\n",
              " 'uit_002063': 'L√£nh ƒë·ªãa B√° t∆∞·ªõc Chester v√† nh·ªØng v√πng ƒë·∫•t tr√™n kh·∫Øp B·∫Øc Wales',\n",
              " 'uit_002064': 'huy ƒë·ªông m·ªôt l·ª±c l∆∞·ª£ng qu√¢n vi·ªÖn chinh m·ªõi',\n",
              " 'uit_002065': 'Robert the Bruce',\n",
              " 'uit_002066': 's·ª± tr·ª´ng ph·∫°t, s·ª± tr·∫£ th√π kh·ªßng khi·∫øp',\n",
              " 'uit_002067': 'huy ƒë·ªông m·ªôt l·ª±c l∆∞·ª£ng qu√¢n vi·ªÖn chinh m·ªõi',\n",
              " 'uit_002068': 's·ª± tr·ª´ng ph·∫°t, s·ª± tr·∫£ th√π kh·ªßng khi·∫øp',\n",
              " 'uit_002069': 'Robert the Bruce',\n",
              " 'uit_002070': 'ƒë√†m ph√°n v·ªÅ ng√†y c∆∞·ªõi c·ªßa √¥ng v·ªõi Isabella',\n",
              " 'uit_002071': 'c√°c cu·ªôc ƒë√†m ph√°n v·ªÅ ng√†y c∆∞·ªõi c·ªßa √¥ng v·ªõi Isabella ti·∫øp t·ª•c',\n",
              " 'uit_002072': 'con trai √¥ng',\n",
              " 'uit_002073': 'Piers Gaveston',\n",
              " 'uit_002074': 'hi·ªáp sƒ© trong gia trang c·ªßa Nh√† vua c√≥ ƒë·∫•t phong gi√°p v·ªõi Gascony',\n",
              " 'uit_002075': 'ng∆∞·ªùi b·∫°n th√¢n thi·∫øt',\n",
              " 'uit_002076': 'Nh√† vua ƒë√°p l·∫°i m·ªôt c√°ch gi·∫≠n d·ªØ, ƒë√°nh ƒë·∫≠p v√† gi·∫≠t t√≥c con trai m√¨nh, tr∆∞·ªõc khi ƒë∆∞a Gaveston ƒëi l∆∞u ƒë√†y',\n",
              " 'uit_002077': 'b·∫°n th√¢n thi·∫øt',\n",
              " 'uit_002078': 'kh√¥ng r√µ l√Ω do',\n",
              " 'uit_002079': 'Eleanor de Clare',\n",
              " 'uit_002080': 'l√™n √°n quy·∫øt li·ªát',\n",
              " 'uit_002081': 'C·∫£ Edward v√† Gaveston ƒë·ªÅu c√≥ quan h·ªá v·ªõi v·ª£ h·ªç, v√† ƒë·ªÅu c√≥ con; Edward c≈©ng c√≥ m·ªôt ƒë·ª©a con ngo·∫°i h√¥n, v√† c√≥ th·ªÉ ƒë√£ c√≥ cu·ªôc t√¨nh v·ªõi c√¥ ch√°u g√°i, Eleanor de Clare',\n",
              " 'uit_002082': 'Eleanor de Clare',\n",
              " 'uit_002083': 'r·∫•t ph·ª©c t·∫°p b·ªüi nh·ªØng b·∫±ng ch·ª©ng x√°c ƒë·ªãnh v·ªÅ m·ªëi quan h·ªá chi ti·∫øt th·ª±c s·ª± c√≤n kh√° √≠t ·ªèi',\n",
              " 'uit_002084': 'Adam Orleton',\n",
              " 'uit_002085': 'm√¥ t·∫£ c√°ch m√† Edward \"c·∫£m th·∫•y nh∆∞ t√¨nh y√™u\" d√†nh cho Gaveston',\n",
              " 'uit_002086': 'b·ªã bu·ªôc t·ªôi',\n",
              " 'uit_002087': 'Gi√°m m·ª•c Winchester',\n",
              " 'uit_002088': 'Adam Orleton',\n",
              " 'uit_002089': 'Adam Orleton',\n",
              " 'uit_002090': '1320',\n",
              " 'uit_002091': 'c√°ch m√† Edward \"c·∫£m th·∫•y nh∆∞ t√¨nh y√™u\" d√†nh cho Gaveston, r·∫±ng \"√¥ng d√≠nh d√°ng v√†o m·ªôt giao ∆∞·ªõc kh√¥ng thay ƒë·ªïi, v√† r√†ng bu·ªôc ch√≠nh √¥ng v·ªõi √¥ng ta tr∆∞·ªõc t·∫•t c·∫£ nh·ªØng ng∆∞·ªùi kh√°c v·ªõi m·ªôt t√¨nh y√™u b·∫•t kh·∫£ ph√¢n li, v·ªØng b·ªÅn quy·∫øn r≈© v√† bu·ªôc ch·∫∑t b·∫±ng m·ªôt m·ªëi r√†ng\"',\n",
              " 'uit_002092': '1308',\n",
              " 'uit_002093': 'Gi√°o ho√†ng Boniface VIII v√† Hi·ªáp sƒ© Templar',\n",
              " 'uit_002094': 'l√Ω cho ch√°nh tr·ªã',\n",
              " 'uit_002095': 'Gi√°o ho√†ng Boniface VIII v√† Hi·ªáp sƒ© Templar',\n",
              " 'uit_002096': 'cha v√† cha v·ª£ c·ªßa Edward',\n",
              " 'uit_002097': 'cha v√† cha v·ª£',\n",
              " 'uit_002098': 'm·ªôt ph·∫ßn t·ª´ l√Ω cho ch√°nh tr·ªã',\n",
              " 'uit_002099': 'nh∆∞ anh em, v√† nh·ªØng ch√∫ th√≠ch d·ª©t kho√°t r·∫±ng Edward coi Gaveston l√† ng∆∞·ªùi anh nu√¥i c·ªßa √¥ng',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Raise\n",
        "import json\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Chu·∫©n h√≥a c√¢u tr·∫£ l·ªùi: lo·∫°i b·ªè k√Ω t·ª± th·ª´a, vi·∫øt th∆∞·ªùng.\"\"\"\n",
        "    import re\n",
        "    s = s.lower()\n",
        "    # s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)  # Lo·∫°i b·ªè c√°c t·ª´ kh√¥ng c·∫ßn thi·∫øt\n",
        "    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)  # Lo·∫°i b·ªè d·∫•u c√¢u\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
        "    return s\n",
        "\n",
        "def compute_scores(gold_answers, predicted_answers):\n",
        "    exact_matches = 0\n",
        "    total_f1 = 0\n",
        "    n = len(gold_answers)\n",
        "\n",
        "    for id_, gold in gold_answers.items():\n",
        "        pred = predicted_answers.get(id_, \"\")\n",
        "\n",
        "        # # Chu·∫©n h√≥a c√¢u tr·∫£ l·ªùi\n",
        "        gold = normalize_answer(gold)\n",
        "        pred = normalize_answer(pred)\n",
        "\n",
        "        # Exact Match\n",
        "        if gold == pred:\n",
        "            exact_matches += 1\n",
        "\n",
        "        # F1 Score\n",
        "        gold_tokens = gold.split()\n",
        "        pred_tokens = pred.split()\n",
        "        common_tokens = set(gold_tokens) & set(pred_tokens)\n",
        "        num_common = len(common_tokens)\n",
        "\n",
        "        if num_common == 0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            precision = num_common / len(pred_tokens)\n",
        "            recall = num_common / len(gold_tokens)\n",
        "            f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "        total_f1 += f1\n",
        "\n",
        "    exact_match = exact_matches / n\n",
        "    f1_score_avg = total_f1 / n\n",
        "\n",
        "    return exact_match, f1_score_avg\n",
        "\n",
        "# Load d·ªØ li·ªáu t·ª´ file JSON\n",
        "with open(\"/content/ground_truth.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ground_truth = json.load(f)\n",
        "\n",
        "with open(\"/content/predict_predictions.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    predictions = json.load(f)\n",
        "\n",
        "# T√≠nh to√°n EM v√† F1\n",
        "em, f1 = compute_scores(ground_truth, predictions)\n",
        "\n",
        "print(f\"Exact Match: {em * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gm-Q_xAaTKL",
        "outputId": "639c3b08-7bab-470e-db0b-4210dbd1fb06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match: 42.48%\n",
            "F1 Score: 64.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/transformers/evaluate_v2.py /content/ground_truth.json /content/transformers/results/predictions/predict_predictions.json --out-file eval_results.json"
      ],
      "metadata": {
        "id": "syvc0mChcg7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exact Match : 42.16%\n",
        "\n",
        "# F1 Score : 64.32%\n",
        "\n",
        "# Total : 3712"
      ],
      "metadata": {
        "id": "Jt894chlbr7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import argparse\n",
        "# import collections\n",
        "# import json\n",
        "# import re\n",
        "# import string\n",
        "\n",
        "# def parse_args():\n",
        "#     parser = argparse.ArgumentParser('Evaluation script for custom QA dataset.')\n",
        "#     parser.add_argument('data_file', metavar='data.json', help='Input ground truth JSON file.')\n",
        "#     parser.add_argument('pred_file', metavar='pred.json', help='Model predictions JSON file.')\n",
        "#     parser.add_argument('--out-file', '-o', metavar='eval.json', help='Write accuracy metrics to file (default is stdout).')\n",
        "#     return parser.parse_args()\n",
        "\n",
        "# def normalize_answer(s):\n",
        "#     \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "#     def remove_articles(text):\n",
        "#         regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "#         return re.sub(regex, ' ', text)\n",
        "\n",
        "#     def white_space_fix(text):\n",
        "#         return ' '.join(text.split())\n",
        "\n",
        "#     def remove_punc(text):\n",
        "#         exclude = set(string.punctuation)\n",
        "#         return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "#     def lower(text):\n",
        "#         return text.lower()\n",
        "\n",
        "#     return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "# def compute_exact(a_gold, a_pred):\n",
        "#     return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "# def compute_f1(a_gold, a_pred):\n",
        "#     gold_toks = normalize_answer(a_gold).split()\n",
        "#     pred_toks = normalize_answer(a_pred).split()\n",
        "#     common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "#     num_same = sum(common.values())\n",
        "\n",
        "#     if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "#         return int(gold_toks == pred_toks)\n",
        "\n",
        "#     if num_same == 0:\n",
        "#         return 0\n",
        "\n",
        "#     precision = 1.0 * num_same / len(pred_toks)\n",
        "#     recall = 1.0 * num_same / len(gold_toks)\n",
        "#     return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "# def evaluate(data_file, pred_file):\n",
        "#     with open(data_file, 'r', encoding='utf-8') as f:\n",
        "#         ground_truth = json.load(f)\n",
        "\n",
        "#     with open(pred_file, 'r', encoding='utf-8') as f:\n",
        "#         predictions = json.load(f)\n",
        "\n",
        "#     exact_scores = {}\n",
        "#     f1_scores = {}\n",
        "\n",
        "#     for qid, gold_answer in ground_truth.items():\n",
        "#         if qid not in predictions:\n",
        "#             print(f\"Missing prediction for {qid}\")\n",
        "#             continue\n",
        "\n",
        "#         pred_answer = predictions[qid]\n",
        "#         exact_scores[qid] = compute_exact(gold_answer, pred_answer)\n",
        "#         f1_scores[qid] = compute_f1(gold_answer, pred_answer)\n",
        "\n",
        "#     total = len(ground_truth)\n",
        "#     exact_match = 100.0 * sum(exact_scores.values()) / total\n",
        "#     f1 = 100.0 * sum(f1_scores.values()) / total\n",
        "\n",
        "#     return {\n",
        "#         'exact': exact_match,\n",
        "#         'f1': f1,\n",
        "#         'total': total\n",
        "#     }\n",
        "\n",
        "# def main():\n",
        "#     args = parse_args()\n",
        "#     metrics = evaluate(args.data_file, args.pred_file)\n",
        "\n",
        "#     if args.out_file:\n",
        "#         with open(args.out_file, 'w', encoding='utf-8') as f:\n",
        "#             json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
        "#     else:\n",
        "#         print(json.dumps(metrics, indent=2, ensure_ascii=False))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "OB18NgfkcCZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twpl-jALDzSf",
        "outputId": "a40bc551-3563-4276-9898-773d23e6011c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    }
  ]
}