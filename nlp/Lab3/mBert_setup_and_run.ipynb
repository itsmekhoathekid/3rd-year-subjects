{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2seOirvygK-",
        "outputId": "aab1e852-c859-46f4-eb50-b1380c937c3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EGiDy8pwg_Uu-JRKHutu-gfDe0tXbpp2\n",
            "To: /content/ViQuAD2.0.zip\n",
            "100% 14.2M/14.2M [00:00<00:00, 27.9MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1EGiDy8pwg_Uu-JRKHutu-gfDe0tXbpp2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git\n",
        "# Báº­t sparse-checkout\n",
        "!git clone --depth 1 --filter=blob:none --sparse https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!git sparse-checkout set examples/pytorch/question-answering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX9Yp7XZ0aXr",
        "outputId": "6b6ca4ec-70f7-46ae-e652-dab6443ae28a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 855, done.\u001b[K\n",
            "remote: Counting objects: 100% (855/855), done.\u001b[K\n",
            "remote: Compressing objects: 100% (821/821), done.\u001b[K\n",
            "remote: Total 855 (delta 1), reused 279 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (855/855), 208.11 KiB | 4.73 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 3 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 55.31 KiB | 4.61 MiB/s, done.\n",
            "/content/transformers\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 10 (delta 4), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), 37.27 KiB | 7.45 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ViQuAD2.0.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5J0iMEJ0aoF",
        "outputId": "1ce04e76-9801-41da-f388-8d6b1210f36b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ViQuAD2.0.zip\n",
            "  inflating: ViQuAD2.0/test/ground_truth_private_test.json  \n",
            "  inflating: ViQuAD2.0/train/train.json  \n",
            "  inflating: ViQuAD2.0/dev/dev.json  \n",
            "  inflating: ViQuAD2.0/test/Private_Test_ref.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/transformers/examples/pytorch/question-answering/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XItwMvl70bpH",
        "outputId": "566a8a80-1954-420c-b976-39d756b498c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (1.2.1)\n",
            "Collecting datasets>=1.8.0 (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (2.5.1+cu121)\n",
            "Collecting evaluate (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 4))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.67.1)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers -y\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgn9oGCV0e9Q",
        "outputId": "51f6d597-dcdc-45f1-f0c3-fbb0bc393802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.47.1\n",
            "Uninstalling transformers-4.47.1:\n",
            "  Successfully uninstalled transformers-4.47.1\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-550myz37\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-550myz37\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit e5fd865ebae062b7cf03a81b8c6affeb39f30bec\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2024.12.14)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.48.0.dev0-py3-none-any.whl size=10329101 sha256=f4d004a307811efdd68377326d17a08a0bfd4249623042646eb1a2c6f8d9b1cf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l_8903na/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.48.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "  --model_name_or_path bert-base-multilingual-cased \\\n",
        "  --train_file /content/transformers/ViQuAD2.0/train/train.json \\\n",
        "  --validation_file /content/transformers/ViQuAD2.0/dev/dev.json \\\n",
        "  --test_file /content/transformers/ViQuAD2.0/test/Private_Test_ref.json \\\n",
        "  --output_dir ./results \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --overwrite_output_dir \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjXK9UaB0gHe",
        "outputId": "96f4e717-1d4c-4393-c622-343fc7937870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-03 15:49:08.505895: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-03 15:49:08.525402: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-03 15:49:08.531244: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-03 15:49:08.545840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-03 15:49:09.725491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/03/2025 15:49:11 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "01/03/2025 15:49:11 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./results/runs/Jan03_15-49-11_0811d96ebebd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./results,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./results,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-7d7ac5ea573e91b3\n",
            "01/03/2025 15:49:12 - INFO - datasets.builder - Using custom data configuration default-7d7ac5ea573e91b3\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "01/03/2025 15:49:12 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "01/03/2025 15:49:12 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "01/03/2025 15:49:12 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "Downloading took 0.0 min\n",
            "01/03/2025 15:49:12 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "01/03/2025 15:49:12 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "01/03/2025 15:49:12 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 22765 examples [00:01, 11687.23 examples/s]\n",
            "Generating validation split\n",
            "01/03/2025 15:49:14 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 5692 examples [00:00, 12969.05 examples/s]\n",
            "Generating test split\n",
            "01/03/2025 15:49:15 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 7301 examples [00:00, 16188.84 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "01/03/2025 15:49:15 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "01/03/2025 15:49:15 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "config.json: 100% 625/625 [00:00<00:00, 4.10MB/s]\n",
            "[INFO|configuration_utils.py:696] 2025-01-03 15:49:16,080 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-03 15:49:16,082 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 49.0/49.0 [00:00<00:00, 240kB/s]\n",
            "[INFO|configuration_utils.py:696] 2025-01-03 15:49:16,589 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-03 15:49:16,590 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "vocab.txt: 100% 996k/996k [00:00<00:00, 1.34MB/s]\n",
            "tokenizer.json: 100% 1.96M/1.96M [00:00<00:00, 7.41MB/s]\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-03 15:49:20,322 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:696] 2025-01-03 15:49:20,322 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-03 15:49:20,323 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"bert-base-multilingual-cased\",\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 714M/714M [00:05<00:00, 129MB/s]\n",
            "[INFO|modeling_utils.py:3893] 2025-01-03 15:49:26,928 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/3f076fdb1ab68d5b2880cb87a0886f315b8146f8/model.safetensors\n",
            "[INFO|logging.py:343] 2025-01-03 15:49:27,066 >> A pretrained model of type `BertForQuestionAnswering` contains parameters that have been renamed internally (a few are listed below but more are present in the model):\n",
            "* `bert.embeddings.LayerNorm.beta` -> `bert.embeddings.LayerNorm.bias`\n",
            "* `bert.embeddings.LayerNorm.gamma` -> `bert.embeddings.LayerNorm.weight`\n",
            "If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.\n",
            "[INFO|modeling_utils.py:4838] 2025-01-03 15:49:27,092 >> Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:4850] 2025-01-03 15:49:27,092 >> Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/22765 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f83a4f40cb5fae15.arrow\n",
            "01/03/2025 15:49:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f83a4f40cb5fae15.arrow\n",
            "Running tokenizer on train dataset: 100% 22765/22765 [00:21<00:00, 1074.87 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/5692 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-34dd4d2106e5c28d.arrow\n",
            "01/03/2025 15:49:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-34dd4d2106e5c28d.arrow\n",
            "Running tokenizer on validation dataset: 100% 5692/5692 [00:06<00:00, 909.88 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/7301 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-30ca8675cfa61c3b.arrow\n",
            "01/03/2025 15:49:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-30ca8675cfa61c3b.arrow\n",
            "Running tokenizer on prediction dataset: 100% 7301/7301 [00:07<00:00, 960.64 examples/s] \n",
            "Downloading builder script: 100% 4.53k/4.53k [00:00<00:00, 14.7MB/s]\n",
            "Downloading extra modules: 100% 3.32k/3.32k [00:00<00:00, 14.7MB/s]\n",
            "[INFO|trainer.py:2369] 2025-01-03 15:50:06,827 >> ***** Running training *****\n",
            "[INFO|trainer.py:2370] 2025-01-03 15:50:06,827 >>   Num examples = 24,438\n",
            "[INFO|trainer.py:2371] 2025-01-03 15:50:06,827 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2372] 2025-01-03 15:50:06,827 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:2375] 2025-01-03 15:50:06,827 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2376] 2025-01-03 15:50:06,827 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2377] 2025-01-03 15:50:06,827 >>   Total optimization steps = 4,584\n",
            "[INFO|trainer.py:2378] 2025-01-03 15:50:06,828 >>   Number of trainable parameters = 177,264,386\n",
            "[INFO|integration_utils.py:811] 2025-01-03 15:50:06,836 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/transformers/wandb/run-20250103_155220-a9x4m5tz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./results\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/21522798-uit/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ð View run at \u001b[34m\u001b[4mhttps://wandb.ai/21522798-uit/huggingface/runs/a9x4m5tz\u001b[0m\n",
            "{'loss': 2.2019, 'grad_norm': 18.974044799804688, 'learning_rate': 2.6727748691099477e-05, 'epoch': 0.33}\n",
            " 11% 500/4584 [09:34<1:18:08,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:01:55,817 >> Saving model checkpoint to ./results/checkpoint-500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:01:55,819 >> Configuration saved in ./results/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:02:03,114 >> Model weights saved in ./results/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:02:03,116 >> tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:02:03,116 >> Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 1.6754, 'grad_norm': 19.527013778686523, 'learning_rate': 2.3455497382198953e-05, 'epoch': 0.65}\n",
            " 22% 1000/4584 [19:25<1:08:31,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:11:47,359 >> Saving model checkpoint to ./results/checkpoint-1000\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:11:47,361 >> Configuration saved in ./results/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:11:58,586 >> Model weights saved in ./results/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:11:58,589 >> tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:11:58,589 >> Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.5526, 'grad_norm': 26.89529800415039, 'learning_rate': 2.018324607329843e-05, 'epoch': 0.98}\n",
            " 33% 1500/4584 [29:18<59:00,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:21:40,436 >> Saving model checkpoint to ./results/checkpoint-1500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:21:40,438 >> Configuration saved in ./results/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:21:47,681 >> Model weights saved in ./results/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:21:47,683 >> tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:21:47,683 >> Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 1.2215, 'grad_norm': 22.804689407348633, 'learning_rate': 1.6910994764397905e-05, 'epoch': 1.31}\n",
            " 44% 2000/4584 [39:09<49:24,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:31:30,606 >> Saving model checkpoint to ./results/checkpoint-2000\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:31:30,608 >> Configuration saved in ./results/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:31:39,148 >> Model weights saved in ./results/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:31:39,150 >> tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:31:39,150 >> Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 1.1682, 'grad_norm': 23.251840591430664, 'learning_rate': 1.3638743455497383e-05, 'epoch': 1.64}\n",
            " 55% 2500/4584 [48:59<39:54,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:41:21,258 >> Saving model checkpoint to ./results/checkpoint-2500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:41:21,261 >> Configuration saved in ./results/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:41:32,924 >> Model weights saved in ./results/checkpoint-2500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:41:32,926 >> tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:41:32,927 >> Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 1.1427, 'grad_norm': 21.312780380249023, 'learning_rate': 1.0366492146596857e-05, 'epoch': 1.96}\n",
            " 65% 3000/4584 [58:54<30:21,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 16:51:16,038 >> Saving model checkpoint to ./results/checkpoint-3000\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 16:51:16,039 >> Configuration saved in ./results/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 16:51:32,109 >> Model weights saved in ./results/checkpoint-3000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 16:51:32,112 >> tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 16:51:32,112 >> Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 0.8837, 'grad_norm': 25.836029052734375, 'learning_rate': 7.094240837696335e-06, 'epoch': 2.29}\n",
            " 76% 3500/4584 [1:09:10<20:48,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 17:01:32,484 >> Saving model checkpoint to ./results/checkpoint-3500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:01:32,486 >> Configuration saved in ./results/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:02:17,874 >> Model weights saved in ./results/checkpoint-3500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:02:17,875 >> tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:02:17,876 >> Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 0.8188, 'grad_norm': 16.85807991027832, 'learning_rate': 3.821989528795812e-06, 'epoch': 2.62}\n",
            " 87% 4000/4584 [1:19:37<11:11,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 17:11:59,088 >> Saving model checkpoint to ./results/checkpoint-4000\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:11:59,089 >> Configuration saved in ./results/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:12:03,966 >> Model weights saved in ./results/checkpoint-4000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:12:03,968 >> tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:12:03,968 >> Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 0.8223, 'grad_norm': 31.01511001586914, 'learning_rate': 5.497382198952879e-07, 'epoch': 2.95}\n",
            " 98% 4500/4584 [1:29:33<01:36,  1.15s/it][INFO|trainer.py:3911] 2025-01-03 17:21:54,642 >> Saving model checkpoint to ./results/checkpoint-4500\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:21:54,645 >> Configuration saved in ./results/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:22:07,379 >> Model weights saved in ./results/checkpoint-4500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:22:07,381 >> tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:22:07,382 >> Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
            "100% 4584/4584 [1:31:31<00:00,  1.06it/s][INFO|trainer.py:3911] 2025-01-03 17:23:53,400 >> Saving model checkpoint to ./results/checkpoint-4584\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:23:53,402 >> Configuration saved in ./results/checkpoint-4584/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:24:13,694 >> Model weights saved in ./results/checkpoint-4584/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:24:13,697 >> tokenizer config file saved in ./results/checkpoint-4584/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:24:13,697 >> Special tokens file saved in ./results/checkpoint-4584/special_tokens_map.json\n",
            "[INFO|trainer.py:2643] 2025-01-03 17:25:13,399 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5706.5714, 'train_samples_per_second': 12.847, 'train_steps_per_second': 0.803, 'train_loss': 1.2669352569713226, 'epoch': 3.0}\n",
            "100% 4584/4584 [1:32:51<00:00,  1.22s/it]\n",
            "[INFO|trainer.py:3911] 2025-01-03 17:25:13,424 >> Saving model checkpoint to ./results\n",
            "[INFO|configuration_utils.py:420] 2025-01-03 17:25:13,426 >> Configuration saved in ./results/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-03 17:25:20,035 >> Model weights saved in ./results/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-03 17:25:20,037 >> tokenizer config file saved in ./results/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-03 17:25:20,037 >> Special tokens file saved in ./results/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 13380807GF\n",
            "  train_loss               =     1.2669\n",
            "  train_runtime            = 1:35:06.57\n",
            "  train_samples            =      24438\n",
            "  train_samples_per_second =     12.847\n",
            "  train_steps_per_second   =      0.803\n",
            "01/03/2025 17:25:20 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:917] 2025-01-03 17:25:20,122 >> The following columns in the evaluation set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:4227] 2025-01-03 17:25:20,125 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4229] 2025-01-03 17:25:20,125 >>   Num examples = 6105\n",
            "[INFO|trainer.py:4232] 2025-01-03 17:25:20,125 >>   Batch size = 8\n",
            "100% 763/764 [02:22<00:00,  5.37it/s]01/03/2025 17:27:56 - INFO - utils_qa - Post-processing 5692 example predictions split into 6105 features.\n",
            "\n",
            "  0% 0/5692 [00:00<?, ?it/s]\u001b[A\n",
            "  0% 26/5692 [00:00<00:22, 257.05it/s]\u001b[A\n",
            "  1% 53/5692 [00:00<00:21, 263.94it/s]\u001b[A\n",
            "  1% 80/5692 [00:00<00:21, 266.27it/s]\u001b[A\n",
            "  2% 113/5692 [00:00<00:19, 290.17it/s]\u001b[A\n",
            "  3% 143/5692 [00:00<00:20, 276.30it/s]\u001b[A\n",
            "  3% 172/5692 [00:00<00:19, 279.81it/s]\u001b[A\n",
            "  4% 203/5692 [00:00<00:19, 286.97it/s]\u001b[A\n",
            "  4% 235/5692 [00:00<00:18, 296.68it/s]\u001b[A\n",
            "  5% 265/5692 [00:00<00:18, 294.31it/s]\u001b[A\n",
            "  5% 295/5692 [00:01<00:18, 295.53it/s]\u001b[A\n",
            "  6% 325/5692 [00:01<00:18, 296.22it/s]\u001b[A\n",
            "  6% 355/5692 [00:01<00:18, 290.19it/s]\u001b[A\n",
            "  7% 386/5692 [00:01<00:18, 293.51it/s]\u001b[A\n",
            "  7% 419/5692 [00:01<00:17, 300.64it/s]\u001b[A\n",
            "  8% 450/5692 [00:01<00:17, 294.55it/s]\u001b[A\n",
            "  8% 480/5692 [00:01<00:18, 286.43it/s]\u001b[A\n",
            "  9% 509/5692 [00:01<00:18, 282.48it/s]\u001b[A\n",
            "  9% 540/5692 [00:01<00:17, 289.11it/s]\u001b[A\n",
            " 10% 569/5692 [00:01<00:17, 288.16it/s]\u001b[A\n",
            " 11% 598/5692 [00:02<00:17, 288.15it/s]\u001b[A\n",
            " 11% 627/5692 [00:02<00:17, 283.45it/s]\u001b[A\n",
            " 12% 659/5692 [00:02<00:17, 293.01it/s]\u001b[A\n",
            " 12% 689/5692 [00:02<00:17, 290.49it/s]\u001b[A\n",
            " 13% 722/5692 [00:02<00:16, 300.39it/s]\u001b[A\n",
            " 13% 753/5692 [00:02<00:16, 295.07it/s]\u001b[A\n",
            " 14% 783/5692 [00:02<00:16, 292.03it/s]\u001b[A\n",
            " 14% 814/5692 [00:02<00:16, 295.79it/s]\u001b[A\n",
            " 15% 846/5692 [00:02<00:16, 301.04it/s]\u001b[A\n",
            " 15% 877/5692 [00:03<00:16, 290.38it/s]\u001b[A\n",
            " 16% 910/5692 [00:03<00:15, 299.83it/s]\u001b[A\n",
            " 17% 941/5692 [00:03<00:16, 294.67it/s]\u001b[A\n",
            " 17% 973/5692 [00:03<00:15, 299.88it/s]\u001b[A\n",
            " 18% 1004/5692 [00:03<00:15, 299.95it/s]\u001b[A\n",
            " 18% 1035/5692 [00:03<00:15, 300.61it/s]\u001b[A\n",
            " 19% 1066/5692 [00:03<00:15, 300.63it/s]\u001b[A\n",
            " 19% 1098/5692 [00:03<00:15, 304.60it/s]\u001b[A\n",
            " 20% 1129/5692 [00:03<00:14, 305.22it/s]\u001b[A\n",
            " 20% 1160/5692 [00:03<00:14, 305.35it/s]\u001b[A\n",
            " 21% 1192/5692 [00:04<00:14, 308.92it/s]\u001b[A\n",
            " 22% 1225/5692 [00:04<00:14, 313.22it/s]\u001b[A\n",
            " 22% 1257/5692 [00:04<00:14, 296.92it/s]\u001b[A\n",
            " 23% 1287/5692 [00:04<00:14, 293.73it/s]\u001b[A\n",
            " 23% 1320/5692 [00:04<00:14, 303.35it/s]\u001b[A\n",
            " 24% 1351/5692 [00:04<00:14, 303.23it/s]\u001b[A\n",
            " 24% 1382/5692 [00:04<00:14, 302.95it/s]\u001b[A\n",
            " 25% 1413/5692 [00:04<00:14, 294.45it/s]\u001b[A\n",
            " 25% 1443/5692 [00:04<00:14, 286.95it/s]\u001b[A\n",
            " 26% 1473/5692 [00:05<00:14, 290.35it/s]\u001b[A\n",
            " 26% 1503/5692 [00:05<00:14, 291.87it/s]\u001b[A\n",
            " 27% 1533/5692 [00:05<00:14, 290.68it/s]\u001b[A\n",
            " 27% 1563/5692 [00:05<00:14, 285.91it/s]\u001b[A\n",
            " 28% 1596/5692 [00:05<00:13, 296.91it/s]\u001b[A\n",
            " 29% 1626/5692 [00:05<00:13, 297.78it/s]\u001b[A\n",
            " 29% 1656/5692 [00:05<00:13, 294.08it/s]\u001b[A\n",
            " 30% 1688/5692 [00:05<00:13, 300.18it/s]\u001b[A\n",
            " 30% 1719/5692 [00:05<00:13, 296.16it/s]\u001b[A\n",
            " 31% 1750/5692 [00:05<00:13, 300.14it/s]\u001b[A\n",
            " 31% 1781/5692 [00:06<00:13, 295.01it/s]\u001b[A\n",
            " 32% 1811/5692 [00:06<00:13, 290.53it/s]\u001b[A\n",
            " 32% 1841/5692 [00:06<00:13, 292.80it/s]\u001b[A\n",
            " 33% 1871/5692 [00:06<00:13, 280.47it/s]\u001b[A\n",
            " 33% 1900/5692 [00:06<00:13, 281.36it/s]\u001b[A\n",
            " 34% 1929/5692 [00:06<00:13, 283.47it/s]\u001b[A\n",
            " 34% 1958/5692 [00:06<00:13, 282.34it/s]\u001b[A\n",
            " 35% 1987/5692 [00:06<00:13, 276.30it/s]\u001b[A\n",
            " 35% 2015/5692 [00:06<00:13, 275.33it/s]\u001b[A\n",
            " 36% 2044/5692 [00:06<00:13, 278.21it/s]\u001b[A\n",
            " 36% 2072/5692 [00:07<00:12, 278.61it/s]\u001b[A\n",
            " 37% 2100/5692 [00:07<00:13, 267.88it/s]\u001b[A\n",
            " 37% 2127/5692 [00:07<00:13, 261.15it/s]\u001b[A\n",
            " 38% 2154/5692 [00:07<00:13, 252.77it/s]\u001b[A\n",
            " 38% 2182/5692 [00:07<00:13, 258.45it/s]\u001b[A\n",
            " 39% 2214/5692 [00:07<00:12, 273.94it/s]\u001b[A\n",
            " 39% 2243/5692 [00:07<00:12, 277.81it/s]\u001b[A\n",
            " 40% 2272/5692 [00:07<00:12, 279.05it/s]\u001b[A\n",
            " 40% 2300/5692 [00:08<00:14, 228.14it/s]\u001b[A\n",
            " 41% 2325/5692 [00:08<00:16, 204.31it/s]\u001b[A\n",
            " 41% 2347/5692 [00:08<00:17, 191.10it/s]\u001b[A\n",
            " 42% 2368/5692 [00:08<00:19, 169.51it/s]\u001b[A\n",
            " 42% 2386/5692 [00:08<00:19, 169.38it/s]\u001b[A\n",
            " 42% 2404/5692 [00:08<00:19, 164.89it/s]\u001b[A\n",
            " 43% 2423/5692 [00:08<00:19, 169.37it/s]\u001b[A\n",
            " 43% 2441/5692 [00:08<00:19, 169.71it/s]\u001b[A\n",
            " 43% 2459/5692 [00:09<00:19, 169.12it/s]\u001b[A\n",
            " 44% 2477/5692 [00:09<00:18, 170.64it/s]\u001b[A\n",
            " 44% 2495/5692 [00:09<00:18, 171.20it/s]\u001b[A\n",
            " 44% 2513/5692 [00:09<00:18, 171.39it/s]\u001b[A\n",
            " 44% 2531/5692 [00:09<00:19, 161.99it/s]\u001b[A\n",
            " 45% 2549/5692 [00:09<00:18, 166.52it/s]\u001b[A\n",
            " 45% 2566/5692 [00:09<00:19, 163.53it/s]\u001b[A\n",
            " 45% 2584/5692 [00:09<00:18, 165.99it/s]\u001b[A\n",
            " 46% 2601/5692 [00:09<00:18, 163.27it/s]\u001b[A\n",
            " 46% 2618/5692 [00:09<00:18, 164.69it/s]\u001b[A\n",
            " 46% 2636/5692 [00:10<00:18, 163.85it/s]\u001b[A\n",
            " 47% 2653/5692 [00:10<00:18, 164.88it/s]\u001b[A\n",
            " 47% 2670/5692 [00:10<00:19, 157.56it/s]\u001b[A\n",
            " 47% 2686/5692 [00:10<00:19, 157.18it/s]\u001b[A\n",
            " 47% 2702/5692 [00:10<00:19, 151.58it/s]\u001b[A\n",
            " 48% 2718/5692 [00:10<00:20, 148.56it/s]\u001b[A\n",
            " 48% 2734/5692 [00:10<00:19, 151.09it/s]\u001b[A\n",
            " 48% 2750/5692 [00:10<00:19, 149.35it/s]\u001b[A\n",
            " 49% 2767/5692 [00:10<00:18, 154.38it/s]\u001b[A\n",
            " 49% 2783/5692 [00:11<00:19, 149.63it/s]\u001b[A\n",
            " 49% 2800/5692 [00:11<00:18, 155.36it/s]\u001b[A\n",
            " 49% 2816/5692 [00:11<00:18, 152.85it/s]\u001b[A\n",
            " 50% 2832/5692 [00:11<00:18, 154.75it/s]\u001b[A\n",
            " 50% 2848/5692 [00:11<00:18, 155.14it/s]\u001b[A\n",
            " 50% 2864/5692 [00:11<00:18, 150.65it/s]\u001b[A\n",
            " 51% 2880/5692 [00:11<00:18, 151.04it/s]\u001b[A\n",
            " 51% 2898/5692 [00:11<00:17, 156.98it/s]\u001b[A\n",
            " 51% 2916/5692 [00:11<00:17, 161.17it/s]\u001b[A\n",
            " 52% 2933/5692 [00:12<00:16, 163.43it/s]\u001b[A\n",
            " 52% 2950/5692 [00:12<00:17, 157.34it/s]\u001b[A\n",
            " 52% 2968/5692 [00:12<00:16, 163.24it/s]\u001b[A\n",
            " 52% 2985/5692 [00:12<00:16, 164.28it/s]\u001b[A\n",
            " 53% 3002/5692 [00:12<00:16, 165.33it/s]\u001b[A\n",
            " 53% 3020/5692 [00:12<00:15, 169.13it/s]\u001b[A\n",
            " 53% 3043/5692 [00:12<00:14, 185.68it/s]\u001b[A\n",
            " 54% 3076/5692 [00:12<00:11, 226.40it/s]\u001b[A\n",
            " 54% 3099/5692 [00:12<00:11, 226.94it/s]\u001b[A\n",
            " 55% 3130/5692 [00:12<00:10, 251.47it/s]\u001b[A\n",
            " 56% 3163/5692 [00:13<00:09, 272.83it/s]\u001b[A\n",
            " 56% 3191/5692 [00:13<00:09, 272.11it/s]\u001b[A\n",
            " 57% 3222/5692 [00:13<00:08, 282.03it/s]\u001b[A\n",
            " 57% 3253/5692 [00:13<00:08, 288.39it/s]\u001b[A\n",
            " 58% 3282/5692 [00:13<00:08, 286.65it/s]\u001b[A\n",
            " 58% 3311/5692 [00:13<00:08, 284.57it/s]\u001b[A\n",
            " 59% 3340/5692 [00:13<00:08, 275.66it/s]\u001b[A\n",
            " 59% 3368/5692 [00:13<00:08, 274.41it/s]\u001b[A\n",
            " 60% 3396/5692 [00:13<00:08, 263.69it/s]\u001b[A\n",
            " 60% 3427/5692 [00:13<00:08, 274.89it/s]\u001b[A\n",
            " 61% 3455/5692 [00:14<00:08, 264.34it/s]\u001b[A\n",
            " 61% 3483/5692 [00:14<00:08, 264.47it/s]\u001b[A\n",
            " 62% 3513/5692 [00:14<00:08, 271.96it/s]\u001b[A\n",
            " 62% 3545/5692 [00:14<00:07, 285.63it/s]\u001b[A\n",
            " 63% 3574/5692 [00:14<00:07, 286.43it/s]\u001b[A\n",
            " 63% 3603/5692 [00:14<00:07, 284.52it/s]\u001b[A\n",
            " 64% 3632/5692 [00:14<00:07, 280.05it/s]\u001b[A\n",
            " 64% 3661/5692 [00:14<00:07, 277.22it/s]\u001b[A\n",
            " 65% 3690/5692 [00:14<00:07, 279.93it/s]\u001b[A\n",
            " 65% 3719/5692 [00:15<00:06, 282.32it/s]\u001b[A\n",
            " 66% 3748/5692 [00:15<00:06, 282.15it/s]\u001b[A\n",
            " 66% 3781/5692 [00:15<00:06, 295.30it/s]\u001b[A\n",
            " 67% 3811/5692 [00:15<00:06, 290.40it/s]\u001b[A\n",
            " 67% 3841/5692 [00:15<00:06, 286.95it/s]\u001b[A\n",
            " 68% 3873/5692 [00:15<00:06, 296.01it/s]\u001b[A\n",
            " 69% 3905/5692 [00:15<00:05, 300.66it/s]\u001b[A\n",
            " 69% 3936/5692 [00:15<00:06, 291.91it/s]\u001b[A\n",
            " 70% 3968/5692 [00:15<00:05, 298.87it/s]\u001b[A\n",
            " 70% 4001/5692 [00:15<00:05, 303.85it/s]\u001b[A\n",
            " 71% 4032/5692 [00:16<00:05, 305.13it/s]\u001b[A\n",
            " 71% 4063/5692 [00:16<00:05, 299.56it/s]\u001b[A\n",
            " 72% 4097/5692 [00:16<00:05, 309.57it/s]\u001b[A\n",
            " 73% 4129/5692 [00:16<00:05, 299.31it/s]\u001b[A\n",
            " 73% 4160/5692 [00:16<00:05, 297.77it/s]\u001b[A\n",
            " 74% 4190/5692 [00:16<00:05, 297.48it/s]\u001b[A\n",
            " 74% 4220/5692 [00:16<00:05, 292.63it/s]\u001b[A\n",
            " 75% 4250/5692 [00:16<00:05, 279.14it/s]\u001b[A\n",
            " 75% 4283/5692 [00:16<00:04, 291.40it/s]\u001b[A\n",
            " 76% 4315/5692 [00:17<00:04, 297.32it/s]\u001b[A\n",
            " 76% 4345/5692 [00:17<00:04, 295.61it/s]\u001b[A\n",
            " 77% 4375/5692 [00:17<00:04, 287.92it/s]\u001b[A\n",
            " 77% 4406/5692 [00:17<00:04, 292.59it/s]\u001b[A\n",
            " 78% 4436/5692 [00:17<00:04, 285.94it/s]\u001b[A\n",
            " 78% 4465/5692 [00:17<00:04, 283.75it/s]\u001b[A\n",
            " 79% 4496/5692 [00:17<00:04, 288.22it/s]\u001b[A\n",
            " 79% 4525/5692 [00:17<00:04, 283.59it/s]\u001b[A\n",
            " 80% 4554/5692 [00:17<00:04, 280.32it/s]\u001b[A\n",
            " 81% 4585/5692 [00:17<00:03, 287.49it/s]\u001b[A\n",
            " 81% 4616/5692 [00:18<00:03, 292.17it/s]\u001b[A\n",
            " 82% 4647/5692 [00:18<00:03, 296.14it/s]\u001b[A\n",
            " 82% 4677/5692 [00:18<00:03, 286.77it/s]\u001b[A\n",
            " 83% 4709/5692 [00:18<00:03, 296.09it/s]\u001b[A\n",
            " 83% 4739/5692 [00:18<00:03, 285.49it/s]\u001b[A\n",
            " 84% 4772/5692 [00:18<00:03, 296.49it/s]\u001b[A\n",
            " 84% 4803/5692 [00:18<00:03, 295.94it/s]\u001b[A\n",
            " 85% 4833/5692 [00:18<00:03, 276.23it/s]\u001b[A\n",
            " 85% 4862/5692 [00:18<00:02, 278.80it/s]\u001b[A\n",
            " 86% 4893/5692 [00:19<00:02, 286.12it/s]\u001b[A\n",
            " 86% 4922/5692 [00:19<00:02, 284.29it/s]\u001b[A\n",
            " 87% 4956/5692 [00:19<00:02, 297.95it/s]\u001b[A\n",
            " 88% 4986/5692 [00:19<00:02, 295.21it/s]\u001b[A\n",
            " 88% 5017/5692 [00:19<00:02, 297.17it/s]\u001b[A\n",
            " 89% 5047/5692 [00:19<00:02, 294.34it/s]\u001b[A\n",
            " 89% 5077/5692 [00:19<00:02, 295.06it/s]\u001b[A\n",
            " 90% 5107/5692 [00:19<00:01, 293.89it/s]\u001b[A\n",
            " 90% 5137/5692 [00:19<00:01, 287.94it/s]\u001b[A\n",
            " 91% 5168/5692 [00:19<00:01, 292.28it/s]\u001b[A\n",
            " 91% 5198/5692 [00:20<00:01, 282.49it/s]\u001b[A\n",
            " 92% 5228/5692 [00:20<00:01, 286.37it/s]\u001b[A\n",
            " 92% 5259/5692 [00:20<00:01, 292.71it/s]\u001b[A\n",
            " 93% 5289/5692 [00:20<00:01, 289.43it/s]\u001b[A\n",
            " 93% 5319/5692 [00:20<00:01, 284.06it/s]\u001b[A\n",
            " 94% 5348/5692 [00:20<00:01, 273.59it/s]\u001b[A\n",
            " 94% 5378/5692 [00:20<00:01, 279.63it/s]\u001b[A\n",
            " 95% 5408/5692 [00:20<00:00, 284.48it/s]\u001b[A\n",
            " 96% 5437/5692 [00:20<00:00, 277.53it/s]\u001b[A\n",
            " 96% 5467/5692 [00:21<00:00, 282.21it/s]\u001b[A\n",
            " 97% 5497/5692 [00:21<00:00, 285.48it/s]\u001b[A\n",
            " 97% 5530/5692 [00:21<00:00, 293.12it/s]\u001b[A\n",
            " 98% 5562/5692 [00:21<00:00, 298.67it/s]\u001b[A\n",
            " 98% 5592/5692 [00:21<00:00, 296.67it/s]\u001b[A\n",
            " 99% 5622/5692 [00:21<00:00, 286.38it/s]\u001b[A\n",
            " 99% 5651/5692 [00:21<00:00, 272.97it/s]\u001b[A\n",
            "100% 5692/5692 [00:21<00:00, 260.46it/s]\n",
            "01/03/2025 17:28:17 - INFO - utils_qa - Saving predictions to ./results/eval_predictions.json.\n",
            "01/03/2025 17:28:17 - INFO - utils_qa - Saving nbest_preds to ./results/eval_nbest_predictions.json.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 715, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 672, in main\n",
            "    metrics = trainer.evaluate()\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 73, in evaluate\n",
            "    metrics = self.compute_metrics(eval_preds)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 634, in compute_metrics\n",
            "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/evaluate/module.py\", line 467, in compute\n",
            "    output = self._compute(**inputs, **compute_kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/squad.py\", line 110, in _compute\n",
            "    score = compute_score(dataset=dataset, predictions=pred_dict)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 67, in compute_score\n",
            "    exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 52, in metric_max_over_ground_truths\n",
            "    return max(scores_for_ground_truths)\n",
            "ValueError: max() arg is an empty sequence\n",
            "\u001b[1;34mwandb\u001b[0m: ð View run \u001b[33m./results\u001b[0m at: \u001b[34mhttps://wandb.ai/21522798-uit/huggingface/runs/a9x4m5tz\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250103_155220-a9x4m5tz/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForQuestionAnswering, AutoTokenizer\n",
        "\n",
        "# ÄÆ°á»ng dáº«n checkpoint\n",
        "checkpoint_path = \"/content/transformers/results/checkpoint-4584\"\n",
        "\n",
        "# Load mÃ´ hÃ¬nh vÃ  tokenizer\n",
        "model = BertForQuestionAnswering.from_pretrained(checkpoint_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)"
      ],
      "metadata": {
        "id": "A2sKK5XwNtkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load táº­p test\n",
        "test_dataset = ViQuAD2(\n",
        "    '/content/Private_Test_ref.json',\n",
        "    '/content/ground_truth_private_test.json',\n",
        "    include_labels=True  # Äá» tÃ­nh F1 vÃ  EM náº¿u cáº§n\n",
        ")\n",
        "\n",
        "# Hoáº·c sá»­ dá»¥ng DataLoader náº¿u cáº§n\n",
        "from torch.utils.data import DataLoader\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
      ],
      "metadata": {
        "id": "ySGDZRqsNvFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r compressed_file.zip /content/transformers/results/checkpoint-4584"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7cX1aZCMSak",
        "outputId": "1f2dc8c5-ab32-4462-e924-a17383cbec70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: content/transformers/results/checkpoint-4584/ (stored 0%)\n",
            "  adding: content/transformers/results/checkpoint-4584/vocab.txt (deflated 45%)\n",
            "  adding: content/transformers/results/checkpoint-4584/tokenizer.json (deflated 67%)\n",
            "  adding: content/transformers/results/checkpoint-4584/scheduler.pt (deflated 57%)\n",
            "  adding: content/transformers/results/checkpoint-4584/trainer_state.json (deflated 66%)\n",
            "  adding: content/transformers/results/checkpoint-4584/optimizer.pt (deflated 50%)\n",
            "  adding: content/transformers/results/checkpoint-4584/tokenizer_config.json (deflated 75%)\n",
            "  adding: content/transformers/results/checkpoint-4584/model.safetensors (deflated 7%)\n",
            "  adding: content/transformers/results/checkpoint-4584/training_args.bin (deflated 51%)\n",
            "  adding: content/transformers/results/checkpoint-4584/rng_state.pth (deflated 25%)\n",
            "  adding: content/transformers/results/checkpoint-4584/special_tokens_map.json (deflated 42%)\n",
            "  adding: content/transformers/results/checkpoint-4584/config.json (deflated 53%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8akVXwm_MpK5",
        "outputId": "42369af5-ce97-4ba1-a670-5fcde5519a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/transformers/mBert.zip /content/drive/MyDrive/data"
      ],
      "metadata": {
        "id": "2F1zCh_EOGp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/master/evaluate-v2.0.py -O evaluate_v2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeZJh6JRO1s5",
        "outputId": "3ae63047-87da-4702-8535-57b2d66d8e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-03 17:36:47--  https://raw.githubusercontent.com/rajpurkar/SQuAD-explorer/master/evaluate-v2.0.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10547 (10K) [text/plain]\n",
            "Saving to: âevaluate_v2.pyâ\n",
            "\n",
            "evaluate_v2.py      100%[===================>]  10.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-01-03 17:36:47 (76.6 MB/s) - âevaluate_v2.pyâ saved [10547/10547]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "    --model_name_or_path /content/transformers/results/checkpoint-4584 \\\n",
        "    --test_file /content/transformers/ViQuAD2.0/test/Private_Test_ref.json \\\n",
        "    --do_predict \\\n",
        "    --output_dir /content/transformers/results/predictions \\\n",
        "    --max_seq_length 384 \\\n",
        "    --doc_stride 128 \\\n",
        "    --pad_to_max_length \\\n",
        "    --per_device_eval_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KU0j6G5FPmK8",
        "outputId": "be65fe65-2b63-4d75-cfbf-5febcc514579"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-03 17:47:24.718576: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-03 17:47:24.742040: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-03 17:47:24.748420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-03 17:47:24.763840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-03 17:47:25.982719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/03/2025 17:47:28 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "01/03/2025 17:47:28 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/transformers/results/predictions/runs/Jan03_17-47-28_0811d96ebebd,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/transformers/results/predictions,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/transformers/results/predictions,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-28d194c0c95f7714\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Using custom data configuration default-28d194c0c95f7714\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "01/03/2025 17:47:29 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "Downloading took 0.0 min\n",
            "01/03/2025 17:47:29 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "01/03/2025 17:47:29 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating test split\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 7301 examples [00:00, 19804.37 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "01/03/2025 17:47:29 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "01/03/2025 17:47:29 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:694] 2025-01-03 17:47:29,643 >> loading configuration file /content/transformers/results/checkpoint-4584/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-03 17:47:29,646 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"/content/transformers/results/checkpoint-4584\",\n",
            "  \"architectures\": [\n",
            "    \"BertForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 119547\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-03 17:47:29,647 >> loading file chat_template.jinja\n",
            "[INFO|modeling_utils.py:3890] 2025-01-03 17:47:29,909 >> loading weights file /content/transformers/results/checkpoint-4584/model.safetensors\n",
            "[INFO|modeling_utils.py:4848] 2025-01-03 17:47:30,289 >> All model checkpoint weights were used when initializing BertForQuestionAnswering.\n",
            "\n",
            "[INFO|modeling_utils.py:4856] 2025-01-03 17:47:30,289 >> All the weights of BertForQuestionAnswering were initialized from the model checkpoint at /content/transformers/results/checkpoint-4584.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForQuestionAnswering for predictions without further training.\n",
            "Running tokenizer on prediction dataset:   0% 0/7301 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f7cbcdec0445826a.arrow\n",
            "01/03/2025 17:47:31 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-f7cbcdec0445826a.arrow\n",
            "Running tokenizer on prediction dataset: 100% 7301/7301 [00:08<00:00, 821.66 examples/s]\n",
            "01/03/2025 17:47:44 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:917] 2025-01-03 17:47:44,930 >> The following columns in the test set don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `BertForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:4227] 2025-01-03 17:47:44,938 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4229] 2025-01-03 17:47:44,939 >>   Num examples = 7743\n",
            "[INFO|trainer.py:4232] 2025-01-03 17:47:44,939 >>   Batch size = 16\n",
            "100% 484/484 [02:47<00:00,  2.89it/s]01/03/2025 17:50:44 - INFO - utils_qa - Post-processing 7301 example predictions split into 7743 features.\n",
            "\n",
            "  0% 0/7301 [00:00<?, ?it/s]\u001b[A\n",
            "  0% 34/7301 [00:00<00:21, 331.95it/s]\u001b[A\n",
            "  1% 70/7301 [00:00<00:21, 342.81it/s]\u001b[A\n",
            "  1% 105/7301 [00:00<00:22, 319.90it/s]\u001b[A\n",
            "  2% 140/7301 [00:00<00:21, 330.53it/s]\u001b[A\n",
            "  2% 174/7301 [00:00<00:26, 273.66it/s]\u001b[A\n",
            "  3% 203/7301 [00:00<00:35, 201.60it/s]\u001b[A\n",
            "  3% 227/7301 [00:00<00:36, 192.26it/s]\u001b[A\n",
            "  3% 249/7301 [00:01<00:43, 162.54it/s]\u001b[A\n",
            "  4% 268/7301 [00:01<00:42, 165.84it/s]\u001b[A\n",
            "  4% 300/7301 [00:01<00:34, 201.28it/s]\u001b[A\n",
            "  5% 332/7301 [00:01<00:30, 230.52it/s]\u001b[A\n",
            "  5% 368/7301 [00:01<00:26, 262.54it/s]\u001b[A\n",
            "  5% 400/7301 [00:01<00:24, 276.68it/s]\u001b[A\n",
            "  6% 433/7301 [00:01<00:23, 289.20it/s]\u001b[A\n",
            "  6% 467/7301 [00:01<00:22, 302.50it/s]\u001b[A\n",
            "  7% 502/7301 [00:01<00:21, 313.81it/s]\u001b[A\n",
            "  7% 536/7301 [00:02<00:21, 319.19it/s]\u001b[A\n",
            "  8% 570/7301 [00:02<00:20, 324.81it/s]\u001b[A\n",
            "  8% 603/7301 [00:02<00:21, 311.80it/s]\u001b[A\n",
            "  9% 635/7301 [00:02<00:21, 310.94it/s]\u001b[A\n",
            "  9% 670/7301 [00:02<00:20, 322.16it/s]\u001b[A\n",
            " 10% 705/7301 [00:02<00:20, 328.82it/s]\u001b[A\n",
            " 10% 739/7301 [00:02<00:19, 328.25it/s]\u001b[A\n",
            " 11% 773/7301 [00:02<00:19, 331.02it/s]\u001b[A\n",
            " 11% 808/7301 [00:02<00:19, 335.34it/s]\u001b[A\n",
            " 12% 842/7301 [00:03<00:22, 288.39it/s]\u001b[A\n",
            " 12% 876/7301 [00:03<00:21, 301.82it/s]\u001b[A\n",
            " 12% 909/7301 [00:03<00:20, 305.43it/s]\u001b[A\n",
            " 13% 941/7301 [00:03<00:21, 300.51it/s]\u001b[A\n",
            " 13% 976/7301 [00:03<00:20, 312.45it/s]\u001b[A\n",
            " 14% 1010/7301 [00:03<00:19, 319.89it/s]\u001b[A\n",
            " 14% 1043/7301 [00:03<00:20, 303.69it/s]\u001b[A\n",
            " 15% 1075/7301 [00:03<00:20, 306.09it/s]\u001b[A\n",
            " 15% 1106/7301 [00:03<00:20, 301.33it/s]\u001b[A\n",
            " 16% 1137/7301 [00:04<00:21, 292.37it/s]\u001b[A\n",
            " 16% 1170/7301 [00:04<00:20, 300.75it/s]\u001b[A\n",
            " 16% 1204/7301 [00:04<00:19, 310.46it/s]\u001b[A\n",
            " 17% 1236/7301 [00:04<00:19, 305.17it/s]\u001b[A\n",
            " 17% 1269/7301 [00:04<00:19, 309.97it/s]\u001b[A\n",
            " 18% 1303/7301 [00:04<00:19, 315.25it/s]\u001b[A\n",
            " 18% 1335/7301 [00:04<00:19, 310.19it/s]\u001b[A\n",
            " 19% 1370/7301 [00:04<00:18, 321.63it/s]\u001b[A\n",
            " 19% 1406/7301 [00:04<00:17, 331.34it/s]\u001b[A\n",
            " 20% 1442/7301 [00:04<00:17, 337.64it/s]\u001b[A\n",
            " 20% 1476/7301 [00:05<00:18, 315.54it/s]\u001b[A\n",
            " 21% 1510/7301 [00:05<00:18, 320.59it/s]\u001b[A\n",
            " 21% 1543/7301 [00:05<00:17, 320.17it/s]\u001b[A\n",
            " 22% 1576/7301 [00:05<00:18, 315.40it/s]\u001b[A\n",
            " 22% 1611/7301 [00:05<00:17, 323.34it/s]\u001b[A\n",
            " 23% 1644/7301 [00:05<00:19, 283.38it/s]\u001b[A\n",
            " 23% 1674/7301 [00:05<00:23, 244.45it/s]\u001b[A\n",
            " 23% 1700/7301 [00:05<00:25, 223.21it/s]\u001b[A\n",
            " 24% 1724/7301 [00:06<00:26, 209.54it/s]\u001b[A\n",
            " 24% 1746/7301 [00:06<00:27, 202.24it/s]\u001b[A\n",
            " 24% 1767/7301 [00:06<00:28, 196.55it/s]\u001b[A\n",
            " 24% 1787/7301 [00:06<00:29, 188.62it/s]\u001b[A\n",
            " 25% 1807/7301 [00:06<00:30, 182.07it/s]\u001b[A\n",
            " 25% 1826/7301 [00:06<00:30, 179.66it/s]\u001b[A\n",
            " 25% 1845/7301 [00:06<00:29, 181.91it/s]\u001b[A\n",
            " 26% 1864/7301 [00:06<00:31, 174.90it/s]\u001b[A\n",
            " 26% 1885/7301 [00:07<00:29, 183.38it/s]\u001b[A\n",
            " 26% 1904/7301 [00:07<00:29, 181.92it/s]\u001b[A\n",
            " 26% 1923/7301 [00:07<00:29, 183.56it/s]\u001b[A\n",
            " 27% 1942/7301 [00:07<00:28, 184.97it/s]\u001b[A\n",
            " 27% 1961/7301 [00:07<00:29, 180.16it/s]\u001b[A\n",
            " 27% 1980/7301 [00:07<00:30, 173.60it/s]\u001b[A\n",
            " 27% 1998/7301 [00:07<00:31, 169.04it/s]\u001b[A\n",
            " 28% 2016/7301 [00:07<00:31, 169.99it/s]\u001b[A\n",
            " 28% 2035/7301 [00:07<00:30, 175.02it/s]\u001b[A\n",
            " 28% 2054/7301 [00:07<00:29, 177.95it/s]\u001b[A\n",
            " 28% 2072/7301 [00:08<00:30, 168.80it/s]\u001b[A\n",
            " 29% 2090/7301 [00:08<00:30, 171.42it/s]\u001b[A\n",
            " 29% 2108/7301 [00:08<00:31, 167.45it/s]\u001b[A\n",
            " 29% 2125/7301 [00:08<00:31, 166.10it/s]\u001b[A\n",
            " 29% 2143/7301 [00:08<00:30, 169.20it/s]\u001b[A\n",
            " 30% 2160/7301 [00:08<00:30, 169.17it/s]\u001b[A\n",
            " 30% 2177/7301 [00:08<00:30, 165.76it/s]\u001b[A\n",
            " 30% 2196/7301 [00:08<00:30, 169.92it/s]\u001b[A\n",
            " 30% 2214/7301 [00:08<00:30, 168.70it/s]\u001b[A\n",
            " 31% 2231/7301 [00:09<00:30, 166.56it/s]\u001b[A\n",
            " 31% 2248/7301 [00:09<00:30, 166.53it/s]\u001b[A\n",
            " 31% 2266/7301 [00:09<00:29, 168.65it/s]\u001b[A\n",
            " 31% 2283/7301 [00:09<00:30, 167.17it/s]\u001b[A\n",
            " 32% 2300/7301 [00:09<00:30, 165.50it/s]\u001b[A\n",
            " 32% 2318/7301 [00:09<00:29, 166.67it/s]\u001b[A\n",
            " 32% 2335/7301 [00:09<00:30, 165.49it/s]\u001b[A\n",
            " 32% 2352/7301 [00:09<00:29, 164.98it/s]\u001b[A\n",
            " 32% 2369/7301 [00:09<00:30, 162.95it/s]\u001b[A\n",
            " 33% 2387/7301 [00:09<00:29, 167.37it/s]\u001b[A\n",
            " 33% 2406/7301 [00:10<00:28, 172.76it/s]\u001b[A\n",
            " 33% 2424/7301 [00:10<00:28, 174.04it/s]\u001b[A\n",
            " 33% 2443/7301 [00:10<00:27, 176.59it/s]\u001b[A\n",
            " 34% 2461/7301 [00:10<00:27, 176.88it/s]\u001b[A\n",
            " 34% 2479/7301 [00:10<00:27, 175.66it/s]\u001b[A\n",
            " 34% 2509/7301 [00:10<00:22, 210.76it/s]\u001b[A\n",
            " 35% 2538/7301 [00:10<00:20, 233.76it/s]\u001b[A\n",
            " 35% 2570/7301 [00:10<00:18, 258.30it/s]\u001b[A\n",
            " 36% 2599/7301 [00:10<00:17, 267.40it/s]\u001b[A\n",
            " 36% 2626/7301 [00:11<00:18, 253.44it/s]\u001b[A\n",
            " 36% 2658/7301 [00:11<00:17, 271.98it/s]\u001b[A\n",
            " 37% 2691/7301 [00:11<00:16, 287.91it/s]\u001b[A\n",
            " 37% 2724/7301 [00:11<00:15, 299.00it/s]\u001b[A\n",
            " 38% 2757/7301 [00:11<00:14, 304.39it/s]\u001b[A\n",
            " 38% 2788/7301 [00:11<00:15, 293.67it/s]\u001b[A\n",
            " 39% 2818/7301 [00:11<00:15, 289.88it/s]\u001b[A\n",
            " 39% 2848/7301 [00:11<00:16, 276.34it/s]\u001b[A\n",
            " 39% 2880/7301 [00:11<00:15, 287.83it/s]\u001b[A\n",
            " 40% 2913/7301 [00:11<00:14, 299.44it/s]\u001b[A\n",
            " 40% 2948/7301 [00:12<00:13, 313.69it/s]\u001b[A\n",
            " 41% 2981/7301 [00:12<00:13, 316.56it/s]\u001b[A\n",
            " 41% 3014/7301 [00:12<00:13, 319.21it/s]\u001b[A\n",
            " 42% 3047/7301 [00:12<00:13, 318.98it/s]\u001b[A\n",
            " 42% 3079/7301 [00:12<00:13, 317.94it/s]\u001b[A\n",
            " 43% 3114/7301 [00:12<00:12, 325.63it/s]\u001b[A\n",
            " 43% 3147/7301 [00:12<00:13, 316.19it/s]\u001b[A\n",
            " 44% 3179/7301 [00:12<00:14, 286.93it/s]\u001b[A\n",
            " 44% 3211/7301 [00:12<00:13, 293.79it/s]\u001b[A\n",
            " 44% 3246/7301 [00:13<00:13, 308.07it/s]\u001b[A\n",
            " 45% 3278/7301 [00:13<00:13, 295.33it/s]\u001b[A\n",
            " 45% 3311/7301 [00:13<00:13, 303.97it/s]\u001b[A\n",
            " 46% 3342/7301 [00:13<00:14, 276.74it/s]\u001b[A\n",
            " 46% 3371/7301 [00:13<00:15, 259.37it/s]\u001b[A\n",
            " 47% 3406/7301 [00:13<00:13, 281.10it/s]\u001b[A\n",
            " 47% 3435/7301 [00:13<00:13, 279.23it/s]\u001b[A\n",
            " 47% 3464/7301 [00:13<00:13, 275.89it/s]\u001b[A\n",
            " 48% 3496/7301 [00:13<00:13, 287.55it/s]\u001b[A\n",
            " 48% 3526/7301 [00:14<00:13, 285.40it/s]\u001b[A\n",
            " 49% 3558/7301 [00:14<00:12, 293.60it/s]\u001b[A\n",
            " 49% 3591/7301 [00:14<00:12, 302.85it/s]\u001b[A\n",
            " 50% 3622/7301 [00:14<00:12, 301.22it/s]\u001b[A\n",
            " 50% 3655/7301 [00:14<00:11, 307.58it/s]\u001b[A\n",
            " 51% 3688/7301 [00:14<00:11, 313.46it/s]\u001b[A\n",
            " 51% 3722/7301 [00:14<00:11, 320.52it/s]\u001b[A\n",
            " 51% 3755/7301 [00:14<00:12, 287.33it/s]\u001b[A\n",
            " 52% 3788/7301 [00:14<00:11, 297.57it/s]\u001b[A\n",
            " 52% 3821/7301 [00:14<00:11, 305.41it/s]\u001b[A\n",
            " 53% 3852/7301 [00:15<00:13, 250.38it/s]\u001b[A\n",
            " 53% 3883/7301 [00:15<00:12, 263.78it/s]\u001b[A\n",
            " 54% 3911/7301 [00:15<00:12, 262.91it/s]\u001b[A\n",
            " 54% 3942/7301 [00:15<00:12, 275.43it/s]\u001b[A\n",
            " 54% 3971/7301 [00:15<00:12, 263.63it/s]\u001b[A\n",
            " 55% 4005/7301 [00:15<00:11, 283.71it/s]\u001b[A\n",
            " 55% 4035/7301 [00:15<00:11, 279.66it/s]\u001b[A\n",
            " 56% 4068/7301 [00:15<00:11, 293.14it/s]\u001b[A\n",
            " 56% 4099/7301 [00:16<00:10, 296.63it/s]\u001b[A\n",
            " 57% 4129/7301 [00:16<00:10, 296.74it/s]\u001b[A\n",
            " 57% 4163/7301 [00:16<00:10, 307.54it/s]\u001b[A\n",
            " 57% 4194/7301 [00:16<00:10, 292.41it/s]\u001b[A\n",
            " 58% 4227/7301 [00:16<00:10, 302.17it/s]\u001b[A\n",
            " 58% 4262/7301 [00:16<00:09, 313.96it/s]\u001b[A\n",
            " 59% 4295/7301 [00:16<00:09, 318.51it/s]\u001b[A\n",
            " 59% 4327/7301 [00:16<00:09, 315.82it/s]\u001b[A\n",
            " 60% 4359/7301 [00:16<00:11, 249.45it/s]\u001b[A\n",
            " 60% 4393/7301 [00:17<00:10, 271.70it/s]\u001b[A\n",
            " 61% 4425/7301 [00:17<00:10, 283.68it/s]\u001b[A\n",
            " 61% 4459/7301 [00:17<00:09, 296.80it/s]\u001b[A\n",
            " 61% 4490/7301 [00:17<00:10, 278.31it/s]\u001b[A\n",
            " 62% 4525/7301 [00:17<00:09, 297.40it/s]\u001b[A\n",
            " 62% 4559/7301 [00:17<00:08, 307.42it/s]\u001b[A\n",
            " 63% 4591/7301 [00:17<00:08, 309.15it/s]\u001b[A\n",
            " 63% 4625/7301 [00:17<00:08, 317.79it/s]\u001b[A\n",
            " 64% 4658/7301 [00:17<00:08, 306.52it/s]\u001b[A\n",
            " 64% 4692/7301 [00:17<00:08, 315.89it/s]\u001b[A\n",
            " 65% 4725/7301 [00:18<00:08, 318.25it/s]\u001b[A\n",
            " 65% 4759/7301 [00:18<00:07, 324.20it/s]\u001b[A\n",
            " 66% 4792/7301 [00:18<00:07, 324.77it/s]\u001b[A\n",
            " 66% 4825/7301 [00:18<00:07, 321.32it/s]\u001b[A\n",
            " 67% 4858/7301 [00:18<00:07, 323.41it/s]\u001b[A\n",
            " 67% 4891/7301 [00:18<00:07, 313.26it/s]\u001b[A\n",
            " 67% 4923/7301 [00:18<00:07, 309.57it/s]\u001b[A\n",
            " 68% 4958/7301 [00:18<00:07, 316.74it/s]\u001b[A\n",
            " 68% 4990/7301 [00:18<00:07, 290.89it/s]\u001b[A\n",
            " 69% 5020/7301 [00:19<00:07, 291.89it/s]\u001b[A\n",
            " 69% 5052/7301 [00:19<00:07, 297.53it/s]\u001b[A\n",
            " 70% 5082/7301 [00:19<00:07, 292.37it/s]\u001b[A\n",
            " 70% 5116/7301 [00:19<00:07, 304.40it/s]\u001b[A\n",
            " 71% 5148/7301 [00:19<00:07, 307.41it/s]\u001b[A\n",
            " 71% 5183/7301 [00:19<00:06, 317.46it/s]\u001b[A\n",
            " 71% 5215/7301 [00:19<00:06, 317.29it/s]\u001b[A\n",
            " 72% 5247/7301 [00:19<00:06, 308.28it/s]\u001b[A\n",
            " 72% 5282/7301 [00:19<00:06, 318.99it/s]\u001b[A\n",
            " 73% 5314/7301 [00:19<00:06, 311.42it/s]\u001b[A\n",
            " 73% 5348/7301 [00:20<00:06, 319.51it/s]\u001b[A\n",
            " 74% 5383/7301 [00:20<00:05, 326.31it/s]\u001b[A\n",
            " 74% 5416/7301 [00:20<00:05, 326.97it/s]\u001b[A\n",
            " 75% 5450/7301 [00:20<00:05, 328.38it/s]\u001b[A\n",
            " 75% 5483/7301 [00:20<00:05, 324.35it/s]\u001b[A\n",
            " 76% 5516/7301 [00:20<00:07, 252.42it/s]\u001b[A\n",
            " 76% 5544/7301 [00:20<00:07, 229.59it/s]\u001b[A\n",
            " 76% 5569/7301 [00:21<00:08, 210.69it/s]\u001b[A\n",
            " 77% 5592/7301 [00:21<00:08, 202.26it/s]\u001b[A\n",
            " 77% 5614/7301 [00:21<00:08, 192.19it/s]\u001b[A\n",
            " 77% 5634/7301 [00:21<00:08, 185.46it/s]\u001b[A\n",
            " 77% 5653/7301 [00:21<00:09, 181.83it/s]\u001b[A\n",
            " 78% 5672/7301 [00:21<00:09, 177.33it/s]\u001b[A\n",
            " 78% 5690/7301 [00:21<00:09, 162.20it/s]\u001b[A\n",
            " 78% 5707/7301 [00:21<00:11, 144.63it/s]\u001b[A\n",
            " 78% 5722/7301 [00:22<00:11, 139.49it/s]\u001b[A\n",
            " 79% 5742/7301 [00:22<00:10, 153.42it/s]\u001b[A\n",
            " 79% 5759/7301 [00:22<00:09, 157.34it/s]\u001b[A\n",
            " 79% 5778/7301 [00:22<00:09, 164.42it/s]\u001b[A\n",
            " 79% 5797/7301 [00:22<00:08, 170.28it/s]\u001b[A\n",
            " 80% 5816/7301 [00:22<00:08, 173.24it/s]\u001b[A\n",
            " 80% 5835/7301 [00:22<00:08, 176.71it/s]\u001b[A\n",
            " 80% 5854/7301 [00:22<00:08, 177.57it/s]\u001b[A\n",
            " 80% 5873/7301 [00:22<00:07, 178.85it/s]\u001b[A\n",
            " 81% 5892/7301 [00:22<00:07, 181.29it/s]\u001b[A\n",
            " 81% 5911/7301 [00:23<00:07, 179.26it/s]\u001b[A\n",
            " 81% 5929/7301 [00:23<00:07, 174.67it/s]\u001b[A\n",
            " 81% 5947/7301 [00:23<00:07, 174.71it/s]\u001b[A\n",
            " 82% 5965/7301 [00:23<00:07, 171.97it/s]\u001b[A\n",
            " 82% 5983/7301 [00:23<00:07, 168.89it/s]\u001b[A\n",
            " 82% 6001/7301 [00:23<00:07, 171.11it/s]\u001b[A\n",
            " 82% 6019/7301 [00:23<00:07, 173.34it/s]\u001b[A\n",
            " 83% 6038/7301 [00:23<00:07, 176.43it/s]\u001b[A\n",
            " 83% 6056/7301 [00:23<00:07, 174.25it/s]\u001b[A\n",
            " 83% 6074/7301 [00:24<00:07, 174.70it/s]\u001b[A\n",
            " 83% 6092/7301 [00:24<00:06, 173.18it/s]\u001b[A\n",
            " 84% 6110/7301 [00:24<00:06, 174.80it/s]\u001b[A\n",
            " 84% 6129/7301 [00:24<00:06, 177.16it/s]\u001b[A\n",
            " 84% 6148/7301 [00:24<00:06, 178.05it/s]\u001b[A\n",
            " 84% 6166/7301 [00:24<00:06, 175.39it/s]\u001b[A\n",
            " 85% 6184/7301 [00:24<00:06, 166.36it/s]\u001b[A\n",
            " 85% 6202/7301 [00:24<00:06, 169.24it/s]\u001b[A\n",
            " 85% 6220/7301 [00:24<00:06, 166.60it/s]\u001b[A\n",
            " 85% 6237/7301 [00:24<00:06, 161.30it/s]\u001b[A\n",
            " 86% 6256/7301 [00:25<00:06, 167.40it/s]\u001b[A\n",
            " 86% 6273/7301 [00:25<00:06, 166.00it/s]\u001b[A\n",
            " 86% 6290/7301 [00:25<00:06, 163.00it/s]\u001b[A\n",
            " 86% 6308/7301 [00:25<00:05, 165.86it/s]\u001b[A\n",
            " 87% 6326/7301 [00:25<00:05, 168.66it/s]\u001b[A\n",
            " 87% 6346/7301 [00:25<00:05, 176.21it/s]\u001b[A\n",
            " 87% 6378/7301 [00:25<00:04, 214.87it/s]\u001b[A\n",
            " 88% 6408/7301 [00:25<00:03, 239.56it/s]\u001b[A\n",
            " 88% 6441/7301 [00:25<00:03, 265.86it/s]\u001b[A\n",
            " 89% 6475/7301 [00:26<00:02, 286.20it/s]\u001b[A\n",
            " 89% 6509/7301 [00:26<00:02, 300.86it/s]\u001b[A\n",
            " 90% 6540/7301 [00:26<00:02, 281.96it/s]\u001b[A\n",
            " 90% 6574/7301 [00:26<00:02, 296.08it/s]\u001b[A\n",
            " 91% 6611/7301 [00:26<00:02, 315.30it/s]\u001b[A\n",
            " 91% 6646/7301 [00:26<00:02, 323.11it/s]\u001b[A\n",
            " 92% 6681/7301 [00:26<00:01, 329.97it/s]\u001b[A\n",
            " 92% 6715/7301 [00:26<00:01, 332.22it/s]\u001b[A\n",
            " 92% 6749/7301 [00:26<00:01, 332.80it/s]\u001b[A\n",
            " 93% 6783/7301 [00:26<00:01, 328.91it/s]\u001b[A\n",
            " 93% 6818/7301 [00:27<00:01, 333.71it/s]\u001b[A\n",
            " 94% 6852/7301 [00:27<00:01, 329.68it/s]\u001b[A\n",
            " 94% 6886/7301 [00:27<00:01, 302.62it/s]\u001b[A\n",
            " 95% 6919/7301 [00:27<00:01, 309.03it/s]\u001b[A\n",
            " 95% 6953/7301 [00:27<00:01, 316.14it/s]\u001b[A\n",
            " 96% 6987/7301 [00:27<00:00, 322.17it/s]\u001b[A\n",
            " 96% 7020/7301 [00:27<00:00, 316.40it/s]\u001b[A\n",
            " 97% 7052/7301 [00:27<00:00, 315.95it/s]\u001b[A\n",
            " 97% 7084/7301 [00:27<00:00, 303.86it/s]\u001b[A\n",
            " 97% 7115/7301 [00:28<00:00, 300.18it/s]\u001b[A\n",
            " 98% 7146/7301 [00:28<00:00, 276.91it/s]\u001b[A\n",
            " 98% 7177/7301 [00:28<00:00, 281.52it/s]\u001b[A\n",
            " 99% 7206/7301 [00:28<00:00, 256.61it/s]\u001b[A\n",
            " 99% 7238/7301 [00:28<00:00, 269.43it/s]\u001b[A\n",
            "100% 7268/7301 [00:28<00:00, 276.45it/s]\u001b[A\n",
            "100% 7301/7301 [00:28<00:00, 253.98it/s]\n",
            "01/03/2025 17:51:13 - INFO - utils_qa - Saving predictions to /content/transformers/results/predictions/predict_predictions.json.\n",
            "01/03/2025 17:51:13 - INFO - utils_qa - Saving nbest_preds to /content/transformers/results/predictions/predict_nbest_predictions.json.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 715, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 683, in main\n",
            "    results = trainer.predict(predict_dataset, predict_examples)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 130, in predict\n",
            "    metrics = self.compute_metrics(predictions)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 634, in compute_metrics\n",
            "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/evaluate/module.py\", line 467, in compute\n",
            "    output = self._compute(**inputs, **compute_kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/squad.py\", line 110, in _compute\n",
            "    score = compute_score(dataset=dataset, predictions=pred_dict)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 67, in compute_score\n",
            "    exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 52, in metric_max_over_ground_truths\n",
            "    return max(scores_for_ground_truths)\n",
            "ValueError: max() arg is an empty sequence\n",
            "100% 484/484 [03:30<00:00,  2.30it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def get_ground_truth_format(path):\n",
        "  with open(path, 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "  res = {}\n",
        "  for i in range(len(json_data[\"data\"])):\n",
        "    data = json_data[\"data\"][i][\"paragraphs\"]\n",
        "    for qa in data:\n",
        "      for content in qa[\"qas\"]:\n",
        "        if content[\"is_impossible\"] == 0:\n",
        "          res[content[\"id\"]] = content[\"answers\"][-1][\"text\"]\n",
        "        else:\n",
        "          res[content[\"id\"]] = content[\"plausible_answers\"][-1][\"text\"]\n",
        "  with open('/content/ground_truth.json', 'w') as f:\n",
        "    json.dump(res, f)\n",
        "  return res"
      ],
      "metadata": {
        "id": "Sux5k9oSTmU5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_ground_truth_format('/content/ViQuAD2.0/test/ground_truth_private_test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZYPsnW5VCa0",
        "outputId": "8a8a9245-85e7-4261-a450-40bb01a9551e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'uit_000001': 'ThÃ¡i BÃ¬nh DÆ°Æ¡ng',\n",
              " 'uit_000002': 'trong thung lÅ©ng Trung tÃ¢m',\n",
              " 'uit_000003': '411,000 km2 (160,000 mi2)',\n",
              " 'uit_000004': 'Báº¯c Fork',\n",
              " 'uit_000005': 'California',\n",
              " 'uit_000006': 'tiá»u bang Baja California',\n",
              " 'uit_000007': 'trong thung lÅ©ng Trung tÃ¢m',\n",
              " 'uit_000008': '411,000 km2',\n",
              " 'uit_000009': 'tiá»u bang Baja California',\n",
              " 'uit_000010': 'sÃ¡t hay gáº§n bá» biá»n ThÃ¡i BÃ¬nh DÆ°Æ¡ng',\n",
              " 'uit_000011': 'dÃ£y nÃºi ÄÃ¡ granit Sierra Nevada',\n",
              " 'uit_000012': 'vá»«a lÃ  cá»­a sÃ´ng quan trá»ng há» trá»£ há» sinh thÃ¡i nÆ°á»c máº·n vÃ  vá»«a lÃ  nguá»n nÆ°á»c chá»§ yáº¿u cá»§a pháº§n lá»n dÃ¢n cÆ° tiá»u bang',\n",
              " 'uit_000013': 'dÃ£y nÃºi ÄÃ¡ granit Sierra Nevada',\n",
              " 'uit_000014': 'vá»«a lÃ  cá»­a sÃ´ng quan trá»ng há» trá»£ há» sinh thÃ¡i nÆ°á»c máº·n vÃ  vá»«a lÃ  nguá»n nÆ°á»c chá»§ yáº¿u cá»§a pháº§n lá»n dÃ¢n cÆ° tiá»u bang',\n",
              " 'uit_000015': 'Tehachapi',\n",
              " 'uit_000016': 'viá»c trá»ng trá»t bá» tÃ n phÃ¡',\n",
              " 'uit_000017': 'nhiá»t Äá» tháº¥p gáº§n Äiá»m ÄÃ´ng trong mÃ¹a ÄÃ´ng',\n",
              " 'uit_000018': 'vÃ i con sÃ´ng ÄÃ£ Äá»§ rá»ng vÃ  sÃ¢u Äá» cho vÃ i thÃ nh phá» ná»i Äá»a (nháº¥t lÃ  Stockton) ÄÆ°á»£c trá» thÃ nh háº£i cáº£ng',\n",
              " 'uit_000019': 'há» trá»£ há» thá»ng nÆ°á»c cá»§a má»t sá» thÃ nh phá», nhÆ°ng chá»§ yáº¿u cung cáº¥p cho viá»c tÆ°á»i tiÃªu nÃ´ng nghiá»p',\n",
              " 'uit_000020': '4,421 mÃ©t',\n",
              " 'uit_000021': '4,421 mÃ©t (14,505 feet)',\n",
              " 'uit_000022': 'chim biá»n',\n",
              " 'uit_000023': 'Clear',\n",
              " 'uit_000024': 'há» Clear',\n",
              " 'uit_000025': 'cá»±c nam',\n",
              " 'uit_000026': 'há» Tahoe',\n",
              " 'uit_000027': 'cá»±c nam',\n",
              " 'uit_000028': 'phÃ­a tÃ¢y báº¯c tiá»u bang vÃ  triá»n phÃ­a tÃ¢y dÃ£y Sierra Nevada',\n",
              " 'uit_000029': 'khoáº£ng 35%',\n",
              " 'uit_000030': 'phÃ­a tÃ¢y báº¯c tiá»u bang vÃ  triá»n phÃ­a tÃ¢y dÃ£y Sierra Nevada',\n",
              " 'uit_000031': 'Alaska',\n",
              " 'uit_000032': 'dá»c theo nhá»¯ng dÃ£y nÃºi California gáº§n bá» biá»n hÆ¡n, vÃ  cáº£ nhá»¯ng Äá»i tháº¥p dÆ°á»i chÃ¢n dÃ£y Sierra Nevada',\n",
              " 'uit_000033': 'thÃ´ng',\n",
              " 'uit_000034': 'Alaska',\n",
              " 'uit_000035': '35%',\n",
              " 'uit_000036': 'dá»c theo nhá»¯ng dÃ£y nÃºi California gáº§n bá» biá»n hÆ¡n, vÃ  cáº£ nhá»¯ng Äá»i tháº¥p dÆ°á»i chÃ¢n dÃ£y Sierra Nevada',\n",
              " 'uit_000037': 'biá»n Salton',\n",
              " 'uit_000038': 'ráº¥t cao',\n",
              " 'uit_000039': 'ÄÃ´ng nam',\n",
              " 'uit_000040': '25%',\n",
              " 'uit_000041': 'lÃ  thung lÅ©ng Cháº¿t, lÃ  nÆ¡i cÃ³ Badwater Flat â Äiá»m tháº¥p nháº¥t vÃ  nÃ³ng nháº¥t cá»§a Báº¯c Má»¹',\n",
              " 'uit_000042': 'lÃ  nÆ¡i cÃ³ Badwater Flat â Äiá»m tháº¥p nháº¥t vÃ  nÃ³ng nháº¥t cá»§a Báº¯c Má»¹',\n",
              " 'uit_000043': 'Ã­t hÆ¡n 322 km (200 dáº·m)',\n",
              " 'uit_000044': 'biá»n Salton',\n",
              " 'uit_000045': '25%',\n",
              " 'uit_000046': 'Viá»c buÃ´n bÃ¡n, hÃ´n nhÃ¢n khÃ¡c dÃ¢n tá»c, vÃ  liÃªn minh quÃ¢n sá»±',\n",
              " 'uit_000047': 'Viá»c buÃ´n bÃ¡n, hÃ´n nhÃ¢n khÃ¡c dÃ¢n tá»c, vÃ  liÃªn minh quÃ¢n sá»±',\n",
              " 'uit_000048': 'nhÃ³m, bá» láº¡c, tiá»u bá» láº¡c, vÃ  cÃ¡c cá»ng Äá»ng lá»n hÆ¡n trÃªn bá» biá»n dá»i dÃ o tÃ i nguyÃªn',\n",
              " 'uit_000049': 'Äi sÄn thÃº rá»«ng vÃ  hÃ¡i lÆ°á»£m nhá»¯ng quáº£ háº¡ch, quáº£ Äáº§u, vÃ  quáº£ má»ng',\n",
              " 'uit_000050': 'má»t trong nhá»¯ng vÃ¹ng Äa dáº¡ng vá» vÄn hÃ³a vÃ  ngÃ´n ngá»¯ nháº¥t á» Báº¯c Má»¹ thá»i thá» dÃ¢n',\n",
              " 'uit_000051': 'sÄn nhá»¯ng con thÃº biá»n, cÃ¢u cÃ¡ há»i vÃ  thu nháº·t tÃ´m cua',\n",
              " 'uit_000052': 'má»t trong nhá»¯ng vÃ¹ng Äa dáº¡ng vá» vÄn hÃ³a vÃ  ngÃ´n ngá»¯ nháº¥t á» Báº¯c Má»¹ thá»i thá» dÃ¢n',\n",
              " 'uit_000053': 'Äi sÄn thÃº rá»«ng vÃ  hÃ¡i lÆ°á»£m nhá»¯ng quáº£ háº¡ch, quáº£ Äáº§u, vÃ  quáº£ má»ng',\n",
              " 'uit_000054': 'sÄn thÃº rá»«ng vÃ  hÃ¡i lÆ°á»£m nhá»¯ng quáº£ háº¡ch, quáº£ Äáº§u, vÃ  quáº£ má»ng',\n",
              " 'uit_000055': 'theo hÃ²n Äáº£o láº¡c viÃªn California trong Las sergas de EsplandiÃ¡n (CÃ¡c truyá»n phiÃªu lÆ°u cá»§a Splandian)',\n",
              " 'uit_000056': 'California',\n",
              " 'uit_000057': 'vÃ¹ng tÃ¢y báº¯c cá»§a Äáº¿ quá»c TÃ¢y Ban Nha, tá»©c lÃ  bÃ¡n Äáº£o Baja California (Háº¡ California)',\n",
              " 'uit_000058': 'Alta California (ThÆ°á»£ng California)',\n",
              " 'uit_000059': 'Alta California',\n",
              " 'uit_000060': 'nhá»¯ng ranh giá»i cá»§a biá»n Cortez vÃ  bá» biá»n ThÃ¡i BÃ¬nh DÆ°Æ¡ng chÆ°a ÄÆ°á»£c thÃ¡m hiá»m Äáº§y Äá»§',\n",
              " 'uit_000061': 'nhá»¯ng ranh giá»i cá»§a biá»n Cortez vÃ  bá» biá»n ThÃ¡i BÃ¬nh DÆ°Æ¡ng chÆ°a ÄÆ°á»£c thÃ¡m hiá»m Äáº§y Äá»§',\n",
              " 'uit_000062': 'Garci RodrÃ­guez de Montalvo',\n",
              " 'uit_000063': 'theo hÃ²n Äáº£o láº¡c viÃªn California trong Las sergas de EsplandiÃ¡n',\n",
              " 'uit_000064': '1821',\n",
              " 'uit_000065': 'cuá»i tháº¿ ká»· 18',\n",
              " 'uit_000066': 'CÃ¡c tráº¡i ráº¥t lá»n nuÃ´i bÃ²',\n",
              " 'uit_000067': 'ngÆ°á»i thá» dÃ¢n',\n",
              " 'uit_000068': 'CÃ¡c tráº¡i ráº¥t lá»n nuÃ´i bÃ²',\n",
              " 'uit_000069': 'Mexico giÃ nh ÄÆ°á»£c Äá»c láº­p trong cuá»c Chiáº¿n tranh Äá»c láº­p Mexico',\n",
              " 'uit_000070': 'khi Mexico giÃ nh ÄÆ°á»£c Äá»c láº­p trong cuá»c Chiáº¿n tranh Äá»c láº­p Mexico (1810â1821)',\n",
              " 'uit_000071': 'CÃ¡c thÆ°Æ¡ng gia vÃ  thá»±c dÃ¢n báº¯t Äáº§u Äáº¿n tá»« Hoa Ká»³',\n",
              " 'uit_000072': 'muá»i vÃ  bá» chÃ©t',\n",
              " 'uit_000073': 'khÃ´ng cÃ³ nhiá»u ngÆ°á»i sinh sá»ng',\n",
              " 'uit_000074': 'xÃ¢y dá»±ng má»t sá» phÃ¡o ÄÃ i (presidio)',\n",
              " 'uit_000075': 'sá»t vÃ ng, sá»t rÃ©t, vÃ  dá»ch háº¡ch gÃ¢y ra bá»i muá»i vÃ  bá» chÃ©t',\n",
              " 'uit_000076': 'bÃ¹ng ná» cÃ¡c bá»nh sá»t vÃ ng, sá»t rÃ©t, vÃ  dá»ch háº¡ch gÃ¢y ra bá»i muá»i vÃ  bá» chÃ©t',\n",
              " 'uit_000077': 'do Sa hoÃ ng khÃ´ng quan tÃ¢m',\n",
              " 'uit_000078': 'phÃ­a nam',\n",
              " 'uit_000079': 'Baja California vÃ  Baja California Sur',\n",
              " 'uit_000080': 'má»t con gáº¥u vÃ ng vÃ  má»t ngÃ´i sao',\n",
              " 'uit_000081': 'Baja California vÃ  Baja California Sur',\n",
              " 'uit_000082': 'Thiáº¿u tÆ°á»ng John D. Sloat cá»§a Háº£i quÃ¢n Hoa Ká»³ tiáº¿n vÃ o vá»nh San Francisco vÃ  tuyÃªn bá» chá»§ quyá»n cá»§a Hoa Ká»³ Äá»i vá»i California',\n",
              " 'uit_000083': '1846â1848',\n",
              " 'uit_000084': 'Alta California',\n",
              " 'uit_000085': 'Thiáº¿u tÆ°á»ng John D. Sloat cá»§a Háº£i quÃ¢n Hoa Ká»³ tiáº¿n vÃ o vá»nh San Francisco vÃ  tuyÃªn bá» chá»§ quyá»n cá»§a Hoa Ká»³ Äá»i vá»i California',\n",
              " 'uit_000086': 'Alta California',\n",
              " 'uit_000087': 'ÄÆ°á»ng xe lá»­a xuyÃªn lá»¥c Äá»a Äáº§u tiÃªn ÄÆ°á»£c hoÃ n thÃ nh',\n",
              " 'uit_000088': 'náº¿u tÆ°á»i Äáº¥t vÃ o nhá»¯ng thÃ¡ng hÃ¨ khÃ´ cáº¡n, Äáº¥t ÄÃ³ ráº¥t há»£p Äá» trá»ng cÃ¢y Än quáº£ vÃ  lÃ m nÃ´ng nghiá»p',\n",
              " 'uit_000089': 'Äi theo cÃ¡c chuyáº¿n ÄÆ°á»ng biá»n dÃ i hoáº·c Äi báº±ng xe ngá»±a hay Äi bá» ráº¥t khÃ³ khÄn trÃªn nhá»¯ng con ÄÆ°á»ng Äáº¥t',\n",
              " 'uit_000090': 'cÃ¢y Än quáº£ vÃ  lÃ m nÃ´ng nghiá»p nÃ³i chung',\n",
              " 'uit_000091': 'cam quÃ½t',\n",
              " 'uit_000092': 'viá»c Äi láº¡i láº¡i giá»¯a miá»n TÃ¢y vÃ  cÃ¡c trung tÃ¢m á» miá»n ÄÃ´ng tá»n thÃ¬ giá» vÃ  nguy hiá»m',\n",
              " 'uit_000093': 'tá»n thÃ¬ giá» vÃ  nguy hiá»m',\n",
              " 'uit_000094': 'CÃ¡c loáº¡i cÃ¢y giá»ng cam quÃ½t',\n",
              " 'uit_000095': 'má»t trong nhá»¯ng Äá»a Äiá»m cÃ³ nhiá»u loáº¡i ngÆ°á»i nháº¥t trÃªn tháº¿ giá»i',\n",
              " 'uit_000096': 'California',\n",
              " 'uit_000097': 'California',\n",
              " 'uit_000098': 'ÄÆ°á»ng Lincoln vÃ  Xa lá» 66',\n",
              " 'uit_000099': 'hoÃ n thÃ nh nhá»¯ng con ÄÆ°á»ng xuyÃªn lá»¥c Äá»a lá»n',\n",
              " 'uit_000100': 'sá»± di trÃº Äáº¿n California tÄng nhanh',\n",
              " 'uit_000101': 'ká»¹ thuáº­t vÃ  vÄn hÃ³a, vÃ  lÃ  trung tÃ¢m quá»c táº¿ vá» cÃ´ng ty ká»¹ thuáº­t, ngÃ nh cÃ´ng nghiá»p Äiá»n áº£nh vÃ  truyá»n hÃ¬nh, cÃ´ng nghiá»p Ã¢m nháº¡c',\n",
              " 'uit_000102': 'gáº§n má»t triá»u',\n",
              " 'uit_000103': 'gáº§n má»t triá»u',\n",
              " 'uit_000104': 'California trá» thÃ nh má»t trong nhá»¯ng Äá»a Äiá»m cÃ³ nhiá»u loáº¡i ngÆ°á»i nháº¥t trÃªn tháº¿ giá»i',\n",
              " 'uit_000105': '751.419 ngÆ°á»i',\n",
              " 'uit_000106': 'khoáº£ng 36.132.147 ngÆ°á»i',\n",
              " 'uit_000107': 'thá»© 13',\n",
              " 'uit_000108': 'khoáº£ng 36.132.147 ngÆ°á»i',\n",
              " 'uit_000109': '4 triá»u dÃ¢n',\n",
              " 'uit_000110': '4 triá»u',\n",
              " 'uit_000111': '6,7%',\n",
              " 'uit_000112': 'thá»© 13',\n",
              " 'uit_000113': 'California',\n",
              " 'uit_000114': 'mÃ¡y tÃ­nh vÃ  cÃ´ng nghá» cao',\n",
              " 'uit_000115': '13%',\n",
              " 'uit_000116': 'California',\n",
              " 'uit_000117': 'thá»© sÃ¡u',\n",
              " 'uit_000118': 'thá»© sÃ¡u',\n",
              " 'uit_000119': 'vá» mÃ¡y tÃ­nh vÃ  cÃ´ng nghá» cao',\n",
              " 'uit_000120': 'nÃ´ng nghiá»p, hÃ ng khÃ´ng vÅ© trá»¥, giáº£i trÃ­, cÃ´ng nghiá»p nháº¹, vÃ  du lá»ch',\n",
              " 'uit_000121': 'nÃ´ng nghiá»p, hÃ ng khÃ´ng vÅ© trá»¥, giáº£i trÃ­, cÃ´ng nghiá»p nháº¹, vÃ  du lá»ch',\n",
              " 'uit_000122': 'Hollywood',\n",
              " 'uit_000123': 'Hollywood',\n",
              " 'uit_000124': 'nhá»¯ng trÆ°á»ng trung há»c cÆ¡ sá» cÃ³ lá»p tÃ¹y chá»n vá»i chÆ°Æ¡ng trÃ¬nh táº­p trung vÃ o cÃ¡ch há»c',\n",
              " 'uit_000125': 'khoáº£ng 14â18 tuá»',\n",
              " 'uit_000126': '16 tuá»i',\n",
              " 'uit_000127': '6 tuá»i',\n",
              " 'uit_000128': 'cÃ¡ch há»c, lá»ch sá»­, vÃ  xÃ£ há»i',\n",
              " 'uit_000129': 'nghá» nghiá»p, ngÃ´n ngá»¯, vÃ  khoa há»c nhÃ¢n vÄn',\n",
              " 'uit_000130': 'khoáº£ng 14â18 tuá»i',\n",
              " 'uit_000131': 'nhá»¯ng lá»p tÃ¹y chá»n vá» nghá» nghiá»p, ngÃ´n ngá»¯',\n",
              " 'uit_000132': 'nhá»¯ng trÆ°á»ng trung há»c cÆ¡ sá»',\n",
              " 'uit_000133': 'Bá» NÄng lÆ°á»£ng Hoa Ká»³',\n",
              " 'uit_000134': 'há» thá»ng Viá»n Äáº¡i há»c California (UC)',\n",
              " 'uit_000135': 'nháº­n 12,5% cá»§a nhá»¯ng há»c sinh cao Äiá»m nháº¥t vÃ  thá»±c hiá»n nghiÃªn cá»©u sau Äáº¡i há»c',\n",
              " 'uit_000136': 'sinh cao Äiá»m nháº¥t vÃ  thá»±c hiá»n nghiÃªn cá»©u sau Äáº¡i há»c',\n",
              " 'uit_000137': 'má»t trong nhá»¯ng há» thá»ng viá»n Äáº¡i há»c cÃ´ng láº­p hÃ ng Äáº§u cá»§a Hoa Ká»³',\n",
              " 'uit_000138': 'sinh viÃªn sau Äáº¡i há»c ngÃ nh y',\n",
              " 'uit_000139': 'PhÃ²ng thÃ­ nghiá»m Quá»c gia Lawrence táº¡i Livermore, PhÃ²ng thÃ­ nghiá»m Quá»c gia Lawrence táº¡i Berkeley, vÃ  PhÃ²ng thÃ­ nghiá»m Quá»c gia Los Alamos',\n",
              " 'uit_000140': 'nhá»¯ng sinh viÃªn sau Äáº¡i há»c ngÃ nh y',\n",
              " 'uit_000141': 'ÄÆ°á»£c coi nhÆ° má»t trong nhá»¯ng há» thá»ng viá»n Äáº¡i há»c cÃ´ng láº­p hÃ ng Äáº§u cá»§a Hoa Ká»³',\n",
              " 'uit_000142': 'hÆ¡n 400.000 sinh viÃªn',\n",
              " 'uit_000143': 'CÃ¡n bá» ThÆ° viá»n Tiá»u bang Kevin Star',\n",
              " 'uit_000144': 'pháº§n ba há»c sinh trung há»c phá» thÃ´ng cao Äiá»m nháº¥t',\n",
              " 'uit_000145': 'Äáº¡i há»c',\n",
              " 'uit_000146': 'Äáº¡i há»c',\n",
              " 'uit_000147': 'nháº­n pháº§n ba há»c sinh trung há»c phá» thÃ´ng cao Äiá»m nháº¥t',\n",
              " 'uit_000148': 'CSU-Long Beach, CSU-Fresno, San Diego State University, vÃ  San Jose State University',\n",
              " 'uit_000149': 'khoa há»c á»©ng dá»¥ng',\n",
              " 'uit_000150': 'khoa há»c á»©ng dá»¥ng',\n",
              " 'uit_000151': 'hÆ¡n 400.000',\n",
              " 'uit_000152': 'khoáº£ng 4,6 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000153': 'tinh vÃ¢n Máº·t Trá»i',\n",
              " 'uit_000154': 'khoáº£ng 150 triá»u kilÃ´mÃ©t',\n",
              " 'uit_000155': 'Khi Máº·t Trá»i ngÃ y cÃ ng Äáº·c láº¡i, nÃ³ nÃ³ng lÃªn, pháº£n á»©ng háº¡t nhÃ¢n bÃ¹ng ná» vÃ  táº¡o nÃªn giÃ³ Máº·t Trá»i',\n",
              " 'uit_000156': 'tinh vÃ¢n Máº·t Trá»i',\n",
              " 'uit_000157': 'khoáº£ng 4,6 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000158': 'lá»±c háº¥p dáº«n vÃ  quÃ¡n tÃ­nh',\n",
              " 'uit_000159': '150 triá»u kilÃ´mÃ©t',\n",
              " 'uit_000160': 'lá»±c háº¥p dáº«n vÃ  quÃ¡n tÃ­nh',\n",
              " 'uit_000161': 'tinh vÃ¢n Máº·t Trá»i',\n",
              " 'uit_000162': 'Sao Hoáº£',\n",
              " 'uit_000163': '23,5Â°',\n",
              " 'uit_000164': 'Theia',\n",
              " 'uit_000165': 'khoáº£ng 4.533 tá»· nÄm (cÃ³ láº½ 0 giá» 05 phÃºt ÄÃªm theo giá» cÃ¡i Äá»ng há» cá»§a chÃºng ta)',\n",
              " 'uit_000166': '23,5Â°',\n",
              " 'uit_000167': '150 triá»u km',\n",
              " 'uit_000168': 'Theia',\n",
              " 'uit_000169': 'Theia',\n",
              " 'uit_000170': 'Sao Hoáº£',\n",
              " 'uit_000171': 'amoniac, mÃªtan, hÆ¡i nÆ°á»c, cacbon ÄiÃ´xÃ­t, vÃ  nitÆ¡, cÅ©ng nhÆ° má»t lÆ°á»£ng nhá» cÃ¡c cháº¥t khÃ­',\n",
              " 'uit_000172': 'HÆ¡i nÆ°á»c thoÃ¡t ra tá»« lá»p vá» khi cÃ¡c khÃ­ gas bá» nÃºi lá»­a phun lÃªn',\n",
              " 'uit_000173': 'amoniac, mÃªtan, hÆ¡i nÆ°á»c, cacbon ÄiÃ´xÃ­t, vÃ  nitÆ¡, cÅ©ng nhÆ° má»t lÆ°á»£ng nhá» cÃ¡c cháº¥t khÃ­',\n",
              " 'uit_000174': 'lá»p ozone',\n",
              " 'uit_000175': 'ThÃ¡i Viá»n Cá»',\n",
              " 'uit_000176': 'giÃ³ máº·t trá»i vÃ  chÃ­nh nhiá»t lÆ°á»£ng cá»§a TrÃ¡i Äáº¥t',\n",
              " 'uit_000177': 'nhá»¯ng cuá»c va cháº¡m cá»§a sao bÄng',\n",
              " 'uit_000178': 'vÅ© trá»¥',\n",
              " 'uit_000179': 'má»t phÃ¢n tá»­ (hay tháº­m chÃ­ lÃ  má»t thá»© gÃ¬ khÃ¡c) ÄÃ£ cÃ³ kháº£ nÄng tá»± phÃ¢n chia thÃ nh cÃ¡c báº£n sao cá»§a chÃ­nh nÃ³',\n",
              " 'uit_000180': 'cÃ¡c dÃ²ng dÃµi sau ÄÃ³ cÃ³ thá» khai thÃ¡c cÃ¡c nguyÃªn liá»u khÃ¡c, hay cÃ³ láº½ lÃ  há»c cÃ¡ch tiáº¿n triá»n cá»§a cÃ¡c kiá»u dÃ²ng dÃµi khÃ¡c, vÃ  trá» nÃªn ÄÃ´ng Äáº£o hÆ¡n',\n",
              " 'uit_000181': 'tá»« vÅ© trá»¥',\n",
              " 'uit_000182': 'khoáº£ng 4 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000183': 'thÃºc Äáº©y cÃ¡c pháº£n á»©ng hÃ³a há»c táº¡o thÃ nh báº£n sao cá»§a chÃ­nh nÃ³',\n",
              " 'uit_000184': 'may máº¯n',\n",
              " 'uit_000185': 'DNA',\n",
              " 'uit_000186': 'DNA',\n",
              " 'uit_000187': 'protein hiá»n Äáº¡i cá»§a cÃ¡c acid nucleic, phospholipid, tinh thá», hay tháº­m chÃ­ cÃ¡c há» lÆ°á»£ng tá»­',\n",
              " 'uit_000188': 'cÃ³ tÃ­nh cháº¥t ká»³ dá» thÃºc Äáº©y cÃ¡c pháº£n á»©ng hÃ³a há»c táº¡o thÃ nh báº£n sao cá»§a chÃ­nh nÃ³, vÃ  tiáº¿n trÃ¬nh phÃ¡t triá»n thá»±c sá»± báº¯t Äáº§u',\n",
              " 'uit_000189': 'phÃ¢n tá»­ phospholipid',\n",
              " 'uit_000190': 'enzym',\n",
              " 'uit_000191': 'CÃ¡c protein',\n",
              " 'uit_000192': 'RNA',\n",
              " 'uit_000193': 'trao Äá»i thÃ´ng tin vÃ  tá»ng há»£p protein, vÃ  cÃ¡c enzyme lÃ m xÃºc tÃ¡c cho pháº£n á»©ng',\n",
              " 'uit_000194': 'ÄiÃ´xÃ­t cacbon vÃ  nÆ°á»c',\n",
              " 'uit_000195': 'thá»© ba',\n",
              " 'uit_000196': 'háº¥p thá»¥ Ã¡nh sÃ¡ng máº·t trá»i nhÆ° má»t nguá»n nÄng lÆ°á»£ng',\n",
              " 'uit_000197': 'biáº¿n Äá»i lá»n',\n",
              " 'uit_000198': 'Ãxy lÃ  cháº¥t Äá»c',\n",
              " 'uit_000199': 'ÄiÃ´xÃ­t cacbon vÃ  nÆ°á»c',\n",
              " 'uit_000200': 'Ãxy lÃ  cháº¥t Äá»c Äá»i vá»i nhiá»u dáº¡ng sá»ng vÃ o thá»i ká»³ nÃ y',\n",
              " 'uit_000201': 'khÃ­ Ã´xy',\n",
              " 'uit_000202': 'thá»i ká»³ khÃ­ quyá»n thá»© ba',\n",
              " 'uit_000203': 'Archarea vÃ  Eukarya',\n",
              " 'uit_000204': 'chloroplast',\n",
              " 'uit_000205': 'táº¿ bÃ o nhá» tÃ¬m cÃ¡ch kÃ½ sinh trÃªn táº¿ bÃ o lá»n',\n",
              " 'uit_000206': 'mitochondria',\n",
              " 'uit_000207': 'PhÃ©p phÃ¢n loáº¡i hiá»n Äáº¡i',\n",
              " 'uit_000208': 'Archarea vÃ  Eukarya',\n",
              " 'uit_000209': 'Vá»±c Bacteria',\n",
              " 'uit_000210': 'sá»± phÃ¢n chia giá»¯a má»t táº­p ÄoÃ n vá»i cÃ¡c táº¿ bÃ o',\n",
              " 'uit_000211': 'táº¥t cáº£ cÃ¡c táº¿ bÃ o Äá»u mang tÃ­nh toÃ n nÄng (totipotent)',\n",
              " 'uit_000212': 'khoáº£ng 750 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000213': 'Khoáº£ng 1.1 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000214': 'táº£o lá»¥c',\n",
              " 'uit_000215': 'eukaryotes',\n",
              " 'uit_000216': 'cÃ³ láº½ lÃ  táº£o lá»¥c',\n",
              " 'uit_000217': 'khoáº£ng 750 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000218': 'Khoáº£ng 1.1 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000219': 'khoáº£ng 530 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000220': 'khoáº£ng 600 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000221': 'cÃ¡c sinh váº­t ÄÆ¡n bÃ o Äi lÃªn máº·t Äáº¥t sáº½ cÃ³ cÆ¡ há»i sá»ng sÃ³t cao hÆ¡n, vÃ  cÃ¡c sinh váº­t chÆ°a cÃ³ nhÃ¢n ÄÃ£ báº¯t Äáº§u sinh sÃ´i vÃ  trá» nÃªn thÃ­ch á»©ng tá»t hÆ¡n vá»i mÃ´i trÆ°á»ng sá»ng bÃªn ngoÃ i Äáº¡i dÆ°Æ¡ng',\n",
              " 'uit_000222': 'CÃ¡',\n",
              " 'uit_000223': 'CÃ¡, nhá»¯ng Äá»ng váº­t cÃ³ xÆ°Æ¡ng sá»ng',\n",
              " 'uit_000224': '50 triá»u nÄm',\n",
              " 'uit_000225': 'khoáº£ng 600 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000226': '50 triá»u nÄm',\n",
              " 'uit_000227': 'chÃ¢n Äá»t',\n",
              " 'uit_000228': '530 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000229': 'cÃ¡c vÃ¢y',\n",
              " 'uit_000230': '530 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000231': '1 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000232': 'Khoáº£ng 365 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000233': 'sá»± láº¡nh Äi toÃ n cáº§u',\n",
              " 'uit_000234': 'Pangea',\n",
              " 'uit_000235': 'Jura',\n",
              " 'uit_000236': 'Pangea',\n",
              " 'uit_000237': '10 kilÃ´mÃ©t',\n",
              " 'uit_000238': '340 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000239': '340 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000240': 'khoáº£ng thá»i gian phÃ¢n tÃ¡ch giá»¯a ká»· Permi vÃ  Trias',\n",
              " 'uit_000241': 'Pangaea',\n",
              " 'uit_000242': 'hai triá»u nÄm trÆ°á»c',\n",
              " 'uit_000243': 'loÃ i tinh tinh',\n",
              " 'uit_000244': 'kháº£ nÄng Äá»©ng tháº³ng',\n",
              " 'uit_000245': 'chÃ¢u Phi',\n",
              " 'uit_000246': 'kháº£ nÄng Äá»©ng tháº³ng',\n",
              " 'uit_000247': 'NgÆ°á»i Cro-Magnon',\n",
              " 'uit_000248': 'NgÆ°á»i Cro-Magnon',\n",
              " 'uit_000249': 'chÃ¢u Phi',\n",
              " 'uit_000250': 'hai triá»u nÄm trÆ°á»c',\n",
              " 'uit_000251': 'thá» hiá»n Äá»©c tin tÃ´n giÃ¡o',\n",
              " 'uit_000252': 'Thá»i Äiá»m nÃ o ÄÃ³ trong khoáº£ng 8500 tá»i 7000 trÆ°á»c CÃ´ng NguyÃªn',\n",
              " 'uit_000253': 'sÄn báº¯t - hÃ¡i lÆ°á»£m',\n",
              " 'uit_000254': 'LÆ°á»¡ng HÃ ',\n",
              " 'uit_000255': 'táº§ng lá»p cai trá» vÃ  tháº§y cÃºng xuáº¥t hiá»n, tiáº¿p ÄÃ³ lÃ  sá»± phÃ¢n cÃ´ng lao Äá»ng',\n",
              " 'uit_000256': 'Sumer vÃ¹ng Trung ÄÃ´ng',\n",
              " 'uit_000257': 'má»t hÃ¬nh thá»©c tÃ¡i truyá»n táº£i thÃ´ng tin má»i',\n",
              " 'uit_000258': 'sÄn báº¯t - hÃ¡i lÆ°á»£m',\n",
              " 'uit_000259': 'nhá»¯ng thÆ° khá» vÃ  cÃ¡c thÆ° viá»n trá» thÃ nh nÆ¡i lÆ°u giá»¯ nhá»¯ng hiá»u biáº¿t cá»§a nhÃ¢n loáº¡i cÅ©ng nhÆ° tÄng cÆ°á»ng sá»± chuyá»n giao vÄn hÃ³a vÃ  thÃ´ng tin',\n",
              " 'uit_000260': 'Sá»± phÃ¡t minh ra chá»¯ viáº¿t',\n",
              " 'uit_000261': 'tÃ­nh tÃ² mÃ² vÃ  giÃ¡o dá»¥c',\n",
              " 'uit_000262': 'tÃ­nh tÃ² mÃ² vÃ  giÃ¡o dá»¥c khiáº¿n má»i ngÆ°á»i nhanh chÃ³ng cÃ³ ÄÆ°á»£c sá»± hiá»u biáº¿t vÃ  khÃ´n ngoan',\n",
              " 'uit_000263': 'phÃ¡t triá»n tá»i cá»±c Äiá»m',\n",
              " 'uit_000264': 'cho phÃ©p cÃ¡c xÃ£ há»i phá»©c táº¡p hÆ¡n xuáº¥t hiá»n',\n",
              " 'uit_000265': 'phÃ¡t triá»n tá»i cá»±c Äiá»m',\n",
              " 'uit_000266': 'Há»i quá»c liÃªn',\n",
              " 'uit_000267': 'Tá»« nÄm 1914 Äáº¿n 1918',\n",
              " 'uit_000268': 'Há»i quá»c liÃªn',\n",
              " 'uit_000269': 'váº­n táº£i vÃ  thÃ´ng tin phÃ¡t triá»n',\n",
              " 'uit_000270': 'Italia',\n",
              " 'uit_000271': 'LiÃªn minh chÃ¢u Ãu',\n",
              " 'uit_000272': 'khoáº£ng nÄm 1500',\n",
              " 'uit_000273': 'Tá»« nÄm 1914 Äáº¿n 1918',\n",
              " 'uit_000274': 'Italia',\n",
              " 'uit_000275': 'sá»± tuyá»t chá»§ng hÃ ng loáº¡t vÃ  sá»± áº¥m lÃªn toÃ n cáº§u',\n",
              " 'uit_000276': 'Sá»± thay Äá»i tiáº¿p tá»¥c diá»n ra vá»i tá»c Äá» ngÃ y cÃ ng nhanh trong pháº§n nghÃ¬n giÃ¢y cuá»i cÃ¹ng cá»§a 24 giá» tÆ°á»ng tÆ°á»£ng',\n",
              " 'uit_000277': 'sá»± tuyá»t chá»§ng hÃ ng loáº¡t vÃ  sá»± áº¥m lÃªn toÃ n cáº§u',\n",
              " 'uit_000278': 'LiÃªn bang xÃ´ viáº¿t',\n",
              " 'uit_000279': 'LiÃªn bang xÃ´ viáº¿t',\n",
              " 'uit_000280': 'nÄm 2000',\n",
              " 'uit_000281': 'LiÃªn bang xÃ´ viáº¿t',\n",
              " 'uit_000282': 'Yuri Gagarin',\n",
              " 'uit_000283': 'NÄm',\n",
              " 'uit_000284': 'nÄm 2000',\n",
              " 'uit_000285': 'hiá»n diá»n thÆ°á»ng xuyÃªn trÃªn vÅ© trá»¥ hay tháº­m chÃ­ chiáº¿m lÃ m thuá»c Äá»a nhá»¯ng tháº¿ giá»i xa xÃ´i',\n",
              " 'uit_000286': 'hiá»n diá»n thÆ°á»ng xuyÃªn trÃªn vÅ© trá»¥ hay tháº­m chÃ­ chiáº¿m lÃ m thuá»c Äá»a nhá»¯ng tháº¿ giá»i xa xÃ´i',\n",
              " 'uit_000287': 'Yuri Gagarin',\n",
              " 'uit_000513': 'bá»©c xáº¡ Hawking',\n",
              " 'uit_000514': 'sá»± thá»ng nháº¥t giá»¯a thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t vÃ  cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000515': 'lÃ  má»t nhÃ  váº­t lÃ½ lÃ½ thuyáº¿t, vÅ© trá»¥ há»c, tÃ¡c giáº£ viáº¿t sÃ¡ch khoa há»c thÆ°á»ng thá»©c ngÆ°á»i Anh, nguyÃªn GiÃ¡m Äá»c NghiÃªn cá»©u táº¡i Trung tÃ¢m VÅ© trá»¥ há»c lÃ½ thuyáº¿t thuá»c Äáº¡i há»c Cambridge',\n",
              " 'uit_000516': 'Hawking lÃ  ngÆ°á»i Äáº§u tiÃªn khá»i Äáº§u má»t ná»n vÅ© trá»¥ há»c dá»±a trÃªn sá»± thá»ng nháº¥t giá»¯a thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t vÃ  cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000517': 'Roger Penrose',\n",
              " 'uit_000518': 'lÃ  má»t nhÃ  váº­t lÃ½ lÃ½ thuyáº¿t, vÅ© trá»¥ há»c, tÃ¡c giáº£ viáº¿t sÃ¡ch khoa há»c thÆ°á»ng thá»©c ngÆ°á»i Anh, nguyÃªn GiÃ¡m Äá»c NghiÃªn cá»©u táº¡i Trung tÃ¢m VÅ© trá»¥ há»c lÃ½ thuyáº¿t thuá»c Äáº¡i há»c Cambridge',\n",
              " 'uit_000519': 'bá»©c xáº¡ Hawking',\n",
              " 'uit_000520': '21 tuá»i',\n",
              " 'uit_000521': 'cÄn bá»nh ALS sáº½ khiáº¿n Ã´ng chá» sá»ng thÃªm ÄÆ°á»£c vÃ i nÄm',\n",
              " 'uit_000522': 'kháº£ nÄng nÃ³i chuyá»n',\n",
              " 'uit_000523': 'chá»©ng bá»nh xÆ¡ cá»©ng teo cÆ¡',\n",
              " 'uit_000524': 'Ã´ng chá» sá»ng thÃªm ÄÆ°á»£c vÃ i nÄm',\n",
              " 'uit_000525': 'giao tiáº¿p thÃ´ng qua má»t thiáº¿t bá» táº¡o giá»ng nÃ³i ÄÆ°á»£c gáº¯n trá»±c tiáº¿p vÃ o chiáº¿c xe lÄn cá»§a Ã´ng',\n",
              " 'uit_000526': 'thÃ´ng qua má»t thiáº¿t bá» táº¡o giá»ng nÃ³i ÄÆ°á»£c gáº¯n trá»±c tiáº¿p vÃ o chiáº¿c xe lÄn cá»§a Ã´ng',\n",
              " 'uit_000527': 'xÆ¡ cá»©ng teo cÆ¡',\n",
              " 'uit_000528': 'tháº§y dáº¡y toÃ¡n ná»i tiáº¿ng Dikran Tahta',\n",
              " 'uit_000529': 'Hawking duy trÃ¬ ÄÆ°á»£c má»t nhÃ³m báº¡n thÃ¢n mÃ  Ã´ng thÆ°á»ng tham gia chÆ¡i bÃ i, lÃ m phÃ¡o hoa, cÃ¡c mÃ´ hÃ¬nh phi cÆ¡ vÃ  tÃ u thuyá»n, cÅ©ng nhÆ° tháº£o luáº­n vá» CÆ¡ Äá»c giÃ¡o vÃ  nÄng lá»±c ngoáº¡i cáº£m. Tá»« 1958, vá»i sá»± giÃºp Äá»¡ cá»§a tháº§y dáº¡y toÃ¡n ná»i tiáº¿ng Dikran Tahta, há» xÃ¢y dá»±ng má»t mÃ¡y tÃ­nh vá»i cÃ¡c linh kiá»n láº¥y tá»« Äá»ng há», má»t mÃ¡y tá»ng ÄÃ i Äiá»n thoáº¡i cÅ© vÃ  cÃ¡c thiáº¿t bá» tÃ¡i cháº¿ khÃ¡c',\n",
              " 'uit_000530': 'tháº§y dáº¡y toÃ¡n ná»i tiáº¿ng Dikran Tahta',\n",
              " 'uit_000531': 'ráº¥t Äá» cao giÃ¡ trá» cá»§a viá»c há»c hÃ nh',\n",
              " 'uit_000532': 'Hawking duy trÃ¬ ÄÆ°á»£c má»t nhÃ³m báº¡n thÃ¢n mÃ  Ã´ng thÆ°á»ng tham gia chÆ¡i bÃ i, lÃ m phÃ¡o hoa, cÃ¡c mÃ´ hÃ¬nh phi cÆ¡ vÃ  tÃ u thuyá»n, cÅ©ng nhÆ° tháº£o luáº­n vá» CÆ¡ Äá»c giÃ¡o vÃ  nÄng lá»±c ngoáº¡i cáº£m',\n",
              " 'uit_000533': 'bá» á»m vÃ o ÄÃºng ngÃ y thi láº¥y há»c bá»ng',\n",
              " 'uit_000534': 'lo ngáº¡i ráº±ng khÃ´ng cÃ³ máº¥y viá»c lÃ m cho má»t sinh viÃªn ngÃ nh toÃ¡n ra trÆ°á»ng',\n",
              " 'uit_000535': 'Einstein',\n",
              " 'uit_000536': 'máº·c dÃ¹ Äiá»m sá» khÃ´ng tá»t nhÆ°ng cáº£ giÃ¡o viÃªn vÃ  báº¡n bÃ¨ Äá»u tháº¥y ÄÆ°á»£c tá» cháº¥t thiÃªn tÃ i cá»§a Ã´ng',\n",
              " 'uit_000537': 'cÃ¡c mÃ´n khoa há»c tá»± nhiÃªn',\n",
              " 'uit_000538': 'vÃ¬ lo ngáº¡i ráº±ng khÃ´ng cÃ³ máº¥y viá»c lÃ m cho má»t sinh viÃªn ngÃ nh toÃ¡n ra trÆ°á»ng',\n",
              " 'uit_000539': 'sá»± trá» náº£i cá»§a Ã´ng',\n",
              " 'uit_000540': 'lÃ¡i Äá»i Äua theo nhá»¯ng hÆ°á»ng nguy hiá»m thÆ°á»ng dáº«n tá»i thuyá»n bá» hÆ° háº¡i',\n",
              " 'uit_000541': 'pháº¥n Äáº¥u vÃ  trá» thÃ nh má»t sinh viÃªn ÄÆ°á»£c quÃ½ máº¿n, hoáº¡t bÃ¡t, dÃ­ dá»m, há»©ng thÃº vá»i nháº¡c cá» Äiá»n vÃ  tiá»u thuyáº¿t viá»n tÆ°á»ng',\n",
              " 'uit_000542': 'Äá»i vá»i cáº­u ta chá» cáº§n biáº¿t Äiá»u gÃ¬ ÄÃ³ cÃ³ thá» thá»±c hiá»n, vÃ  cáº­u cÃ³ thá» lÃ m nÃ³ mÃ  khÃ´ng cáº§n pháº£i ngÃ³ xem nhá»¯ng ngÆ°á»i khÃ¡c ÄÃ£ lÃ m tháº¿ nÃ o',\n",
              " 'uit_000543': 'Hawking pháº¥n Äáº¥u vÃ  trá» thÃ nh má»t sinh viÃªn ÄÆ°á»£c quÃ½ máº¿n, hoáº¡t bÃ¡t, dÃ­ dá»m, há»©ng thÃº vá»i nháº¡c cá» Äiá»n vÃ  tiá»u thuyáº¿t viá»n tÆ°á»ng',\n",
              " 'uit_000544': 'tÃ¡o báº¡o',\n",
              " 'uit_000545': 'Ã´ng Ã­t tuá»i hÆ¡n pháº§n lá»n sinh viÃªn',\n",
              " 'uit_000546': 'lÃ¡i Äá»i Äua theo nhá»¯ng hÆ°á»ng nguy hiá»m thÆ°á»ng dáº«n tá»i thuyá»n bá» hÆ° háº¡i',\n",
              " 'uit_000547': 'Äá»i vá»i cáº­u ta chá» cáº§n biáº¿t Äiá»u gÃ¬ ÄÃ³ cÃ³ thá» thá»±c hiá»n, vÃ  cáº­u cÃ³ thá» lÃ m nÃ³ mÃ  khÃ´ng cáº§n pháº£i ngÃ³ xem nhá»¯ng ngÆ°á»i khÃ¡c ÄÃ£ lÃ m tháº¿ nÃ o',\n",
              " 'uit_000548': '1000 giá»',\n",
              " 'uit_000549': 'káº¿t quáº£ náº±m á» ÄÃºng Äiá»m sá» ranh giá»i giá»¯a háº¡ng nháº¥t vÃ  háº¡ng nhÃ¬',\n",
              " 'uit_000550': 'Äá» phÃ¢n háº¡ng',\n",
              " 'uit_000551': 'pháº£i cÃ³ má»t báº±ng danh dá»± háº¡ng nháº¥t',\n",
              " 'uit_000552': 'káº¿t quáº£ náº±m á» ÄÃºng Äiá»m sá» ranh giá»i giá»¯a háº¡ng nháº¥t vÃ  háº¡ng nhÃ¬',\n",
              " 'uit_000553': 'chá» tráº£ lá»i nhá»¯ng cÃ¢u há»i váº­t lÃ½ lÃ½ thuyáº¿t vÃ  bá» qua nhá»¯ng cÃ¢u ÄÃ²i há»i kiáº¿n thá»©c thá»±c táº¿',\n",
              " 'uit_000554': 'phÃ¢n háº¡ng',\n",
              " 'uit_000555': 'Náº¿u cÃ¡c vá» trao cho tÃ´i háº¡ng Nháº¥t, tÃ´i sáº½ tá»i Cambridge. Náº¿u tÃ´i nháº­n háº¡ng NhÃ¬, tÃ´i sáº½ á» láº¡i Oxford, vÃ¬ váº­y tÃ´i hi vá»ng cÃ¡c vá» cho tÃ´i háº¡ng Nháº¥t',\n",
              " 'uit_000556': 'bá» xem lÃ  má»t sinh viÃªn lÆ°á»i nhÃ¡c vÃ  khÃ³ tÃ­nh',\n",
              " 'uit_000557': 'báº¯t Äáº§u vÃ o há»c báº­c trÃªn Äáº¡i há»c táº¡i Trinity Hall (Äáº¡i há»c Cambridge)',\n",
              " 'uit_000558': 'thÃ¡ng 10 nÄm 1962',\n",
              " 'uit_000559': 'Hawking rÆ¡i vÃ o tráº§m uáº¥t; máº·c dÃ¹ cÃ¡c bÃ¡c sÄ© khuyÃªn Ã´ng tiáº¿p tá»¥c há»c hÃ nh, Ã´ng cáº£m tháº¥y cháº³ng cÃ²n máº¥y Ã½ nghÄ©a',\n",
              " 'uit_000560': 'rÆ¡i vÃ o tráº§m uáº¥t',\n",
              " 'uit_000561': 'Jane Wilde, báº¡n cá»§a em gÃ¡i Ã´ng',\n",
              " 'uit_000562': 'Jane Wilde',\n",
              " 'uit_000563': 'ngÆ°á»£c ngáº¡o',\n",
              " 'uit_000564': 'CÃ¡c ká»³ dá» vÃ  HÃ¬nh há»c cá»§a KhÃ´ng-Thá»i gian',\n",
              " 'uit_000565': 'giáº£i dÃ nh cho nghiÃªn cá»©u toÃ¡n há»c xuáº¥t sáº¯c nháº¥t hÃ ng nÄm cá»§a Cambridge',\n",
              " 'uit_000566': 'CÃ¡c ká»³ dá» vÃ  HÃ¬nh há»c cá»§a KhÃ´ng-Thá»i gian',\n",
              " 'uit_000567': 'cÃ¡c lÃ½ thuyáº¿t Äang thá»nh hÃ nh liÃªn quan tá»i sá»± khai sinh vÅ© trá»¥: thuyáº¿t Vá»¥ Ná» Lá»n vÃ  thuyáº¿t vÅ© trá»¥ tÄ©nh táº¡i',\n",
              " 'uit_000568': 'Ã¡p dá»¥ng Ã½ tÆ°á»ng tÆ°Æ¡ng tá»± cho toÃ n thá» vÅ© trá»¥',\n",
              " 'uit_000569': 'cÃ¡c lÃ½ thuyáº¿t Äang thá»nh hÃ nh liÃªn quan tá»i sá»± khai sinh vÅ© trá»¥',\n",
              " 'uit_000570': 'Äá»nh lÃ½ vá» kÃ¬ dá» khÃ´ng-thá»i gian trong tÃ¢m cÃ¡c há» Äen cá»§a Roger Penrose',\n",
              " 'uit_000571': 'DÆ°á»i áº£nh hÆ°á»ng cá»§a Äá»nh lÃ½ vá» kÃ¬ dá» khÃ´ng-thá»i gian trong tÃ¢m cÃ¡c há» Äen cá»§a Roger Penrose, Hawking Ã¡p dá»¥ng Ã½ tÆ°á»ng tÆ°Æ¡ng tá»± cho toÃ n thá» vÅ© trá»¥',\n",
              " 'uit_000572': 'vá» nhÃ¬ trong cuá»c thi cá»§a Quá»¹ NghiÃªn cá»©u Lá»±c Háº¥p dáº«n nÄm 1968',\n",
              " 'uit_000573': 'vÅ© trá»¥ tá»± nÃ³ cÃ³ thá» khá»i Äáº§u tá»« má»t kÃ¬ dá»',\n",
              " 'uit_000574': 'cÃ¡c quan niá»m vá» Äá»nh lÃ½ Äiá»m kÃ¬ dá»',\n",
              " 'uit_000575': 'cÃ¡c quan niá»m vá» Äá»nh lÃ½ Äiá»m kÃ¬ dá» mÃ  Ã´ng khÃ¡m phÃ¡ trong luáº­n Ã¡n tiáº¿n sÄ©',\n",
              " 'uit_000576': 'cÃ´ng bá» má»t phÃ©p chá»©ng minh ráº±ng náº¿u vÅ© trá»¥ tuÃ¢n theo lÃ½ thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t vÃ  phÃ¹ há»£p vá»i báº¥t ká»³ mÃ´ hÃ¬nh nÃ o vá» vÅ© trá»¥ há»c váº­t lÃ½ phÃ¡t triá»n bá»i Alexander Friedmann, thÃ¬ nÃ³ pháº£i khá»i Äáº§u tá»« má»t kÃ¬ dá»',\n",
              " 'uit_000577': 'cÃ´ng bá» má»t phÃ©p chá»©ng minh ráº±ng náº¿u vÅ© trá»¥ tuÃ¢n theo lÃ½ thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t vÃ  phÃ¹ há»£p vá»i báº¥t ká»³ mÃ´ hÃ¬nh nÃ o vá» vÅ© trá»¥ há»c váº­t lÃ½ phÃ¡t triá»n bá»i Alexander Friedmann, thÃ¬ nÃ³ pháº£i khá»i Äáº§u tá»« má»t kÃ¬ dá»',\n",
              " 'uit_000578': 'vá» nhÃ¬ trong cuá»c thi cá»§a Quá»¹ NghiÃªn cá»©u Lá»±c Háº¥p dáº«n nÄm 1968',\n",
              " 'uit_000579': 'trÆ°á»c háº¿t nhÆ° má»t nhÃ  khoa há»c, thá»© Äáº¿n nhÆ° má»t nhÃ  vÄn phá» biáº¿n khoa há»c, vÃ , trong má»i cÃ¡ch mÃ  nÃ³ ÄÃ¡ng ká», má»t ngÆ°á»i bÃ¬nh thÆ°á»ng vá»i cÃ¹ng nhá»¯ng ham muá»n, nghá» lá»±c, Æ°á»c mÆ¡ vÃ  tham vá»ng nhÆ° nhá»¯ng ngÆ°á»i xung quanh',\n",
              " 'uit_000580': 'Äá»c láº­p má»t cÃ¡ch mÃ£nh liá»t vÃ  khÃ´ng báº±ng lÃ²ng cháº¥p nháº­n giÃºp Äá»¡ hay chá»u nhÆ°á»£ng bá» vÃ¬ sá»± tÃ n táº­t cá»§a mÃ¬nh',\n",
              " 'uit_000581': 'Ã´ng phÃ¡t triá»n cÃ¡c phÆ°Æ¡ng phÃ¡p thá» giÃ¡c Äá» bÃ¹ Äáº¯p, bao gá»m nhÃ¬n cÃ¡c phÆ°Æ¡ng trÃ¬nh theo cÃ¡ch hiá»u hÃ¬nh há»c',\n",
              " 'uit_000582': 'Ã´ng báº¯t Äáº§u pháº£i dÃ¹ng náº¡ng vÃ  thÆ°á»ng xuyÃªn há»§y cÃ¡c buá»i giáº£ng',\n",
              " 'uit_000583': 'trÆ°á»c háº¿t nhÆ° má»t nhÃ  khoa há»c, thá»© Äáº¿n nhÆ° má»t nhÃ  vÄn phá» biáº¿n khoa há»c, vÃ , trong má»i cÃ¡ch mÃ  nÃ³ ÄÃ¡ng ká», má»t ngÆ°á»i bÃ¬nh thÆ°á»ng vá»i cÃ¹ng nhá»¯ng ham muá»n, nghá» lá»±c, Æ°á»c mÆ¡ vÃ  tham vá»ng nhÆ° nhá»¯ng ngÆ°á»i xung quanh',\n",
              " 'uit_000584': 'Ã´ng phÃ¡t triá»n cÃ¡c phÆ°Æ¡ng phÃ¡p thá» giÃ¡c Äá» bÃ¹ Äáº¯p, bao gá»m nhÃ¬n cÃ¡c phÆ°Æ¡ng trÃ¬nh theo cÃ¡ch hiá»u hÃ¬nh há»c',\n",
              " 'uit_000585': 'Äá» giá»¯ Ã´ng láº¡i á» Caius',\n",
              " 'uit_000586': 'bá»nh táº­t cÅ©ng nhÆ° danh tiáº¿ng vá» trÃ­ tuá» vÃ  sá»± ngáº¡o ngÆ°á»£c cá»§a Ã´ng',\n",
              " 'uit_000587': 'bá»nh táº­t cÅ©ng nhÆ° danh tiáº¿ng vá» trÃ­ tuá» vÃ  sá»± ngáº¡o ngÆ°á»£c cá»§a Ã´ng',\n",
              " 'uit_000588': 'cuá»i nhá»¯ng nÄm 1960',\n",
              " 'uit_000589': 'cuá»i nhá»¯ng nÄm 1960',\n",
              " 'uit_000590': 'Carter, Werner Israel vÃ  David C. Robinson',\n",
              " 'uit_000591': 'Jacob Bekenstein, má»t nghiÃªn cá»©u sinh cá»§a John Wheeler',\n",
              " 'uit_000592': 'báº¥t ká» há» Äen ban Äáº§u táº¡o thÃ nh tá»« váº­t liá»u nÃ o, nÃ³ hoÃ n toÃ n cÃ³ thá» mÃ´ táº£ báº±ng ba tÃ­nh cháº¥t khá»i lÆ°á»£ng, Äiá»n tÃ­ch vÃ  sá»± tá»± quay',\n",
              " 'uit_000593': 'Cáº¥u trÃºc VÄ© mÃ´ cá»§a KhÃ´ng-Thá»i gian',\n",
              " 'uit_000594': 'chÃ¢n trá»i sá»± kiá»n cá»§a há» Äen khÃ´ng bao giá» cÃ³ thá» thu nhá» hÆ¡n',\n",
              " 'uit_000595': 'báº¥t ká» há» Äen ban Äáº§u táº¡o thÃ nh tá»« váº­t liá»u nÃ o, nÃ³ hoÃ n toÃ n cÃ³ thá» mÃ´ táº£ báº±ng ba tÃ­nh cháº¥t khá»i lÆ°á»£ng, Äiá»n tÃ­ch vÃ  sá»± tá»± quay',\n",
              " 'uit_000596': 'Jacob Bekenstein',\n",
              " 'uit_000597': 'kháº³ng Äá»nh ráº±ng chÃ¢n trá»i sá»± kiá»n cá»§a há» Äen khÃ´ng bao giá» cÃ³ thá» thu nhá» hÆ¡n',\n",
              " 'uit_000598': \"má»t chuyáº¿n thÄm tá»i Moskva vÃ  nhá»¯ng cuá»c tháº£o luáº­n vá»i Yakov Borisovich Zel'dovich vÃ  Alexander Starobinsky\",\n",
              " 'uit_000599': 'há» Äen phÃ¡t ra bá»©c xáº¡ - mÃ  ngÃ y nay ÄÆ°á»£c gá»i lÃ  bá»©c xáº¡ Hawking - cho Äáº¿n khi chÃºng cáº¡n kiá»t nÄng lÆ°á»£ng vÃ  bay hÆ¡i',\n",
              " 'uit_000600': 'khÃ¡m phÃ¡ nÃ y ÄÆ°á»£c cháº¥p nháº­n rá»ng rÃ£i nhÆ° má»t Äá»t phÃ¡ quan trá»ng trong váº­t lÃ½ lÃ½ thuyáº¿t',\n",
              " 'uit_000601': 'há» Äen phÃ¡t ra bá»©c xáº¡ - mÃ  ngÃ y nay ÄÆ°á»£c gá»i lÃ  bá»©c xáº¡ Hawking',\n",
              " 'uit_000602': 'theo nguyÃªn lÃ½ báº¥t Äá»nh cÃ¡c há» Äen quay phÃ¡t ra cÃ¡c háº¡t',\n",
              " 'uit_000603': 'háº¥p dáº«n lÆ°á»£ng tá»­ vÃ  cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000604': 'nhá»¯ng tÃ­nh toÃ¡n ÄÆ°á»£c kiá»m tra nhiá»u láº§n cá»§a Ã´ng cho ra nhá»¯ng phÃ¡t hiá»n mÃ¢u thuáº«n vá»i Äá»nh luáº­t cá»§a Ã´ng',\n",
              " 'uit_000605': 'háº¥p dáº«n lÆ°á»£ng tá»­ vÃ  cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000606': 'nhá»¯ng tÃ­nh toÃ¡n ÄÆ°á»£c kiá»m tra nhiá»u láº§n cá»§a Ã´ng cho ra nhá»¯ng phÃ¡t hiá»n mÃ¢u thuáº«n vá»i Äá»nh luáº­t cá»§a Ã´ng, vá»n kháº³ng Äá»nh ráº±ng cÃ¡c há» Äen khÃ´ng bao giá» co láº¡i (chá» giá»¯ nguyÃªn hoáº·c lá»n lÃªn), vÃ  á»§ng há» láº­p luáº­n cá»§a Bekenstein vá» entropy cá»§a chÃºng',\n",
              " 'uit_000607': 'má»t nghiÃªn cá»©u sinh hoáº·c sinh viÃªn háº­u tiáº¿n sÄ© sá»ng vá»i há» vÃ  giÃºp chÄm sÃ³c Ã´ng',\n",
              " 'uit_000608': 'khiáº¿n cho cÃ¡c trÃ¡ch nhiá»m gia ÄÃ¬nh rÆ¡i xuá»ng ÄÃ´i vai ngÃ y cÃ ng cháº¥t náº·ng cá»§a vá»£ Ã´ng',\n",
              " 'uit_000609': 'Bernard Carr',\n",
              " 'uit_000610': 'nghiÃªn cá»©u sinh hoáº·c sinh viÃªn háº­u tiáº¿n sÄ© sá»ng vá»i há»',\n",
              " 'uit_000611': 'cÃ¡c trÃ¡ch nhiá»m gia ÄÃ¬nh rÆ¡i xuá»ng ÄÃ´i vai ngÃ y cÃ ng cháº¥t náº·ng cá»§a vá»£ Ã´ng',\n",
              " 'uit_000612': 'Cygnus X-1 lÃ  má»t há» Äen',\n",
              " 'uit_000613': 'váº­t cháº¥t tá»i neutralino',\n",
              " 'uit_000614': 'má»t thÃ¡ng',\n",
              " 'uit_000615': 'sao tá»i (sao chá»©a má»t hÃ m lÆ°á»£ng lá»n váº­t cháº¥t tá»i neutralino) Cygnus X-1 lÃ  má»t há» Äen',\n",
              " 'uit_000616': 'há» Äen vÃ  nhá»¯ng nhÃ  váº­t lÃ½ nghiÃªn cá»©u Äá» tÃ i nÃ y',\n",
              " 'uit_000617': 'PhÃ³ GiÃ¡o sÆ°',\n",
              " 'uit_000618': 'Hawking thÆ°á»ng xuyÃªn ÄÆ°á»£c bÃ¡o chÃ­ vÃ  truyá»n hÃ¬nh má»i phá»ng váº¥n',\n",
              " 'uit_000619': 'cÃ´ng chÃºng cÃ³ sá»± quan tÃ¢m ngÃ y cÃ ng tÄng tá»i há» Äen',\n",
              " 'uit_000620': 'PhÃ³ GiÃ¡o sÆ°',\n",
              " 'uit_000621': 'thÆ°á»ng xuyÃªn ÄÆ°á»£c bÃ¡o chÃ­ vÃ  truyá»n hÃ¬nh má»i phá»ng váº¥n',\n",
              " 'uit_000622': 'Bá» khÃ­ch Äá»ng tá»« má»t cuá»c tranh luáº­n vá»i Äáº¡i há»c vá» viá»c ai sáº½ tráº£ tiá»n cho cÃ¡c bá» dá»c thoáº£i Äá» Ã´ng cÃ³ thá» Äi xe lÄn tá»i chá» lÃ m',\n",
              " 'uit_000623': 'cuá»i nhá»¯ng nÄm 1970',\n",
              " 'uit_000624': 'váº­n Äá»ng cho viá»c cáº£i thiá»n cÃ¡c lá»i Äi vÃ o há» trá»£ cho nhá»¯ng ngÆ°á»i bá» táº­t nguyá»n á» Cambridge, bao gá»m viá»c nuÃ´i cÃ¡c sinh viÃªn tÃ n táº­t trong trÆ°á»ng',\n",
              " 'uit_000625': 'chá» cÃ²n gia ÄÃ¬nh vÃ  nhá»¯ng ngÆ°á»i báº¡n thÃ¢n nháº¥t hiá»u ÄÆ°á»£c Ã´ng',\n",
              " 'uit_000626': 'Äá» giao tiáº¿p vá»i nhá»¯ng ngÆ°á»i khÃ¡c, ai ÄÃ³ hiá»u rÃµ sáº½ dá»ch lá»i Ã´ng cho ngÆ°á»i kia',\n",
              " 'uit_000627': 'Bá» khÃ­ch Äá»ng tá»« má»t cuá»c tranh luáº­n vá»i Äáº¡i há»c vá» viá»c ai sáº½ tráº£ tiá»n cho cÃ¡c bá» dá»c thoáº£i Äá» Ã´ng cÃ³ thá» Äi xe lÄn tá»i chá» lÃ m',\n",
              " 'uit_000628': 'trong khi muá»n giÃºp Äá»¡ ngÆ°á»i khÃ¡c, Ã´ng tÃ¬m cÃ¡ch tÃ¡ch báº£n thÃ¢n ra khá»i chuyá»n bá»nh táº­t vÃ  cÃ¡c khÃ³ khÄn cá»§a nÃ³',\n",
              " 'uit_000629': 'vai trÃ² cá»§a mÃ¬nh nhÆ° má»t ngÆ°á»i bÃªnh vá»±c cho quyá»n cá»§a ngÆ°á»i tÃ n táº­t',\n",
              " 'uit_000630': 'vai trÃ² cá»§a mÃ¬nh nhÆ° má»t ngÆ°á»i bÃªnh vá»±c cho quyá»n cá»§a ngÆ°á»i tÃ n táº­t',\n",
              " 'uit_000631': 'gháº¿ GiÃ¡o sÆ° ToÃ¡n há»c Lucas',\n",
              " 'uit_000632': 'má»t vá» trÃ­ danh tiáº¿ng hÃ ng Äáº§u á» Äáº¡i há»c Cambridge cÅ©ng nhÆ° trÃªn tháº¿ giá»i',\n",
              " 'uit_000633': 'tá»«ng lÃ  vá» trÃ­ cá»§a Isaac Newton vÃ  Paul Dirac',\n",
              " 'uit_000634': 'Viá»c Ã´ng thiáº¿u dáº¥n thÃ¢n vÃ o cuá»c Äáº¥u tranh',\n",
              " 'uit_000635': 'trong khi muá»n giÃºp Äá»¡ ngÆ°á»i khÃ¡c, Ã´ng tÃ¬m cÃ¡ch tÃ¡ch báº£n thÃ¢n ra khá»i chuyá»n bá»nh táº­t vÃ  cÃ¡c khÃ³ khÄn cá»§a nÃ³',\n",
              " 'uit_000636': 'suy nghÄ© theo trá»±c giÃ¡c vÃ  Æ°á»c ÄoÃ¡n hÆ¡n lÃ  nháº¥n máº¡nh vÃ o cÃ¡c phÃ©p chá»©ng minh toÃ¡n há»c',\n",
              " 'uit_000637': 'suy nghÄ© theo trá»±c giÃ¡c vÃ  Æ°á»c ÄoÃ¡n hÆ¡n lÃ  nháº¥n máº¡nh vÃ o cÃ¡c phÃ©p chá»©ng minh toÃ¡n há»c',\n",
              " 'uit_000638': 'Is the end in sight for Theoretical Physics',\n",
              " 'uit_000639': 'dÃ¹ ráº¥t miá»n cÆ°á»¡ng, má»t vÃ i dá»ch vá»¥ Äiá»u dÆ°á»¡ng táº¡i gia',\n",
              " 'uit_000640': 'SiÃªu háº¥p dáº«n N=8 nhÆ° lÃ½ thuyáº¿t hÃ ng Äáº§u nháº±m giáº£i quyáº¿t nhiá»u bÃ i toÃ¡n ná»i báº­t mÃ  cÃ¡c nhÃ  váº­t lÃ½ Äang nghiÃªn cá»©u',\n",
              " 'uit_000641': 'má»t vÃ i dá»ch vá»¥ Äiá»u dÆ°á»¡ng táº¡i gia',\n",
              " 'uit_000642': 'thÃ´ng tin cá»§a má»t há» Äen bá» máº¥t khÃ´ng thá» phá»¥c há»i khi má»t há» Äen bá»c hÆ¡i',\n",
              " 'uit_000643': 'Nghá»ch lÃ½ thÃ´ng tin há» Äen nÃ y vi pháº¡m nguyÃªn lÃ½ cÆ¡ báº£n cá»§a cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000644': 'ThÃ¡ng 12 nÄm 1977',\n",
              " 'uit_000645': 'nhá»¯ng nÄm 1980',\n",
              " 'uit_000646': 'khi hÃ¡t táº¡i má»t dÃ n nháº¡c nhÃ  thá»',\n",
              " 'uit_000647': 'Jane vÃ  Hellyer Jones quyáº¿t Äá»nh khÃ´ng phÃ¡ vá»¡ gia ÄÃ¬nh vÃ  má»i quan há» cá»§a há» váº«n giá»¯ trong sÃ¡ng trong má»t thá»i gian dÃ i',\n",
              " 'uit_000648': 'giá»¯a nhá»¯ng nÄm 1980',\n",
              " 'uit_000649': 'náº£y ná» tÃ¬nh cáº£m lÃ£ng máº¡n vá»i nhau',\n",
              " 'uit_000650': 'nghiÃªn cá»©u lÃ½ thuyáº¿t lÆ°á»£ng tá»­ má»i',\n",
              " 'uit_000651': 'theo sau Vá»¥ Ná» Lá»n vÅ© trá»¥ ban Äáº§u má» rá»ng cá»±c ká»³ nhanh chÃ³ng trÆ°á»c khi giáº£m tá»c Äá» thÃ nh má»t sá»± giÃ£n ná» cháº­m hÆ¡n',\n",
              " 'uit_000652': 'vÅ© trá»¥ cÃ³ thá» khÃ´ng cÃ³ biÃªn-khÃ´ng cÃ³ Äiá»m Äáº§u hay Äiá»m cuá»i',\n",
              " 'uit_000653': 'theo sau Vá»¥ Ná» Lá»n vÅ© trá»¥ ban Äáº§u má» rá»ng cá»±c ká»³ nhanh chÃ³ng trÆ°á»c khi giáº£m tá»c Äá» thÃ nh má»t sá»± giÃ£n ná» cháº­m hÆ¡n',\n",
              " 'uit_000654': 'nghiÃªn cá»©u lÃ½ thuyáº¿t lÆ°á»£ng tá»­',\n",
              " 'uit_000655': 'vÅ© trá»¥ cÃ³ thá» khÃ´ng cÃ³ biÃªn-khÃ´ng cÃ³ Äiá»m Äáº§u hay Äiá»m cuá»i',\n",
              " 'uit_000656': 'VÅ© trá»¥ NguyÃªn thá»§y',\n",
              " 'uit_000657': 'Náº¿u vÅ© trá»¥ khÃ´ng cÃ³ biÃªn mÃ  tá»± bao bá»c... thÃ¬ ChÃºa sáº½ khÃ´ng cÃ³ báº¥t ká»³ tá»± do lá»±a chá»n nÃ o vá» viá»c vÅ© trá»¥ báº¯t Äáº§u ra sao',\n",
              " 'uit_000658': 'ÄÃ³ lÃ  Äiá»m mÃ  táº¥t cáº£ cÃ¡c ÄÆ°á»ng kinh tuyáº¿n hÆ°á»ng vá» phÃ­a báº¯c gáº·p nhau vÃ  káº¿t thÃºc',\n",
              " 'uit_000659': 'ÄÆ°á»£c thay tháº¿ báº±ng má»t vÃ¹ng tÆ°Æ¡ng tá»± nhÆ° Báº¯c Cá»±c',\n",
              " 'uit_000660': 'James Hartle',\n",
              " 'uit_000661': 'khÃ´ng cÃ³ biÃªn trong khÃ´ng-thá»i gian',\n",
              " 'uit_000662': 'ÄÃ³ lÃ  Äiá»m mÃ  táº¥t cáº£ cÃ¡c ÄÆ°á»ng kinh tuyáº¿n hÆ°á»ng vá» phÃ­a báº¯c gáº·p nhau vÃ  káº¿t thÃºc',\n",
              " 'uit_000663': 'James Hartle',\n",
              " 'uit_000664': 'khÃ´ng cáº§n thiáº¿t Äá» giáº£i thÃ­ch nguá»n gá»c cá»§a vÅ© trá»¥',\n",
              " 'uit_000665': 'má»t vÅ© trá»¥ má»',\n",
              " 'uit_000666': 'sá»± tá»n táº¡i cá»§a má»t Äáº¥ng SÃ¡ng Tháº¿',\n",
              " 'uit_000667': 'nÃ³ cÅ©ng tÆ°Æ¡ng thÃ­ch vá»i má»t vÅ© trá»¥ má»',\n",
              " 'uit_000668': 'nÄm 1981 Ã´ng nháº­n Huy chÆ°Æ¡ng Franklin, vÃ  nÄm 1982 nháº­n tÆ°á»c CBE (má»t tÆ°á»c báº­c hiá»p sÄ© háº¡ng tháº¥p cá»§a Äáº¿ quá»c Anh)',\n",
              " 'uit_000669': 'tÆ°á»c CBE',\n",
              " 'uit_000670': 'vÃ o lÃºc vÅ© trá»¥ ngá»«ng dÃ£n ná» vÃ  cuá»i cÃ¹ng suy sá»¥p, thá»i gian sáº½ cháº¡y theo hÆ°á»ng ngÆ°á»£c láº¡i',\n",
              " 'uit_000671': 'cÃ´ng bá» cá»§a Don Page vÃ  Raymond Laflamme',\n",
              " 'uit_000672': 'vÃ o lÃºc vÅ© trá»¥ ngá»«ng dÃ£n ná» vÃ  cuá»i cÃ¹ng suy sá»¥p, thá»i gian sáº½ cháº¡y theo hÆ°á»ng ngÆ°á»£c láº¡i',\n",
              " 'uit_000673': 'A Brief History of Time',\n",
              " 'uit_000674': 'Bantam Books',\n",
              " 'uit_000675': 'nháº­n má»t khoáº£n tiá»n Äáº·t cá»c lá»n cho tÃ¡c pháº©m',\n",
              " 'uit_000676': 'Bantam Books',\n",
              " 'uit_000677': 'A Brief History of Time',\n",
              " 'uit_000678': 'thanh toÃ¡n hÃ³a ÄÆ¡n, nÃªn dÆ°á»i nhu cáº§u trang tráº£i chi phÃ­ viá»c há»c hÃ nh cá»§a con cÃ¡i vÃ  sinh hoáº¡t gia ÄÃ¬nh',\n",
              " 'uit_000679': 'CÃ¡c y tÃ¡ ÄÆ°á»£c thuÃª suá»t ba ca Äá» chÄm sÃ³c Ã´ng hai mÆ°Æ¡i bá»n tiáº¿ng Äá»ng há» má»i ngÃ y',\n",
              " 'uit_000680': 'cÃ³ thá» Äe dá»a tÃ­nh máº¡ng',\n",
              " 'uit_000681': 'CÆ¡ quan ChÄm sÃ³c Sá»©c khá»e Anh',\n",
              " 'uit_000682': 'chÄm sÃ³c Äiá»u dÆ°á»¡ng suá»t ngÃ y ÄÃªm vÃ  loáº¡i bá» nÄng lá»±c phÃ¡t Ã¢m Ã­t á»i cÃ²n láº¡i cá»§a Ã´ng',\n",
              " 'uit_000683': 'chÄm sÃ³c Ã´ng hai mÆ°Æ¡i bá»n tiáº¿ng Äá»ng há» má»i ngÃ y',\n",
              " 'uit_000684': 'má»t quá»¹ á» Hoa Ká»³',\n",
              " 'uit_000685': 'sá»­ dá»¥ng má»t cÃ´ng táº¯c Ã´ng chá»n cÃ¡c cá»¥m tá»«, tá»«, hoáº·c chá»¯ cÃ¡i tá»« má»t bá» nhá» chá»©a khoáº£ng 2500-3000 lá»±a chá»n ÄÆ°á»£c quÃ©t qua bá»i mÃ¡y',\n",
              " 'uit_000686': 'Giá» tÃ´i ÄÃ¢m ra giao tiáº¿p tá»t hÆ¡n lÃ  trÆ°á»c khi tÃ´i máº¥t giá»ng nÃ³i',\n",
              " 'uit_000687': 'Equalizer',\n",
              " 'uit_000688': 'Giá» tÃ´i ÄÃ¢m ra giao tiáº¿p tá»t hÆ¡n lÃ  trÆ°á»c khi tÃ´i máº¥t giá»ng nÃ³i',\n",
              " 'uit_000689': 'sá»­ dá»¥ng má»t cÃ´ng táº¯c Ã´ng chá»n cÃ¡c cá»¥m tá»«, tá»«, hoáº·c chá»¯ cÃ¡i tá»« má»t bá» nhá» chá»©a khoáº£ng 2500-3000 lá»±a chá»n ÄÆ°á»£c quÃ©t qua bá»i mÃ¡y',\n",
              " 'uit_000690': 'thÃºc Äáº©y Ã´ng pháº£i giáº£i thÃ­ch cÃ¡c Ã½ tÆ°á»ng má»t cÃ¡ch rÃµ rÃ ng trong ngÃ´n ngá»¯ khÃ´ng mang tÃ­nh ká»¹ thuáº­t',\n",
              " 'uit_000691': 'yÃªu cáº§u trá»£ lÃ½ giÃºp Ã´ng hoÃ n thÃ nh viá»c viáº¿t \"LÆ°á»£c sá»­ Thá»i gian\"',\n",
              " 'uit_000692': 'trá» thÃ nh má»t thÃ nh cÃ´ng phi thÆ°á»ng, nhanh chÃ³ng vÆ°Æ¡n lÃªn Äáº§u cÃ¡c danh sÃ¡ch bÃ¡n cháº¡y nháº¥t á» cáº£ hai quá»c gia vÃ  duy trÃ¬ vá» trÃ­ khÃ´ng chá» nhiá»u tuáº§n mÃ  nhiá»u nÄm liÃªn tá»¥c',\n",
              " 'uit_000693': 'trá» thÃ nh má»t thÃ nh cÃ´ng phi thÆ°á»ng, nhanh chÃ³ng vÆ°Æ¡n lÃªn Äáº§u cÃ¡c danh sÃ¡ch bÃ¡n cháº¡y nháº¥t á» cáº£ hai quá»c gia vÃ  duy trÃ¬ vá» trÃ­ khÃ´ng chá» nhiá»u tuáº§n mÃ  nhiá»u nÄm liÃªn tá»¥c',\n",
              " 'uit_000694': 'yÃªu cáº§u trá»£ lÃ½ giÃºp Ã´ng hoÃ n thÃ nh viá»c viáº¿t \"LÆ°á»£c sá»­ Thá»i gian\"',\n",
              " 'uit_000695': 'ÄÆ°á»£c dá»ch sang nhiá»u thá»© tiáº¿ng, vÃ  tá»i nÄm 2009 bÃ¡n ÄÆ°á»£c Ã­t nháº¥t 9 triá»u báº£n',\n",
              " 'uit_000696': 'Master of the Universe',\n",
              " 'uit_000697': '\"LÆ°á»£c sá»­ Thá»i gian\" ÄÆ°á»£c dá»ch sang nhiá»u thá»© tiáº¿ng, vÃ  tá»i nÄm 2009 bÃ¡n ÄÆ°á»£c Ã­t nháº¥t 9 triá»u báº£n',\n",
              " 'uit_000698': 'Hawking ÄÃ£ du hÃ nh liÃªn tá»¥c Äá» quáº£ng bÃ¡ cÃ´ng trÃ¬nh cá»§a mÃ¬nh, vÃ  tham gia tiá»c tÃ¹ng vÃ  khiÃªu vÅ© tá»i táº­n ÄÃªm khuya',\n",
              " 'uit_000699': 'trong vai trÃ² ngÆ°á»i ná»i tiáº¿ng',\n",
              " 'uit_000700': 'nÄm báº±ng tiáº¿n sÄ© danh dá»±, Huy chÆ°Æ¡ng VÃ ng cá»§a Há»i ThiÃªn vÄn há»c HoÃ ng gia (1985), Huy chÆ°Æ¡ng Paul Dirac (1987) vÃ , cÃ¹ng vá»i Penrose, Giáº£i Wolf danh tiáº¿ng (1988). NÄm 1989, Ã´ng ÄÆ°á»£c Ná»¯ hoÃ ng Elizabeth II phong tÆ°á»c CH',\n",
              " 'uit_000701': 'Ã­t cÃ³ thá»i gian dÃ nh cho cÃ´ng viá»c vÃ  cÃ¡c há»c trÃ²',\n",
              " 'uit_000702': 'tÆ°á»c hiá»u dÃ¢n sá»± cao thá»© hai mÃ  má»t bÃ¬nh dÃ¢n Anh cÃ³ thá» Äáº¡t ÄÆ°á»£c, tháº¥p hÆ¡n HuÃ¢n chÆ°Æ¡ng CÃ´ng tráº¡ng-OM',\n",
              " 'uit_000703': 'khiáº¿n Ã´ng Ã­t cÃ³ thá»i gian dÃ nh cho cÃ´ng viá»c vÃ  cÃ¡c há»c trÃ²',\n",
              " 'uit_000704': 'chá»§ yáº¿u lÃ  do sá»± tÃ n táº­t cá»§a Ã´ng',\n",
              " 'uit_000705': 'sá»± tÃ n táº­t cá»§a Ã´ng',\n",
              " 'uit_000706': 'Quan Äiá»m báº¥t kháº£ tri vá» tÃ´n giÃ¡o cá»§a Hawking cÅ©ng tÆ°Æ¡ng pháº£n vá»i Äá»©c tin Ki tÃ´ giÃ¡o máº¡nh máº½ cá»§a ngÆ°á»i vá»£',\n",
              " 'uit_000707': 'Cuá»c hÃ´n nhÃ¢n giá»¯a Jane vÃ  Stephen Hawking',\n",
              " 'uit_000708': 'gÃ¢y thÃ¡ch thá»©c cho cÃ¡c Äá»ng nghiá»p vÃ  thÃ nh viÃªn gia ÄÃ¬nh',\n",
              " 'uit_000709': 'gÃ¢y thÃ¡ch thá»©c cho cÃ¡c Äá»ng nghiá»p vÃ  thÃ nh viÃªn gia ÄÃ¬nh',\n",
              " 'uit_000710': 'Äá» thÄm NhÃ n',\n",
              " 'uit_000711': 'nÄm 1990',\n",
              " 'uit_000712': 'Hawking trá» nÃªn ngÃ y cÃ ng gáº§n gÅ©i vá»i má»t trong sá» cÃ¡c y tÃ¡ cá»§a Ã´ng, Elaine Mason',\n",
              " 'uit_000713': 'nháº­n má»t cÃ´ gÃ¡i Viá»t Nam sá»ng á» LÃ ng tráº» em SOS tÃªn lÃ  Nguyá»n Thá» Thu NhÃ n lÃ m con nuÃ´i',\n",
              " 'uit_000714': 'Äá» xuáº¥t cá»§a Penrose vá» má»t \"phá»ng ÄoÃ¡n kiá»m duyá»t vÅ© trá»¥\"',\n",
              " 'uit_000715': 'Äá» xuáº¥t cá»§a Penrose vá» má»t \"phá»ng ÄoÃ¡n kiá»m duyá»t vÅ© trá»¥\"-ráº±ng khÃ´ng thá» nÃ o cÃ³ \"kÃ¬ dá» tráº§n truá»ng\" khÃ´ng che bá»i má»t chÃ¢n trá»i-lÃ  ÄÃºng',\n",
              " 'uit_000716': 'Gary Gibbons',\n",
              " 'uit_000717': 'Gary Gibbons',\n",
              " 'uit_000718': 'há» Äen vÃ  Vá»¥ Ná» Lá»n',\n",
              " 'uit_000719': 'quan niá»m vá» há» Äen cho bá»i thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t',\n",
              " 'uit_000720': 'theo hÆ°á»ng ngÆ°á»£c láº¡i, ráº±ng cÆ¡ há»c lÆ°á»£ng tá»­ Äá» xuáº¥t ráº±ng thÃ´ng tin phÃ¡t ra bá»i má»t há» Äen liÃªn quan tá»i thÃ´ng tin rÆ¡i vÃ o nÃ³ á» má»t thá»i Äiá»m trÆ°á»c Äáº¥y, quan niá»m vá» há» Äen cho bá»i thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t pháº£i ÄÆ°á»£c hiá»u chá»nh theo má»t cÃ¡ch nÃ o ÄÃ³',\n",
              " 'uit_000721': 'cÆ¡ há»c lÆ°á»£ng tá»­ Äá» xuáº¥t ráº±ng thÃ´ng tin phÃ¡t ra bá»i má»t há» Äen liÃªn quan tá»i thÃ´ng tin rÆ¡i vÃ o nÃ³ á» má»t thá»i Äiá»m trÆ°á»c Äáº¥y',\n",
              " 'uit_000722': 'nghá»ch lÃ½ thÃ´ng tin há» Äen',\n",
              " 'uit_000723': 'nghá»ch lÃ½ thÃ´ng tin há» Äen',\n",
              " 'uit_000724': 'Äem khoa há»c tá»i má»t lá»p cÃ´ng chÃºng rá»ng rÃ£i hÆ¡n',\n",
              " 'uit_000725': 'khoa há»c',\n",
              " 'uit_000726': 'Há» Äen vÃ  cÃ¡c VÅ© trá»¥ SÆ¡ sinh vÃ  nhá»¯ng Tiá»u luáº­n khÃ¡c',\n",
              " 'uit_000727': 'khoa há»c',\n",
              " 'uit_000728': 'Leonard Nimoy, ngÆ°á»i ÄÃ³ng vai Spock trong Star Trek, biáº¿t ÄÆ°á»£c ráº±ng Hawking há»©ng thÃº vá»i viá»c xuáº¥t hiá»n trong chÆ°Æ¡ng trÃ¬nh',\n",
              " 'uit_000729': 'biáº¿t ÄÆ°á»£c ráº±ng Hawking há»©ng thÃº vá»i viá»c xuáº¥t hiá»n trong chÆ°Æ¡ng trÃ¬nh',\n",
              " 'uit_000730': 'Keep Talking',\n",
              " 'uit_000731': 'sÃª-ri hÃ i ká»ch tÃ¬nh huá»ng The Simpsons',\n",
              " 'uit_000732': 'xuáº¥t hiá»n nÄm 1999 trong sÃª-ri hÃ i ká»ch tÃ¬nh huá»ng The Simpsons',\n",
              " 'uit_000733': 'má»t vÅ© trá»¥ há»c \"trÃªn-xuá»ng\", phÃ¡t biá»u ráº±ng vÅ© trá»¥ khÃ´ng pháº£i cÃ³ má»t tráº¡ng thÃ¡i ban Äáº§u duy nháº¥t mÃ  lÃ  nhiá»u tráº¡ng thÃ¡i, vÃ  do ÄÃ³ lÃ  khÃ´ng thÃ­ch há»£p Äá» hÃ¬nh thÃ nh má»t lÃ½ thuyáº¿t tiÃªn ÄoÃ¡n hÃ¬nh dáº¡ng hiá»n táº¡i cá»§a vÅ© trá»¥ tá»« má»t tráº¡ng thÃ¡i ban Äáº§u Äáº·c biá»t nÃ o',\n",
              " 'uit_000734': 'phÃ¡t triá»n má»t vÅ© trá»¥ há»c \"trÃªn-xuá»ng\"',\n",
              " 'uit_000735': 'VÅ© trá»¥ trong vá» háº¡t dáº»',\n",
              " 'uit_000736': 'tiÃªn ÄoÃ¡n hÃ¬nh dáº¡ng hiá»n táº¡i cá»§a vÅ© trá»¥ tá»« má»t tráº¡ng thÃ¡i ban Äáº§u Äáº·c biá»t nÃ o',\n",
              " 'uit_000737': 'thiáº¿u nhi',\n",
              " 'uit_000738': 'nÄm 2006',\n",
              " 'uit_000739': 'thiáº¿u nhi',\n",
              " 'uit_000740': 'Má»t phiÃªn báº£n hiá»u chá»nh cá»§a cuá»n sÃ¡ch trÆ°á»c ÄÃ¢y cá»§a Jane, nay mang tÃªn má»i \"HÃ nh trÃ¬nh tá»i VÃ´ háº¡n, Cuá»c Äá»i tÃ´i vá»i Stephen\", xuáº¥t hiá»n nÄm 2007',\n",
              " 'uit_000741': 'trÃ¬nh bÃ y váº­t lÃ½ lÃ½ thuyáº¿t theo cÃ¡ch dá» hiá»u vÃ  mÃ´ táº£ cÃ¡c nhÃ¢n váº­t tÆ°Æ¡ng tá»± cÃ¡c thÃ nh viÃªn gia ÄÃ¬nh Hawking',\n",
              " 'uit_000742': 'nÄm 2006',\n",
              " 'uit_000743': 'phiÃªn báº£n hiá»u chá»nh cá»§a cuá»n sÃ¡ch trÆ°á»c ÄÃ¢y cá»§a Jane, nay mang tÃªn má»i \"HÃ nh trÃ¬nh tá»i VÃ´ háº¡n, Cuá»c Äá»i tÃ´i vá»i Stephen\"',\n",
              " 'uit_000744': 'phim tÃ i liá»u cÃ³ tÃªn The Real Stephen Hawking: (2001) vÃ  \"Stephen Hawking: Profile\" (2002), má»t phim truyá»n hÃ¬nh Hawking vá» giai Äoáº¡n báº¯t Äáº§u cÄn bá»nh cá»§a Hawking (2004), cÃ¹ng má»t sÃª-ri phim tÃ i liá»u Stephen Hawking, Master of the Universe (2008)',\n",
              " 'uit_000745': 'mÃ¡y bay pháº£n lá»±c cÃ¡ nhÃ¢n',\n",
              " 'uit_000746': 'Chile, Äáº£o Phá»¥c Sinh, Nam Phi, rá»i TÃ¢y Ban Nha (Äá» nháº­n Giáº£i Fonseca nÄm 2008), Canada vÃ  nhiá»u chuyáº¿n Äi tá»i Hoa Ká»³',\n",
              " 'uit_000747': 'Chile, Äáº£o Phá»¥c Sinh, Nam Phi, rá»i TÃ¢y Ban Nha (Äá» nháº­n Giáº£i Fonseca nÄm 2008), Canada vÃ  nhiá»u chuyáº¿n Äi tá»i Hoa Ká»³',\n",
              " 'uit_000748': 'liÃªn quan tá»i sá»± tÃ n táº­t cá»§a Ã´ng',\n",
              " 'uit_000749': 'má»t loáº¡t tuyÃªn bá» gÃ¢y chÃº Ã½ vÃ  thÆ°á»ng gÃ¢y tranh cÃ£i',\n",
              " 'uit_000750': 'sá»± sá»ng trÃªn TrÃ¡i Äáº¥t bá» Äe dá»a do \"má»t cuá»c chiáº¿n tranh háº¡t nhÃ¢n Äá»t ngá»t, má»t virus ÄÆ°á»£c láº­p trÃ¬nh gien hay cÃ¡c má»i hiá»m há»a mÃ  chÃºng ta cÃ²n chÆ°a nghÄ© tá»i\"',\n",
              " 'uit_000751': 'má»t cuá»c chiáº¿n tranh háº¡t nhÃ¢n Äá»t ngá»t, má»t virus ÄÆ°á»£c láº­p trÃ¬nh gien hay cÃ¡c má»i hiá»m há»a mÃ  chÃºng ta cÃ²n chÆ°a nghÄ© tá»i',\n",
              " 'uit_000752': 'Ã´ng tá»«ng kháº³ng Äá»nh ráº±ng virus mÃ¡y tÃ­nh lÃ  má»t dáº¡ng sá»± sá»ng, ráº±ng con ngÆ°á»i nÃªn sá»­ dá»¥ng kÄ© thuáº­t di truyá»n Äá» trÃ¡nh khá»i bá» vÆ°á»£t máº·t bá»i mÃ¡y tÃ­nh, vÃ  ráº±ng ngÆ°á»i ngoÃ i hÃ nh tinh cÃ³ láº½ tá»n táº¡i vÃ  cáº§n trÃ¡nh giao tiáº¿p vá»i há» vÃ¬ há» cÃ³ thá» sáº½ chinh pháº¡t con ngÆ°á»i',\n",
              " 'uit_000753': 'Mong muá»n tÄng cÆ°á»ng má»i quan tÃ¢m cá»§a cÃ´ng chÃºng tá»i cÃ¡c chuyáº¿n bay ra ngoÃ i khÃ´ng gian vÃ  thá» hiá»n tiá»m nÄng cá»§a nhá»¯ng ngÆ°á»i tÃ n táº­t',\n",
              " 'uit_000754': 'cÃ¡c chuyáº¿n bay ra ngoÃ i khÃ´ng gian vÃ  thá» hiá»n tiá»m nÄng cá»§a nhá»¯ng ngÆ°á»i tÃ n táº­t',\n",
              " 'uit_000755': 'Ã´ng tá»«ng kháº³ng Äá»nh ráº±ng virus mÃ¡y tÃ­nh lÃ  má»t dáº¡ng sá»± sá»ng, ráº±ng con ngÆ°á»i nÃªn sá»­ dá»¥ng kÄ© thuáº­t di truyá»n Äá» trÃ¡nh khá»i bá» vÆ°á»£t máº·t bá»i mÃ¡y tÃ­nh, vÃ  ráº±ng ngÆ°á»i ngoÃ i hÃ nh tinh cÃ³ láº½ tá»n táº¡i vÃ  cáº§n trÃ¡nh giao tiáº¿p vá»i há» vÃ¬ há» cÃ³ thá» sáº½ chinh pháº¡t con ngÆ°á»i',\n",
              " 'uit_000756': 'cÃ¡c chuyáº¿n bay khÃ´ng gian vÃ  viá»c láº­p thuá»c Äá»a ngoÃ i vÅ© trá»¥',\n",
              " 'uit_000757': 'cÃ¡c chuyáº¿n bay khÃ´ng gian vÃ  viá»c láº­p thuá»c Äá»a ngoÃ i vÅ© trá»¥',\n",
              " 'uit_000758': 'bÃ y tá» sá»± á»§ng há» vá»i á»©ng cá»­ viÃªn DÃ¢n chá»§ Al Gore trong cuá»c báº§u cá»­ Tá»ng thá»ng Hoa Ká»³ nÄm 2000, gá»i Cuá»c táº¥n cÃ´ng Iraq 2003 lÃ  má»t \"tá»i Ã¡c chiáº¿n tranh\", táº©y chay má»t há»i tháº£o á» Israel do lo ngáº¡i vá» chÃ­nh sÃ¡ch cá»§a Israel Äá»i vá»i ngÆ°á»i Palestine, duy trÃ¬ chiáº¿n dá»ch lÃ¢u dÃ i cá»§a Ã´ng váº­n Äá»ng giáº£i trá»« vÅ© khÃ­ háº¡t nhÃ¢n, vÃ  á»§ng há» nghiÃªn cá»©u táº¿ bÃ o gá»c, há» thá»ng y táº¿ toÃ n cáº§u, vÃ  hÃ nh Äá»ng ngÄn cháº·n biáº¿n Äá»i khÃ­ háº­u',\n",
              " 'uit_000759': 'bÃ y tá» sá»± á»§ng há» vá»i á»©ng cá»­ viÃªn DÃ¢n chá»§ Al Gore trong cuá»c báº§u cá»­ Tá»ng thá»ng Hoa Ká»³ nÄm 2000, gá»i Cuá»c táº¥n cÃ´ng Iraq 2003 lÃ  má»t \"tá»i Ã¡c chiáº¿n tranh\", táº©y chay má»t há»i tháº£o á» Israel do lo ngáº¡i vá» chÃ­nh sÃ¡ch cá»§a Israel Äá»i vá»i ngÆ°á»i Palestine, duy trÃ¬ chiáº¿n dá»ch lÃ¢u dÃ i cá»§a Ã´ng váº­n Äá»ng giáº£i trá»« vÅ© khÃ­ háº¡t nhÃ¢n, vÃ  á»§ng há» nghiÃªn cá»©u táº¿ bÃ o gá»c, há» thá»ng y táº¿ toÃ n cáº§u, vÃ  hÃ nh Äá»ng ngÄn cháº·n biáº¿n Äá»i khÃ­ háº­u',\n",
              " 'uit_000760': 'má»t chiáº¿c xe lÄn, National Savings, British Telecom, Specsavers, Egg Banking, vÃ  Go Compare',\n",
              " 'uit_000761': 'Äáº£ng Lao Äá»ng',\n",
              " 'uit_000762': 'Äáº£ng Lao Äá»ng',\n",
              " 'uit_000763': 'National Savings, British Telecom, Specsavers, Egg Banking, vÃ  Go Compare',\n",
              " 'uit_000764': 'sá»± máº¥t mÃ¡t thÃ´ng tin cá»§a má»t há» Äen',\n",
              " 'uit_000765': 'cÃ¡c há» Äen cÃ³ nhiá»u hÆ¡n má»t tÃ´ pÃ´',\n",
              " 'uit_000766': 'Ã´ng láº­p luáº­n ráº±ng nghá»ch lÃ½ thÃ´ng tin ÄÆ°á»£c giáº£i thÃ­ch báº±ng cÃ¡ch kiá»m tra táº¥t cáº£ nhá»¯ng lá»ch sá»­ tÆ°Æ¡ng ÄÆ°Æ¡ng cá»§a vÅ© trá»¥, vá»i máº¥t mÃ¡t thÃ´ng tin trong nhá»¯ng vÅ© trá»¥ cÃ³ há» Äen sáº½ ÄÆ°á»£c triá»t tiÃªu bá»i nhá»¯ng vÅ© trá»¥ khÃ´ng cÃ³',\n",
              " 'uit_000767': 'nghá»ch lÃ½ thÃ´ng tin ÄÆ°á»£c giáº£i thÃ­ch báº±ng cÃ¡ch kiá»m tra táº¥t cáº£ nhá»¯ng lá»ch sá»­ tÆ°Æ¡ng ÄÆ°Æ¡ng cá»§a vÅ© trá»¥, vá»i máº¥t mÃ¡t thÃ´ng tin trong nhá»¯ng vÅ© trá»¥ cÃ³ há» Äen sáº½ ÄÆ°á»£c triá»t tiÃªu bá»i nhá»¯ng vÅ© trá»¥ khÃ´ng cÃ³',\n",
              " 'uit_000768': 'Hawking nhanh chÃ³ng thá»«a nháº­n thua cuá»c vÃ  nÃ³i ráº±ng Higgs nÃªn nháº­n ÄÆ°á»£c Giáº£i Nobel Váº­t lÃ½',\n",
              " 'uit_000769': 'vá» tháº¿ ná»i tiáº¿ng cá»§a Hawking Äem láº¡i cho Ã´ng ta sá»± tin cáº­y mÃ  ngÆ°á»i khÃ¡c khÃ´ng cÃ³',\n",
              " 'uit_000770': 'vá» tháº¿ ná»i tiáº¿ng cá»§a Hawking Äem láº¡i cho Ã´ng ta sá»± tin cáº­y mÃ  ngÆ°á»i khÃ¡c khÃ´ng cÃ³',\n",
              " 'uit_000771': 'thÃ¡ng 7 nÄm 2012',\n",
              " 'uit_000772': 'cÃ´ng khai vá» váº¥n Äá» nÃ y nÄm 2002 vÃ  tiáº¿p tá»¥c nÄm 2008, trong ÄÃ³ Higgs chá» trÃ­ch cÃ´ng trÃ¬nh cá»§a Hawking',\n",
              " 'uit_000773': 'Hawking ÄÃ£ kháº³ng Äá»nh dá»©t khoÃ¡t, vÃ  ÄÃ¡nh cÆ°á»£c, ráº±ng sáº½ khÃ´ng bao giá» tÃ¬m tháº¥y ÄÆ°á»£c Boson Higgs',\n",
              " 'uit_000774': 'Hawking ÄÃ£ kháº³ng Äá»nh dá»©t khoÃ¡t, vÃ  ÄÃ¡nh cÆ°á»£c, ráº±ng sáº½ khÃ´ng bao giá» tÃ¬m tháº¥y ÄÆ°á»£c Boson Higgs',\n",
              " 'uit_000775': 'tÃ¬nh tráº¡ng váº«n tá»nh tÃ¡o nhÆ°ng khÃ´ng thá» cá»­ Äá»ng báº¥t cá»© bá» pháº­n nÃ o ngoÃ i máº¯t',\n",
              " 'uit_000776': 'Ã´ng Äang há»£p tÃ¡c vá»i cÃ¡c nhÃ  nghiÃªn cá»©u vá» cÃ¡c há» thá»ng cÃ³ thá» diá»n dá»ch cÃ¡c hÃ¬nh áº£nh nÃ£o bá» hoáº·c biá»u diá»n nÃ©t máº·t thÃ nh phÆ°Æ¡ng thá»©c kÃ­ch hoáº¡t cÃ´ng táº¯c',\n",
              " 'uit_000777': 'Ã´ng báº¯t Äáº§u pháº£i Äiá»u khiá»n thiáº¿t bá» giao tiáº¿p báº±ng cá»­ Äá»ng cá»§a cÆ¡ mÃ¡ do khÃ´ng thá» sá»­ dá»¥ng tay ná»¯a, vá»i tá»c Äá» chá» má»t tá»« má»i phÃºt',\n",
              " 'uit_000778': 'Huy chÆ°Æ¡ng Copley tá»« Há»i HoÃ ng gia (2006), vinh dá»± dÃ¢n sá»± cao nháº¥t cá»§a Hoa Ká»³-HuÃ¢n chÆ°Æ¡ng Tá»± do Tá»ng thá»ng (2009), vÃ  Giáº£i thÆ°á»ng Váº­t lÃ½ CÆ¡ báº£n Nga (2012)',\n",
              " 'uit_000779': 'pháº£i Äiá»u khiá»n thiáº¿t bá» giao tiáº¿p báº±ng cá»­ Äá»ng cá»§a cÆ¡ mÃ¡ do khÃ´ng thá» sá»­ dá»¥ng tay ná»¯a, vá»i tá»c Äá» chá» má»t tá»« má»i phÃºt',\n",
              " 'uit_000780': 'Huy chÆ°Æ¡ng Copley',\n",
              " 'uit_000781': 'há»£p tÃ¡c vá»i cÃ¡c nhÃ  nghiÃªn cá»©u vá» cÃ¡c há» thá»ng cÃ³ thá» diá»n dá»ch cÃ¡c hÃ¬nh áº£nh nÃ£o bá» hoáº·c biá»u diá»n nÃ©t máº·t thÃ nh phÆ°Æ¡ng thá»©c kÃ­ch hoáº¡t cÃ´ng táº¯c',\n",
              " 'uit_000782': 'vÅ© trá»¥ ÄÆ°á»£c váº­n hÃ nh báº±ng cÃ¡c Äá»nh luáº­t khoa há»c. CÃ¡c Äá»nh luáº­t ÄÃ³ cÃ³ thá» ÄÆ°á»£c ChÃºa Trá»i ban bá», nhÆ°ng ChÃºa khÃ´ng can thiá»p Äá» phÃ¡ vá»¡ chÃºng',\n",
              " 'uit_000783': 'Curiosity trÃªn Discovery Channel',\n",
              " 'uit_000784': 'vÅ© trá»¥ ÄÆ°á»£c váº­n hÃ nh báº±ng cÃ¡c Äá»nh luáº­t khoa há»c',\n",
              " 'uit_000785': 'ThiÃªn ÄÆ°á»ng lÃ  má»t huyá»n thoáº¡i, tin ráº±ng \"khÃ´ng cÃ³ thiÃªn ÄÆ°á»ng hay tháº¿ giá»i bÃªn kia\"',\n",
              " 'uit_000786': 'ThiÃªn ÄÆ°á»ng lÃ  má»t huyá»n thoáº¡i, tin ráº±ng \"khÃ´ng cÃ³ thiÃªn ÄÆ°á»ng hay tháº¿ giá»i bÃªn kia\" vÃ  ráº±ng má»t khÃ¡i niá»m nhÆ° tháº¿ lÃ  \"má»t truyá»n cá» tÃ­ch dÃ nh cho nhá»¯ng ngÆ°á»i sá»£ bÃ³ng tá»i.\"',\n",
              " 'uit_000787': 'Curiosity trÃªn Discovery Channel',\n",
              " 'uit_000788': 'cÃ¡c nhÃ  khoa há»c \"ÄÃ£ trá» thÃ nh ngÆ°á»i mang ngá»n Äuá»c khÃ¡m phÃ¡ trong cuá»c truy táº§m tri thá»©c cá»§a chÃºng ta.\"',\n",
              " 'uit_000789': 'cÃ¡c triáº¿t gia \"khÃ´ng báº¯t ká»p vá»i nhá»¯ng tiáº¿n bá» khoa há»c hiá»n Äáº¡i\"',\n",
              " 'uit_000790': 'Khai sÃ¡ng',\n",
              " 'uit_000791': 'cÃ¡c váº¥n Äá» triáº¿t há»c cÃ³ thá» ÄÆ°á»£c khoa há»c tráº£ lá»i, Äáº·c biá»t lÃ  nhá»¯ng lÃ½ thuyáº¿t khoa há»c má»i \"dáº«n chÃºng ta tá»i má»t bá»©c tranh má»i vÃ  háº¿t sá»©c khÃ¡c biá»t vá» vÅ© trá»¥ vÃ  vá» trÃ­ cá»§a chÃºng ta trong nÃ³\"',\n",
              " 'uit_000792': 'Sá»± dÅ©ng cáº£m, can trÆ°á»ng cá»ng vá»i trÃ­ tuá», khiáº¿u hÃ i hÆ°á»c',\n",
              " 'uit_000793': 'Ãng lÃ  má»t nhÃ  khoa há»c vÃ  lÃ  má»t ngÆ°á»i ÄÃ n Ã´ng tuyá»t vá»i, ngÆ°á»i mÃ  nhá»¯ng cá»ng hiáº¿n vÃ  di sáº£n cá»§a mÃ¬nh sáº½ sá»ng mÃ£i nhiá»u nÄm ná»¯a',\n",
              " 'uit_000794': 'Ãng lÃ  má»t nhÃ  khoa há»c vÃ  lÃ  má»t ngÆ°á»i ÄÃ n Ã´ng tuyá»t vá»i, ngÆ°á»i mÃ  nhá»¯ng cá»ng hiáº¿n vÃ  di sáº£n cá»§a mÃ¬nh sáº½ sá»ng mÃ£i nhiá»u nÄm ná»¯a',\n",
              " 'uit_000795': 'VÅ© trá»¥ sáº½ cháº³ng cÃ³ nhiá»u Ã½ nghÄ©a náº¿u nhÆ° ÄÃ³ khÃ´ng pháº£i lÃ  mÃ¡i nhÃ  chá» che cho nhá»¯ng ngÆ°á»i báº¡n yÃªu thÆ°Æ¡ng',\n",
              " 'uit_000796': 'khoáº£ng cÃ¡ch Äi bá» cá»§a Hawking tá»i Bá» mÃ´n ToÃ¡n há»c á»¨ng dá»¥ng vÃ  Váº­t lÃ½ LÃ½ thuyáº¿t',\n",
              " 'uit_000797': 'thÃ¡ng 4 nÄm 1979',\n",
              " 'uit_000798': 'dá»± cÃ¡c há»i nghá» vÃ  liÃªn quan Äáº¿n váº­t lÃ½',\n",
              " 'uit_000799': 'thÃ¡ng 4 nÄm 1979',\n",
              " 'uit_000800': 'dá»± cÃ¡c há»i nghá» vÃ  liÃªn quan Äáº¿n váº­t lÃ½',\n",
              " 'uit_000801': 'bá»nh táº­t vÃ  nhá»¯ng thÃ¡ch thá»©c vá» cÆ¡ thá» cá»§a mÃ¬nh',\n",
              " 'uit_000802': 'Sá»± tÃ n táº­t cá»§a Ã´ng cÃ³ nghÄ©a lÃ  trÃ¡ch nhiá»m cá»§a gia ÄÃ¬nh Äáº·t trÃªn toÃ n bá» ÄÃ´i vai ngÆ°á»i vá»£ ngÃ y cÃ ng cáº£m tháº¥y quÃ¡ táº£i cá»§a Ã´ng',\n",
              " 'uit_000803': 'bá»nh táº­t vÃ  nhá»¯ng thÃ¡ch thá»©c vá» cÆ¡ thá» cá»§a mÃ¬nh, tháº­m chÃ­ - trong má»t tiá»n lá» ÄÆ°á»£c Äáº·t ra trong thá»i gian tÃ¡n tá»nh Jane',\n",
              " 'uit_000804': 'má»t sinh viÃªn sau Äáº¡i há»c hoáº·c sau tiáº¿n sÄ© Äáº¿n sá»ng vá»i há» vÃ  giÃºp Äá»¡ chÄm sÃ³c Hawking',\n",
              " 'uit_001209': 'KÃ¶niggrÃ¤tz',\n",
              " 'uit_001210': 'thá»ng nháº¥t nÆ°á»c Äá»©c',\n",
              " 'uit_001211': 'nÄm 1870',\n",
              " 'uit_001212': 'ngÃ y 18 thÃ¡ng 1 nÄm 1871',\n",
              " 'uit_001213': 'Thá»§ tÆ°á»ng Otto von Bismarck',\n",
              " 'uit_001214': 'Wilhelm I',\n",
              " 'uit_001215': 'thá»ng nháº¥t nÆ°á»c Äá»©c',\n",
              " 'uit_001216': 'Khoa há»c, cÃ´ng nghá», giÃ¡o dá»¥c',\n",
              " 'uit_001217': 'quÃ¢n sá»± vÃ  kinh táº¿',\n",
              " 'uit_001218': '65 triá»u ngÆ°á»i',\n",
              " 'uit_001219': '65 triá»u ngÆ°á»i',\n",
              " 'uit_001220': 'báº¡i tráº­n trong cuá»c Chiáº¿n tranh tháº¿ giá»i láº§n thá»© nháº¥t',\n",
              " 'uit_001221': 'quÃ¢n sá»± vÃ  kinh táº¿',\n",
              " 'uit_001222': 'máº«u má»±c',\n",
              " 'uit_001223': '65 triá»u ngÆ°á»i',\n",
              " 'uit_001224': 'táº¡o dá»±ng má»t guá»ng mÃ¡y',\n",
              " 'uit_001225': 'má»t vÆ°Æ¡ng quá»c Äá»c láº­p, ÄoÃ n káº¿t vÃ  hÃ¹ng máº¡nh',\n",
              " 'uit_001226': 'Tuyá»n háº§u tÆ°á»c (Elector) Friedrich Wilhelm I',\n",
              " 'uit_001227': '30.000 ngÆ°á»i',\n",
              " 'uit_001228': 'Friedrich Wilhelm I',\n",
              " 'uit_001229': '48 nÄm',\n",
              " 'uit_001230': '48 nÄm',\n",
              " 'uit_001231': 'Friedrich III',\n",
              " 'uit_001232': 'NgÃ y 18/1/1701',\n",
              " 'uit_001233': 'Friedrich II',\n",
              " 'uit_001234': 'náº¿u phong Friedrich lÃ m vua thÃ¬ cÃ¡c Tuyá»n háº§u tÆ°á»c cá»§a Hannover, Bayern vÃ  Sachsen cÅ©ng sáº½ muá»n lÃ m vua',\n",
              " 'uit_001235': 'HoÃ ng Äáº¿ cá»§a Äáº¿ quá»c La MÃ£ Tháº§n thÃ¡nh',\n",
              " 'uit_001236': 'NgÃ y 18/1/1701',\n",
              " 'uit_001237': 'náº¿u phong Friedrich lÃ m vua thÃ¬ cÃ¡c Tuyá»n háº§u tÆ°á»c cá»§a Hannover, Bayern vÃ  Sachsen cÅ©ng sáº½ muá»n lÃ m vua',\n",
              " 'uit_001238': 'Friedrich Wilhelm I',\n",
              " 'uit_001239': 'cÃ¡c cÃ´ng trÃ¬nh kiáº¿n trÃºc xa hoa phung phÃ­',\n",
              " 'uit_001240': 'cÃ¡c cÃ´ng trÃ¬nh kiáº¿n trÃºc xa hoa phung phÃ­',\n",
              " 'uit_001241': 'nhÃ  mÃ¡y lÃ m thuá»c sÃºng, lÃ² ÄÃºc Äáº¡i bÃ¡c, kho vÅ© khÃ­, doanh tráº¡i quÃ¢n Äá»i',\n",
              " 'uit_001242': 'má»¥c tiÃªu cá»§a Phá» lÃ  trá» nÃªn má»t cÆ°á»ng quá»c quÃ¢n sá»±',\n",
              " 'uit_001243': 'Vua Friedrich I',\n",
              " 'uit_001244': 'Friedrich Wilhelm I',\n",
              " 'uit_001245': 'nhÃ  mÃ¡y lÃ m thuá»c sÃºng, lÃ² ÄÃºc Äáº¡i bÃ¡c, kho vÅ© khÃ­, doanh tráº¡i quÃ¢n Äá»i',\n",
              " 'uit_001246': 'má»¥c tiÃªu cá»§a Phá» lÃ  trá» nÃªn má»t cÆ°á»ng quá»c quÃ¢n sá»±',\n",
              " 'uit_001247': 'xÃ¢y dá»±ng chá» vá»i má»¥c ÄÃ­ch quÃ¢n sá»±',\n",
              " 'uit_001248': 'Friedrich Wilhelm I',\n",
              " 'uit_001249': 'tá»nh Silesia',\n",
              " 'uit_001250': 'nÄm 1740',\n",
              " 'uit_001251': 'vÃ i thÃ¡ng',\n",
              " 'uit_001252': 'Äáº¿ quá»c La MÃ£ Tháº§n thÃ¡nh',\n",
              " 'uit_001253': 'Friedrich Äáº¡i Äáº¿',\n",
              " 'uit_001254': 'vÃ i thÃ¡ng',\n",
              " 'uit_001255': 'tá»nh Silesia',\n",
              " 'uit_001256': 'Äáº¿ quá»c La MÃ£ Tháº§n thÃ¡nh',\n",
              " 'uit_001257': 'Friedrich Äáº¡i Äáº¿',\n",
              " 'uit_001258': 'tiá»m lá»±c cá»t lÃµi',\n",
              " 'uit_001259': 'vÆ°Æ¡ng quá»c quÃ¢n sá»± kháº¯c khá»',\n",
              " 'uit_001260': 'chÃ­nh sÃ¡ch ngoáº¡i giao má»m dáº»o, sáºµn sÃ ng liÃªn minh vá»i báº¥t ká»³ tháº¿ lá»±c nÃ o',\n",
              " 'uit_001261': 'nghÃ¨o tÃºng',\n",
              " 'uit_001262': 'khÃ´ng cÃ³ Äáº¥t canh tÃ¡c, sá»ng cuá»c Äá»i vÃ´ cÃ¹ng cá»±c khá»',\n",
              " 'uit_001263': 'tiá»m lá»±c cá»t lÃµi',\n",
              " 'uit_001264': 'Äáº¥t khÃ´ cáº±n, khÃ´ng cÃ³ khoÃ¡ng sáº£n',\n",
              " 'uit_001265': 'chÃ­nh sÃ¡ch ngoáº¡i giao má»m dáº»o, sáºµn sÃ ng liÃªn minh vá»i báº¥t ká»³ tháº¿ lá»±c nÃ o',\n",
              " 'uit_001266': 'phá»¥c tÃ¹ng, lÃ m viá»c vÃ  hy sinh',\n",
              " 'uit_001267': 'má»t Äá»i quÃ¢n cÃ³ quá»c gia',\n",
              " 'uit_001268': 'phá»¥c tÃ¹ng, lÃ m viá»c vÃ  hy sinh',\n",
              " 'uit_001269': 'quÃ¢n vÆ°Æ¡ng',\n",
              " 'uit_001270': 'Äáº§u Ã³c háº¹p hÃ²i vÃ  qua quÃ¢n Äá»i cÃ³ ká»· luáº­t má»t cÃ¡ch tÃ n báº¡o',\n",
              " 'uit_001271': 'Hai pháº§n ba, vÃ  cÃ³ lÃºc nÄm pháº§n sÃ¡u',\n",
              " 'uit_001272': 'nÄm pháº§n sÃ¡u',\n",
              " 'uit_001273': 'cÃ´ng quá»c Schleswig vÃ  Holstein',\n",
              " 'uit_001274': 'tráº­n Sedan',\n",
              " 'uit_001275': 'cÃ´ng quá»c Schleswig vÃ  Holstein',\n",
              " 'uit_001276': 'giáº£i tÃ¡n nghá» viá»n rá»i tá»± huy Äá»ng nguá»n kinh phÃ­',\n",
              " 'uit_001277': 'Ão',\n",
              " 'uit_001278': 'Äan Máº¡ch',\n",
              " 'uit_001279': 'giáº£i tÃ¡n nghá» viá»n rá»i tá»± huy Äá»ng nguá»n kinh phÃ­',\n",
              " 'uit_001280': 'phÃ¡t Äá»ng ba cuá»c chiáº¿n',\n",
              " 'uit_001281': 'Ão',\n",
              " 'uit_001282': 'tráº­n Sedan',\n",
              " 'uit_001283': 'Hanover, Hesse, Nassua, Frankfurt vÃ  Elbe',\n",
              " 'uit_001284': 'chÆ°a Äáº§y 2 thÃ¡ng',\n",
              " 'uit_001285': 'vÆ°Æ¡ng quá»c Bayern',\n",
              " 'uit_001286': 'NapolÃ©on III',\n",
              " 'uit_001287': 'Hanover, Hesse, Nassua, Frankfurt vÃ  Elbe',\n",
              " 'uit_001288': 'nÄm 1870',\n",
              " 'uit_001289': 'vÅ© trang tá»t hÆ¡n, tá» chá»©c cao hÆ¡n vÃ  chiáº¿n Äáº¥u tá»t hÆ¡n',\n",
              " 'uit_001290': 'chá» do ThÆ°á»£ng Äáº¿ trao cho, chá»© khÃ´ng pháº£i tá»« nghá» viá»n hoáº·c qua dÃ¢n chÃºng',\n",
              " 'uit_001291': 'cháº¿ Äá» chuyÃªn cháº¿ quÃ¢n phiá»t',\n",
              " 'uit_001292': 'Äáº¡i diá»n nhÃ¢n dÃ¢n bÃ n cÃ£i cho háº£ dáº¡ hoáº·c máº·c cáº£ quyá»n lá»£i nhá» nhoi cho giai cáº¥p mÃ  há» lÃ m Äáº¡i diá»n',\n",
              " 'uit_001293': 'HoÃ ng Äáº¿ Wilhelm II',\n",
              " 'uit_001294': 'nÆ°á»c Phá»',\n",
              " 'uit_001295': 'chá» do ThÆ°á»£ng Äáº¿ trao cho, chá»© khÃ´ng pháº£i tá»« nghá» viá»n hoáº·c qua dÃ¢n chÃºng',\n",
              " 'uit_001296': 'Äáº¡i diá»n nhÃ¢n dÃ¢n bÃ n cÃ£i cho háº£ dáº¡ hoáº·c máº·c cáº£ quyá»n lá»£i nhá» nhoi cho giai cáº¥p mÃ  há» lÃ m Äáº¡i diá»n',\n",
              " 'uit_001297': 'cháº¿ Äá» chuyÃªn cháº¿ quÃ¢n phiá»t',\n",
              " 'uit_001298': 'cuá»c cÃ¡ch máº¡ng cÃ´ng nghiá»p',\n",
              " 'uit_001299': 'cháº¿ Äá» quÃ¢n phiá»t',\n",
              " 'uit_001300': 'chá»u trÃ¡ch nhiá»m vá»i chÃ­nh Ã´ng',\n",
              " 'uit_001301': 'cháº¿ Äá» chuyÃªn cháº¿ cá»§a vÆ°Æ¡ng triá»u Hohenzollern',\n",
              " 'uit_001302': 'cháº¿ Äá» quÃ¢n chá»§ nghá» viá»n',\n",
              " 'uit_001303': 'ÄÆ°á»£c hÆ°á»ng lá»£i tá»« cuá»c cÃ¡ch máº¡ng cÃ´ng nghiá»p',\n",
              " 'uit_001304': 'cháº¿ Äá» chuyÃªn cháº¿ cá»§a vÆ°Æ¡ng triá»u Hohenzollern',\n",
              " 'uit_001305': 'chá»u trÃ¡ch nhiá»m vá»i chÃ­nh Ã´ng',\n",
              " 'uit_001306': 'tá»« chá»§ vÃ  thá»£',\n",
              " 'uit_001307': 'TÃ´i ÄÃ£ nghiÃªn cá»©u phÃ¡p cháº¿ chá»§ nghÄ©a xÃ£ há»i cá»§a Bismarck',\n",
              " 'uit_001308': 'TÃ´i ÄÃ£ nghiÃªn cá»©u phÃ¡p cháº¿ chá»§ nghÄ©a xÃ£ há»i cá»§a Bismarck',\n",
              " 'uit_001309': 'Hitler',\n",
              " 'uit_001310': 'thiáº¿t láº­p chÆ°Æ¡ng trÃ¬nh an ninh xÃ£ há»i rá»ng rÃ£i',\n",
              " 'uit_001311': 'ÄÃ¡nh giÃ¡ cao',\n",
              " 'uit_001312': 'tÆ°á»ng Paul von Hindenburg',\n",
              " 'uit_001313': 'Äá»©c hoÃ ng Wilhelm II',\n",
              " 'uit_001314': 'Äáº¿ quá»c Ão-Hung vÃ  Äáº¿ quá»c Nga',\n",
              " 'uit_001315': 'nÄm 1914',\n",
              " 'uit_001316': 'tÆ°á»ng Paul von Hindenburg',\n",
              " 'uit_001317': 'tráº­n Grunwald',\n",
              " 'uit_001318': 'tráº­n Grunwald',\n",
              " 'uit_001319': 'Äáº¿ quá»c Ão-Hung vÃ  Äáº¿ quá»c Nga',\n",
              " 'uit_001320': 'phÃ­a TÃ¢y lÃ  má»i Äe dá»a lá»n nháº¥t cho Äáº¿ quá»c Äá»©c',\n",
              " 'uit_001321': 'Bismarck khai máº¡c Nghá» viá»n Äáº§u tiÃªn cá»§a Äáº¿ cháº¿ thá»© hai vÃ o nÄm 1871',\n",
              " 'uit_001322': 'cÃ¡c hoÃ ng Äáº¿ vÆ°Æ¡ng triá»u Hohenzollern',\n",
              " 'uit_001323': 'cÃ¡c hoÃ ng Äáº¿ vÆ°Æ¡ng triá»u Hohenzollern',\n",
              " 'uit_001324': 'Cung Äiá»n Sanssouci',\n",
              " 'uit_001325': 'vinh quang cá»§a Äáº¿ quá»c Äá»©c trÆ°á»c ÄÃ³',\n",
              " 'uit_001326': 'TÃ²a nhÃ  Nghá» viá»n bá» chÃ¡y',\n",
              " 'uit_001327': 'vinh quang cá»§a Äáº¿ quá»c Äá»©c trÆ°á»c ÄÃ³',\n",
              " 'uit_001328': 'TÃ²a nhÃ  Nghá» viá»n bá» chÃ¡y',\n",
              " 'uit_001329': 'Cung Äiá»n Sanssouci',\n",
              " 'uit_001330': 'NhÃ  thá» Doanh tráº¡i Potsdam',\n",
              " 'uit_001331': 'Bismarck khai máº¡c Nghá» viá»n Äáº§u tiÃªn cá»§a Äáº¿ cháº¿ thá»© hai vÃ o nÄm 1871',\n",
              " 'uit_001332': 'thÃ nh tá»±u sÃ¡ng chÃ³i mÃ  ngÆ°á»i Äá»©c Äáº¡t ÄÆ°á»£c',\n",
              " 'uit_001333': 'viá»c dung dÆ°á»¡ng ngÆ°á»i Do ThÃ¡i vÃ  ngÆ°á»i theo MÃ¡c-xÃ­t, tÆ° tÆ°á»ng trá»ng váº­t cháº¥t vÃ  Ã­ch ká»· cá»§a giá»i trung lÆ°u, áº£nh hÆ°á»ng báº¥t chÃ­nh cá»§a nhá»¯ng káº» \"luá»n cÃºi vÃ  xu ná»nh\" quanh ngai vÃ ng Hohenzollern, \"chÃ­nh sÃ¡ch liÃªn minh tai háº¡i\" vá»i VÆ°Æ¡ng triá»u Habsburg suy Äá»i vÃ  ngÆ°á»i Ã khÃ´ng ÄÃ¡ng tin thay vÃ¬ vá»i Anh, thiáº¿u chÃ­nh sÃ¡ch vá» chá»§ng tá»c vÃ  xÃ£ há»i cÆ¡ báº£n',\n",
              " 'uit_001334': '\"luá»n cÃºi vÃ  xu ná»nh\" quanh ngai vÃ ng Hohenzollern',\n",
              " 'uit_001335': 'sáº½ kháº¯c phá»¥c',\n",
              " 'uit_001336': 'Äá»©c Quá»c xÃ£ sáº½ kháº¯c phá»¥c',\n",
              " 'uit_001337': 'viá»c dung dÆ°á»¡ng ngÆ°á»i Do ThÃ¡i vÃ  ngÆ°á»i theo MÃ¡c-xÃ­t, tÆ° tÆ°á»ng trá»ng váº­t cháº¥t vÃ  Ã­ch ká»· cá»§a giá»i trung lÆ°u, áº£nh hÆ°á»ng báº¥t chÃ­nh cá»§a nhá»¯ng káº» \"luá»n cÃºi vÃ  xu ná»nh\" quanh ngai vÃ ng Hohenzollern, \"chÃ­nh sÃ¡ch liÃªn minh tai háº¡i\" vá»i VÆ°Æ¡ng triá»u Habsburg suy Äá»i vÃ  ngÆ°á»i Ã khÃ´ng ÄÃ¡ng tin thay vÃ¬ vá»i Anh, thiáº¿u chÃ­nh sÃ¡ch vá» chá»§ng tá»c vÃ  xÃ£ há»i cÆ¡ báº£n',\n",
              " 'uit_001665': 'viá»c quan sÃ¡t quá»¹ Äáº¡o cá»§a chÃºng sáº½ giÃºp cho viá»c xÃ¡c Äá»nh khá»i lÆ°á»£ng cá»§a chÃºng',\n",
              " 'uit_001666': 'báº¡n Äá»ng hÃ nh',\n",
              " 'uit_001667': 'ngoáº¡i suy tá»« nhá»¯ng sao ÄÃ´i',\n",
              " 'uit_001668': 'viá»c quan sÃ¡t quá»¹ Äáº¡o cá»§a chÃºng sáº½ giÃºp cho viá»c xÃ¡c Äá»nh khá»i lÆ°á»£ng cá»§a chÃºng',\n",
              " 'uit_001669': 'ngoáº¡i suy tá»« nhá»¯ng sao ÄÃ´i',\n",
              " 'uit_001670': 'má»t há» thá»ng gá»m hai ngÃ´i sao chuyá»n Äá»ng trÃªn quá»¹ Äáº¡o cá»§a khá»i tÃ¢m hai ngÃ´i sao',\n",
              " 'uit_001671': 'báº¡n Äá»ng hÃ nh',\n",
              " 'uit_001672': 'binary',\n",
              " 'uit_001673': 'Äáº¡i HÃ¹ng',\n",
              " 'uit_001674': 'há» sao ÄÃ´i',\n",
              " 'uit_001675': 'binary',\n",
              " 'uit_001676': 'Äá»nh luáº­t háº¥p dáº«n',\n",
              " 'uit_001677': 'khoáº£ng 50 cáº·p',\n",
              " 'uit_001678': 'Sir William Herschel',\n",
              " 'uit_001679': 'hiá»u á»©ng Doppler cá»§a Ã¡nh sÃ¡ng phÃ¡t ra',\n",
              " 'uit_001680': 'hiá»u á»©ng Doppler cá»§a Ã¡nh sÃ¡ng phÃ¡t ra',\n",
              " 'uit_001681': 'ÄÆ°á»£c phÃ¢n biá»t báº±ng má»t kÃ­nh viá»n vá»ng Äá»§ máº¡nh',\n",
              " 'uit_001682': 'ÄÆ°á»£c phÃ¢n biá»t báº±ng má»t kÃ­nh viá»n vá»ng Äá»§ máº¡nh',\n",
              " 'uit_001683': 'cÃ¡c ÄÆ°á»ng quang phá» trong Ã¡nh sÃ¡ng tá»« má»i ngÃ´i sao ban Äáº§u bá» dá»ch chuyá»n xanh, sau ÄÃ³ bá» dá»ch chuyá»n Äá» khi nÃ³ Äáº§u tiÃªn di chuyá»n vá» phÃ­a chÃºng ta, rá»i láº¡i di chuyá»n ra xa chÃºng ta, trong khi chÃºng chuyá»n Äá»ng quanh khá»i tÃ¢m chung, vá»i chu ká»³ quá»¹ Äáº¡o chung cá»§a chÃºng',\n",
              " 'uit_001684': 'lá»±c háº«p dáº«n',\n",
              " 'uit_001685': 'nhá»¯ng cáº·p sao náº±m gáº§n nhau tá»i má»©c cÃ¡c ÄÆ°á»ng quang phá» trong Ã¡nh sÃ¡ng tá»« má»i ngÃ´i sao ban Äáº§u bá» dá»ch chuyá»n xanh, sau ÄÃ³ bá» dá»ch chuyá»n Äá» khi nÃ³ Äáº§u tiÃªn di chuyá»n vá» phÃ­a chÃºng ta, rá»i láº¡i di chuyá»n ra xa chÃºng ta, trong khi chÃºng chuyá»n Äá»ng quanh khá»i tÃ¢m chung, vá»i chu ká»³ quá»¹ Äáº¡o chung cá»§a chÃºng',\n",
              " 'uit_001686': 'nhanh',\n",
              " 'uit_001687': 'bá»i vÃ¬ chÃºng á» gáº§n nhau',\n",
              " 'uit_001688': 'nhanh',\n",
              " 'uit_001689': 'khÃ¡ gáº§n',\n",
              " 'uit_001690': 'khÃ¡ xa',\n",
              " 'uit_001691': 'Nhá»¯ng sao ÄÃ´i thá» giÃ¡c thÆ°á»ng cÃ¡ch nhau khÃ¡ xa vÃ  thÆ°á»ng cÃ³ nhá»¯ng tá»c Äá» quá»¹ Äáº¡o quÃ¡ nhá» Äá» cÃ³ thá» Äo Äáº¡c ÄÆ°á»£c báº±ng quang phá»',\n",
              " 'uit_001692': 'khÃ¡ gáº§n',\n",
              " 'uit_001693': 'ráº¥t hiáº¿m',\n",
              " 'uit_001694': 'khÃ´ng phÃ¡t ra báº¥t ká»³ má»t bá»©c xáº¡ Äiá»n tá»« nÃ o',\n",
              " 'uit_001695': 'sao neutron',\n",
              " 'uit_001696': 'sao neutron',\n",
              " 'uit_001697': 'gáº¥p khoáº£ng mÆ°á»i láº§n',\n",
              " 'uit_001698': 'ráº¥t tá»i',\n",
              " 'uit_001699': 'cÃ¡c sao ÄÃ´i dao Äá»ng astrometric binaries',\n",
              " 'uit_001700': 'ráº¥t tá»i',\n",
              " 'uit_001701': 'háº§u nhÆ° tiáº¿p xÃºc vá»i nhau',\n",
              " 'uit_001702': 'Äá» lá»ch tÃ¢m cá»§a quá»¹ Äáº¡o',\n",
              " 'uit_001703': '100',\n",
              " 'uit_001704': 'tiáº¿p xÃºc',\n",
              " 'uit_001705': 'Ã­t hÆ¡n',\n",
              " 'uit_001706': '100',\n",
              " 'uit_001707': 'cÃ¡c há» cÃ³ chu ká»³ quá»¹ Äáº¡o ngáº¯n thÃ¬ cÃ³ Äá» lá»ch tÃ¢m Ã­t hÆ¡n',\n",
              " 'uit_001708': 'HD 188753 Ab',\n",
              " 'uit_001709': 'má»t',\n",
              " 'uit_001710': 'HD 188753 Ab',\n",
              " 'uit_001711': 'TrÃªn thá»±c táº¿, Äa sá» cÃ¡c quá»¹ Äáº¡o cá»§a chÃºng khÃ´ng cÃ¢n báº±ng bá»n (hÃ nh tinh cÃ³ thá» bá» Äáº©y khá»i quá»¹ Äáº¡o cá»§a nÃ³ khÃ¡ nhanh chÃ³ng, cÃ³ thá» bá» báº¯n hoÃ n toÃ n khá»i há» hay bá» chuyá»n sang má»t quá»¹ Äáº¡o lá»ch vÃ o trong hay lÃ¹i ra ngoÃ i hÆ¡n), trong khi nhá»¯ng quá»¹ Äáº¡o khÃ¡c láº¡i thá»±c sá»± kháº¯c nghiá»t cho viá»c hÃ¬nh thÃ nh sinh quyá»n bá»i vÃ¬ chÃºng cÃ³ khÃ¡c biá»t ráº¥t lá»n vá» nhiá»t Äá» bá» máº·t á» nhá»¯ng khoáº£ng cÃ¡ch quá»¹ Äáº¡o khÃ¡c nhau',\n",
              " 'uit_001712': 'thÃ¡ng 7 nÄm 2005',\n",
              " 'uit_001713': 'thÃ¡ng 7 nÄm 2005',\n",
              " 'uit_001714': 'cÃ¡c hÃ nh tinh cá»§a cÃ¡c sao ÄÃ´i hay sao ba',\n",
              " 'uit_001858': 'do sá»± gia tÄng cÃ¡c hoáº¡t Äá»ng táº¡o ra cÃ¡c cháº¥t tháº£i khÃ­ nhÃ  kÃ­nh, cÃ¡c hoáº¡t Äá»ng khai thÃ¡c quÃ¡ má»©c cÃ¡c bá» háº¥p thá»¥ vÃ  bá» chá»©a khÃ­ nhÃ  kÃ­nh nhÆ° sinh khá»i, rá»«ng, cÃ¡c há» sinh thÃ¡i biá»n, ven bá» vÃ  Äáº¥t liá»n khÃ¡c',\n",
              " 'uit_001859': 'lÃ  sá»± thay Äá»i cá»§a há» thá»ng khÃ­ háº­u gá»m khÃ­ quyá»n, thá»§y quyá»n, sinh quyá»n, tháº¡ch quyá»n hiá»n táº¡i vÃ  trong tÆ°Æ¡ng lai bá»i cÃ¡c nguyÃªn nhÃ¢n tá»± nhiÃªn vÃ  nhÃ¢n táº¡o trong má»t giai Äoáº¡n nháº¥t Äá»nh tÃ­nh báº±ng tháº­p ká»· hay hÃ ng triá»u nÄm',\n",
              " 'uit_001860': 'khÃ­ quyá»n, thá»§y quyá»n, sinh quyá»n, tháº¡ch quyá»n',\n",
              " 'uit_001861': 'nÃ³ng lÃªn toÃ n cáº§u',\n",
              " 'uit_001862': 'lÃ  sá»± thay Äá»i cá»§a há» thá»ng khÃ­ háº­u gá»m khÃ­ quyá»n, thá»§y quyá»n, sinh quyá»n, tháº¡ch quyá»n hiá»n táº¡i vÃ  trong tÆ°Æ¡ng lai bá»i cÃ¡c nguyÃªn nhÃ¢n tá»± nhiÃªn vÃ  nhÃ¢n táº¡o trong má»t giai Äoáº¡n nháº¥t Äá»nh tÃ­nh báº±ng tháº­p ká»· hay hÃ ng triá»u nÄm',\n",
              " 'uit_001863': 'United Nations',\n",
              " 'uit_001864': 'CÃ´ng Æ°á»c Khung cá»§a LiÃªn há»£p Quá»c vá» Biáº¿n Äá»i KhÃ­ háº­u',\n",
              " 'uit_001865': 'do tÃ¡c Äá»ng cá»§a hoáº¡t Äá»ng con ngÆ°á»i',\n",
              " 'uit_001866': 'áº¥m lÃªn toÃ n cáº§u',\n",
              " 'uit_001867': 'lÃ  sá»± thay Äá»i cá»§a khÃ­ háº­u mÃ  hoáº·c trá»±c tiáº¿p hoáº·c giÃ¡n tiáº¿p do tÃ¡c Äá»ng cá»§a hoáº¡t Äá»ng con ngÆ°á»i dáº«n Äáº¿n thay Äá»i thÃ nh pháº§n khÃ­ quyá»n toÃ n cáº§u vÃ  ngoÃ i ra lÃ  nhá»¯ng biáº¿n thiÃªn tá»± nhiÃªn cá»§a khÃ­ háº­u ÄÆ°á»£c quan sÃ¡t trÃªn má»t chu ká»³ thá»i gian dÃ i',\n",
              " 'uit_001868': 'Climate justice',\n",
              " 'uit_001869': 'má»t thuáº­t ngá»¯ sá»­ dá»¥ng cho khung sá»± nÃ³ng lÃªn toÃ n cáº§u cÃ³ liÃªn quan tá»i váº¥n Äá» vá» Äáº¡o Äá»©c, vÃ  chÃ­nh trá», chá»© khÃ´ng chá» ÄÆ¡n thuáº§n lÃ  hoÃ n toÃ n vá» mÃ´i trÆ°á»ng, hoáº·c thiÃªn nhiÃªn ÄÆ¡n thuáº§n',\n",
              " 'uit_001870': 'lÃ  má»t thuáº­t ngá»¯ sá»­ dá»¥ng cho khung sá»± nÃ³ng lÃªn toÃ n cáº§u cÃ³ liÃªn quan tá»i váº¥n Äá» vá» Äáº¡o Äá»©c, vÃ  chÃ­nh trá», chá»© khÃ´ng chá» ÄÆ¡n thuáº§n lÃ  hoÃ n toÃ n vá» mÃ´i trÆ°á»ng, hoáº·c thiÃªn nhiÃªn ÄÆ¡n thuáº§n',\n",
              " 'uit_001871': 'vÃ¬ khá»i lÆ°á»£ng lá»n',\n",
              " 'uit_001872': 'hÃ ng tháº¿ ká»· hoáº·c lÃ¢u hÆ¡n',\n",
              " 'uit_001873': 'thay Äá»i bá»©c xáº¡ khÃ­ quyá»n, bao gá»m cÃ¡c quÃ¡ trÃ¬nh nhÆ° biáº¿n Äá»i bá»©c xáº¡ máº·t trá»i, Äá» lá»ch quá»¹ Äáº¡o cá»§a TrÃ¡i Äáº¥t, quÃ¡ trÃ¬nh kiáº¿n táº¡o nÃºi, kiáº¿n táº¡o trÃ´i dáº¡t lá»¥c Äá»a vÃ  sá»± thay Äá»i ná»ng Äá» khÃ­ nhÃ  kÃ­nh',\n",
              " 'uit_001874': 'cÃ³ thá» máº¥t hÃ ng tháº¿ ká»· hoáº·c lÃ¢u',\n",
              " 'uit_001875': 'cháº­m',\n",
              " 'uit_001876': 'Äáº¡i dÆ°Æ¡ng vÃ  chá»m bÄng, pháº£n á»©ng cháº­m vá»i biáº¿n Äá»i bá»©c xáº¡ máº·t trá»i vÃ¬ khá»i lÆ°á»£ng lá»n',\n",
              " 'uit_001877': 'ná»n táº£ng',\n",
              " 'uit_001878': 'vÃ i nÄm Äáº¿n vÃ i tháº­p niÃªn',\n",
              " 'uit_001879': 'ÄÃ³ng vai trÃ² quan trá»ng trong sá»± tÃ¡i phÃ¢n bá» nhiá»t trong Äáº¡i dÆ°Æ¡ng trÃªn tháº¿ giá»i',\n",
              " 'uit_001880': 'vÃ i nÄm Äáº¿n vÃ i tháº­p niÃªn',\n",
              " 'uit_001881': '3 kiá»u',\n",
              " 'uit_001882': 'gÃ¢y ra nhá»¯ng thay Äá»i vá» sá»± phÃ¢n bá» nÄng lÆ°á»£ng máº·t trá»i theo mÃ¹a trÃªn bá» máº·t TrÃ¡i Äáº¥t vÃ  cÃ¡ch nÃ³ ÄÆ°á»£c phÃ¢n bá» trÃªn toÃ n cáº§u',\n",
              " 'uit_001883': 'thay Äá»i quá»¹ Äáº¡o lá»ch tÃ¢m cá»§a TrÃ¡i Äáº¥t, thay Äá»i trá»¥c quay, vÃ  tiáº¿n Äá»ng cá»§a trá»¥c TrÃ¡i Äáº¥t',\n",
              " 'uit_001884': '3 kiá»u',\n",
              " 'uit_001885': 'cÃ³ thá» gÃ¢y biáº¿n Äá»i máº¡nh máº½ vá» sá»± phÃ¢n bá» cÃ¡c mÃ¹a vÃ  Äá»a lÃ½',\n",
              " 'uit_001886': 'áº£nh hÆ°á»ng máº¡nh máº½ Äáº¿n khÃ­ háº­u vÃ  má»i tÆ°Æ¡ng quan cá»§a chÃºng vá»i cÃ¡c chu ká»³ bÄng hÃ  vÃ  gian bÄng, quan há» cá»§a chÃºng vá»i sá»± phÃ¡t triá»n vÃ  thoÃ¡i lui cá»§a Sahara, vÃ  Äá»i vá»i sá»± xuáº¥t hiá»n cá»§a chÃºng trong cÃ¡c Äá»a táº§ng',\n",
              " 'uit_001887': 'cÃ³ thá» gÃ¢y biáº¿n Äá»i máº¡nh máº½ vá» sá»± phÃ¢n bá» cÃ¡c mÃ¹a vÃ  Äá»a lÃ½',\n",
              " 'uit_001888': 'quá»¹ Äáº¡o TrÃ¡i Äáº¥t',\n",
              " 'uit_001889': 'gÃ¢y ra sá»± áº¥m lÃªn toÃ n cáº§u vÃ  tuyá»t chá»§ng hÃ ng loáº¡t',\n",
              " 'uit_001890': 'Nhiá»t Äá» toÃ n cáº§u giáº£m khoáº£ng 0,5 Â°C (0.9 Â°F)',\n",
              " 'uit_001891': 'gÃ¢y ra lÃ m mÃ¡t (báº±ng má»t pháº§n ngÄn cháº·n sá»± lÃ¢y truyá»n cá»§a bá»©c xáº¡ máº·t trá»i Äáº¿n bá» máº·t TrÃ¡i Äáº¥t) trong thá»i gian má»t vÃ i nÄm',\n",
              " 'uit_001892': 'gÃ¢y ra lÃ m mÃ¡t (báº±ng má»t pháº§n ngÄn cháº·n sá»± lÃ¢y truyá»n cá»§a bá»©c xáº¡ máº·t trá»i Äáº¿n bá» máº·t TrÃ¡i Äáº¥t) trong thá»i gian má»t vÃ i nÄm',\n",
              " 'uit_001893': 'vá»¥ phun trÃ o nÄm 1912 cá»§a nÃºi lá»­a Novarupta',\n",
              " 'uit_001894': 'Novarupta',\n",
              " 'uit_001895': 'vá»¥ phun trÃ o cá»§a nÃºi lá»­a Pinatubo',\n",
              " 'uit_001896': 'gÃ¢y ra sá»± áº¥m lÃªn toÃ n cáº§u vÃ  tuyá»t chá»§ng hÃ ng loáº¡t',\n",
              " 'uit_001897': 'sá»± hÃ¬nh thÃ nh eo Äáº¥t Panama',\n",
              " 'uit_001898': 'khoáº£ng 300 Äáº¿n 365 triá»u nÄm trÆ°á»c',\n",
              " 'uit_001899': 'Äáº¡i TÃ¢y DÆ°Æ¡ng vÃ  ThÃ¡i BÃ¬nh DÆ°Æ¡ng',\n",
              " 'uit_001900': 'áº£nh hÆ°á»ng ráº¥t máº¡nh máº½ Äáº¿n cÃ¡c cháº¿ Äá» Äá»ng lá»±c há»c cá»§a Äáº¡i dÆ°Æ¡ng cá»§a háº£i lÆ°u Gulf Stream vÃ  ÄÃ£ lÃ m cho báº¯c bÃ¡n cáº§u bá» phá»§ bÄng',\n",
              " 'uit_001901': 'khoáº£ng 300 Äáº¿n 365 triá»u nÄm trÆ°á»c',\n",
              " 'uit_001902': 'ÄÃ³ng vai trÃ² quan trá»ng trong viá»c kiá»m soÃ¡t sá»± truyá»n nhiá»t vÃ  Äá» áº©m trÃªn toÃ n cáº§u vÃ  hÃ¬nh thÃ nh nÃªn khÃ­ háº­u toÃ n cáº§u',\n",
              " 'uit_001903': 'Vá» trÃ­ cá»§a cÃ¡c biá»n ÄÃ³ng vai trÃ² quan trá»ng trong viá»c kiá»m soÃ¡t sá»± truyá»n nhiá»t vÃ  Äá» áº©m trÃªn toÃ n cáº§u vÃ  hÃ¬nh thÃ nh nÃªn khÃ­ háº­u toÃ n cáº§u',\n",
              " 'uit_001904': 'áº£nh hÆ°á»ng ráº¥t máº¡nh máº½ Äáº¿n cÃ¡c cháº¿ Äá» Äá»ng lá»±c há»c cá»§a Äáº¡i dÆ°Æ¡ng cá»§a háº£i lÆ°u Gulf Stream vÃ  ÄÃ£ lÃ m cho báº¯c bÃ¡n cáº§u bá» phá»§ bÄng',\n",
              " 'uit_001905': 'nhiá»t Äá», lÆ°á»£ng tuyáº¿t rÆ¡i, lÆ°á»£ng nÆ°á»c náº±m giá»¯a vÃ  dÆ°á»i lá»p bÄng',\n",
              " 'uit_001906': 'má»t sÃ´ng bÄng vá»n hÃ¬nh thÃ nh tá»« nhiá»u sÃ´ng bÄng nhá» khÃ¡c nhau pháº£i tá»n trung bÃ¬nh hÃ ng tháº¿ ká» hoáº·c tháº­m chÃ­ lÃ¢u hÆ¡n Äá» tan ra bá»i tÃ¡c Äá»ng cá»§a nhá»¯ng biáº¿n Äá»i ngáº¯n háº¡n cá»§a vÃ¹ng',\n",
              " 'uit_001907': 'hÃ ng tháº¿ ká» hoáº·c tháº­m chÃ­ lÃ¢u hÆ¡n',\n",
              " 'uit_001908': 'Sá»± thay Äá»i vá» nhiá»t Äá», lÆ°á»£ng tuyáº¿t rÆ¡i, lÆ°á»£ng nÆ°á»c náº±m giá»¯a vÃ  dÆ°á»i lá»p bÄng',\n",
              " 'uit_001909': 'Sá»± thay Äá»i vá» nhiá»t Äá», lÆ°á»£ng tuyáº¿t rÆ¡i, lÆ°á»£ng nÆ°á»c náº±m giá»¯a vÃ  dÆ°á»i lá»p bÄng',\n",
              " 'uit_001910': 'thu háº¹p ÄÃ¡ng ká», vá»i sá»± lÃ¹i dáº§n máº¡nh cá»§a nhá»¯ng sÃ´ng bÄng trong nhá»¯ng nÄm 1940, cÃ³ Äiá»u kiá»n á»n Äá»nh hoáº·c phÃ¡t triá»n trong nhá»¯ng nÄm 1920 vÃ  1970, vÃ  má»t láº§n ná»¯a báº¯t Äáº§u giáº£m tá»« giá»¯a nhá»¯ng nÄm 1980 Äáº¿n nay',\n",
              " 'uit_001911': 'giáº£m',\n",
              " 'uit_001912': 'khoáº£ng 445.000 km2',\n",
              " 'uit_001913': 'tá»« nhá»¯ng nÄm 1970',\n",
              " 'uit_001914': 'khoáº£ng 240.000 km2',\n",
              " 'uit_001915': 'nhá»¯ng nÄm 1970',\n",
              " 'uit_001916': 'Tá» chá»©c GiÃ¡m sÃ¡t SÃ´ng bÄng Tháº¿ giá»i',\n",
              " 'uit_001917': 'khoáº£ng 445.000 km2',\n",
              " 'uit_001918': 'thay Äá»i quá»¹ Äáº¡o, nhá»¯ng pháº£n á»©ng nhÆ° tÄng hoáº·c giáº£m cÃ¡c dáº£i bÄng lá»¥c Äá»a vÃ  thay Äá»i má»±c nÆ°á»c biá»n táº¡o nÃªn khÃ­ háº­u',\n",
              " 'uit_001919': 'thay Äá»i quá»¹ Äáº¡o',\n",
              " 'uit_001920': 'khoáº£ng 11.700 nÄm',\n",
              " 'uit_001921': 'Dansgaard-Oeschger',\n",
              " 'uit_001922': 'khoáº£ng 11.700 nÄm',\n",
              " 'uit_001923': 'khoáº£ng 3 triá»u nÄm trÆ°á»c',\n",
              " 'uit_001924': 'hiá»n tÆ°á»£ng sa máº¡c hoÃ¡',\n",
              " 'uit_001925': 'nhiá»u loÃ i nhanh chÃ³ng biáº¿n máº¥t',\n",
              " 'uit_001926': 'khÃ­ háº­u',\n",
              " 'uit_001927': 'dáº«n Äáº¿n tÄng trÆ°á»ng thá»±c váº­t ÄÆ°á»£c cáº£i thiá»n vÃ  kÃ©o theo viá»c háº¥p thá»¥ nhiá»u CO2 trong khÃ´ng khÃ­ hÆ¡n',\n",
              " 'uit_001928': 'nhá»¯ng chá» sá» quan trá»ng vá» sá»± thay Äá»i lÆ°á»£ng CO2 qua hÃ ng ngÃ n nÄm, vÃ  tiáº¿p tá»¥c cung cáº¥p nhá»¯ng thÃ´ng tin cÃ³ giÃ¡ trá» vá» sá»± khÃ¡c nhau giá»¯a Äiá»u kiá»n khÃ´ng khÃ­ cá» xÆ°a vÃ  hiá»n Äáº¡i',\n",
              " 'uit_001929': 'CÃ¡c thÃ´ng tin tá»« viá»c phÃ¢n tÃ­ch pháº§n lÃµi bÄng khoan tá»« má»t khá»i bÄng nhÆ° khá»i bÄng Nam Cá»±c',\n",
              " 'uit_001930': 'KhÃ´ng khÃ­ bá» máº¯c káº¹t á» dáº¡ng bong bÃ³ng trong bÄng',\n",
              " 'uit_001931': 'CO2',\n",
              " 'uit_001932': 'KhÃ´ng khÃ­ bá» máº¯c káº¹t á» dáº¡ng bong bÃ³ng trong bÄng',\n",
              " 'uit_001933': 'CÃ¡c thÃ´ng tin tá»« viá»c phÃ¢n tÃ­ch pháº§n lÃµi bÄng khoan tá»« má»t khá»i bÄng nhÆ° khá»i bÄng Nam Cá»±c',\n",
              " 'uit_001934': 'lÃ  bá» mÃ´n khoa há»c hiá»n Äáº¡i nghiÃªn cá»©u vá» lÄ©nh vá»±c hÃ³a tháº¡ch á» kÃ­ch thÆ°á»c táº¿ bÃ o, bao gá»m cáº£ pháº¥n hoa',\n",
              " 'uit_001935': 'Äá» suy ra sá»± phÃ¢n bá» Äá»a lÃ½ cá»§a cÃ¡c loÃ i thá»±c váº­t tá»«ng thay Äá»i theo Äiá»u kiá»n khÃ­ háº­u khÃ¡c nhau',\n",
              " 'uit_001936': 'suy ra sá»± phÃ¢n bá» Äá»a lÃ½ cá»§a cÃ¡c loÃ i thá»±c váº­t tá»«ng thay Äá»i theo Äiá»u kiá»n khÃ­ háº­u khÃ¡c nhau',\n",
              " 'uit_001937': 'lÃ  bá» mÃ´n khoa há»c hiá»n Äáº¡i nghiÃªn cá»©u vá» lÄ©nh vá»±c hÃ³a tháº¡ch á» kÃ­ch thÆ°á»c táº¿ bÃ o, bao gá»m cáº£ pháº¥n hoa',\n",
              " 'uit_001938': 'lá»p ngoÃ i cá»§a pháº¥n hoa ÄÆ°á»£c cáº¥u thÃ nh tá»« má»t lá»p cháº¥t liá»u cÃ³ tÃ­nh ÄÃ n há»i ráº¥t cao',\n",
              " 'uit_001939': 'cÃ¡c thay Äá»i á» tháº¿ giá»i thá»±c váº­t',\n",
              " 'uit_001940': 'lá»p ngoÃ i cá»§a pháº¥n hoa ÄÆ°á»£c cáº¥u thÃ nh tá»« má»t lá»p cháº¥t liá»u cÃ³ tÃ­nh ÄÃ n há»i ráº¥t cao',\n",
              " 'uit_001941': 'cáº¥u trÃºc di truyá»n khÃ´ng thay Äá»i ÄÃ¡ng ká» qua hÃ ng ngÃ n nÄm',\n",
              " 'uit_001942': 'nhá»¯ng vÃ¹ng nÆ°á»c ngá»t vÃ  tráº§m tÃ­ch Äáº¥t Äai',\n",
              " 'uit_001943': 'viá»c nghiÃªn cá»©u dá»±a trÃªn nhá»¯ng loÃ i bá» cÃ¡nh cá»©ng khÃ¡c nhau sáº½ Äem láº¡i kiáº¿n thá»©c vá» pháº¡m vi khÃ­ háº­u hiá»n táº¡i, xÃ¡c Äá»nh ÄÆ°á»£c tuá»i cá»§a cÃ¡c tráº§m tÃ­ch cÃ²n sÃ³t láº¡i, tá»« ÄÃ³ cÃ³ thá» suy ra Äiá»u kiá»n khÃ­ háº­u trong quÃ¡ khá»©',\n",
              " 'uit_001944': 'khÃ´ng',\n",
              " 'uit_001945': 'mÃ¡y Äo Äá» cao - káº¿t há»£p vá»i sá»± Äá»nh vá» chÃ­nh xÃ¡c cá»§a cÃ¡c quá»¹ Äáº¡o vá» tinh',\n",
              " 'uit_001946': 'thÃ´ng qua cÃ¡c dáº¥u váº¿t trÃªn nhá»¯ng ráº·ng san hÃ´, nhá»¯ng lá»p tráº§m tÃ­ch ven biá»n, trÃªn thá»m biá»n, háº¡t trong ÄÃ¡ vÃ´i vÃ  nhá»¯ng di tÃ­ch kháº£o cá» cÃ²n sÃ³t láº¡i gáº§n bá» biá»n',\n",
              " 'uit_001947': 'mÃ¡y Äo thá»§y triá»u',\n",
              " 'uit_001948': 'sá»­ dá»¥ng cÃ¡c mÃ¡y Äo thá»§y triá»u, cÃ¡c sá» liá»u Äo ÄÆ°á»£c Äá»i chiáº¿u trong thá»i gian dÃ i Äá» ÄÆ°a ra má»t má»±c nÆ°á»c trung bÃ¬nh dÃ i háº¡n',\n",
              " 'uit_001949': 'mÃ¡y Äo Äá» cao - káº¿t há»£p vá»i sá»± Äá»nh vá» chÃ­nh xÃ¡c cá»§a cÃ¡c quá»¹ Äáº¡o vá» tinh',\n",
              " 'uit_001950': 'mÃ¡y Äo Äá» cao - káº¿t há»£p vá»i sá»± Äá»nh vá» chÃ­nh xÃ¡c cá»§a cÃ¡c quá»¹ Äáº¡o vá» tinh',\n",
              " 'uit_001951': 'phÆ°Æ¡ng phÃ¡p urani vÃ  cacbon phÃ³ng xáº¡',\n",
              " 'uit_001952': 'sá»­ dá»¥ng cÃ¡c mÃ¡y Äo thá»§y triá»u, cÃ¡c sá» liá»u Äo ÄÆ°á»£c Äá»i chiáº¿u trong thá»i gian dÃ i Äá» ÄÆ°a ra má»t má»±c nÆ°á»c trung bÃ¬nh dÃ i háº¡n',\n",
              " 'uit_001953': 'mÃ¡y Äo thá»§y triá»u',\n",
              " 'uit_001954': 'thÃ´ng qua cÃ¡c dáº¥u váº¿t trÃªn nhá»¯ng ráº·ng san hÃ´, nhá»¯ng lá»p tráº§m tÃ­ch ven biá»n, trÃªn thá»m biá»n, háº¡t trong ÄÃ¡ vÃ´i vÃ  nhá»¯ng di tÃ­ch kháº£o cá» cÃ²n sÃ³t láº¡i gáº§n bá» biá»n',\n",
              " 'uit_001955': 'phÆ°Æ¡ng phÃ¡p urani vÃ  cacbon phÃ³ng xáº¡',\n",
              " 'uit_001956': 'Edward I',\n",
              " 'uit_001957': 'tá»« 1307 cho Äáº¿n khi bá» láº­t Äá» vÃ o thÃ¡ng 1 nÄm 1327',\n",
              " 'uit_001958': 'con gÃ¡i cá»§a vua PhÃ¡p Philippe IV',\n",
              " 'uit_001959': 'Edward II',\n",
              " 'uit_001960': '21 thÃ¡ng 9, 1327',\n",
              " 'uit_001961': 'Alphonso',\n",
              " 'uit_001962': 'giÃ¡i quyáº¿t nhá»¯ng cÄng tháº³ng vÆ°Æ¡ng quyá»n giá»¯a Anh vÃ  PhÃ¡p',\n",
              " 'uit_001963': 'thu há»i sáº¯c lá»nh cáº£i cÃ¡ch vÃ  triá»u há»i sá»§ng tháº§n cá»§a mÃ¬nh',\n",
              " 'uit_001964': 'nÄm 1300',\n",
              " 'uit_001965': 'náº¡n ÄÃ³i lan rá»ng',\n",
              " 'uit_001966': 'lÆ°u ÄÃ y Ã´ng ta',\n",
              " 'uit_001967': 'CÃ¡c nam tÆ°á»c ÄÆ°á»£c trao quyá»n trá»¥c xuáº¥t Gaveston',\n",
              " 'uit_001968': 'má»i quan há» gáº§n thÃ¢n thiáº¿t gÃ¢y nhiá»u tranh cÃ£i vá»i Piers Gaveston',\n",
              " 'uit_001969': 'CÃ¡c nam tÆ°á»c ÄÆ°á»£c trao quyá»n trá»¥c xuáº¥t Gaveston',\n",
              " 'uit_001970': 'Piers Gaveston',\n",
              " 'uit_001971': 'tá»« nÄm 1300',\n",
              " 'uit_001972': 'bá» cÃ¡c hiá»p sÄ© cá»§a vÆ°Æ¡ng triá»u má»i Ã¡m sÃ¡t',\n",
              " 'uit_001973': 'Isabella láº­p liÃªn minh vá»i Roger Mortimer Äang bá» lÆ°u ÄÃ y, vÃ  xÃ¢m lÆ°á»£c Anh báº±ng má»t Äá»i quÃ¢n nhá» nÄm 1326',\n",
              " 'uit_001974': 'Gia ÄÃ¬nh Despenser, Äáº·c biá»t lÃ  Hugh Despenser tráº»',\n",
              " 'uit_001975': 'thÃ¡ng 1 nÄm 1327',\n",
              " 'uit_001976': 'láº­p liÃªn minh vá»i Roger Mortimer Äang bá» lÆ°u ÄÃ y, vÃ  xÃ¢m lÆ°á»£c Anh báº±ng má»t Äá»i quÃ¢n nhá» nÄm 1326',\n",
              " 'uit_001977': 'thÃ¡ng 1 nÄm 1327',\n",
              " 'uit_001978': 'cÃ¡c hiá»p sÄ© cá»§a vÆ°Æ¡ng triá»u má»i Ã¡m sÃ¡t',\n",
              " 'uit_001979': 'thÃ¡ng 1 nÄm 1327',\n",
              " 'uit_001980': 'vá» vua lÆ°á»i nhÃ¡c vÃ  thiáº¿u nÄng lá»±c, hay ÄÆ¡n giáº£n chá» lÃ  má»t nhÃ  cai trá» báº¥t Äáº¯c dÄ© vÃ  hoÃ n toÃ n tháº¥t báº¡i',\n",
              " 'uit_001981': 'nhá»¯ng ngÆ°á»i ÄÆ°Æ¡ng thá»i',\n",
              " 'uit_001982': 'Christopher Marlowe',\n",
              " 'uit_001983': 'sá»± tiáº¿n triá»n triá»n cá»§a cÃ¡c tá» chá»©c Quá»c há»i trong triá»u Äáº¡i cá»§a Ã´ng lÃ  sá»± chá» dáº¥u tÃ­ch cá»±c cho Äáº¥t nÆ°á»c Anh mÃ  trong thá»i gian dÃ i chÆ°a Äáº¡t ÄÆ°á»£c',\n",
              " 'uit_001984': 'Má»i quan há» giá»¯a Edward vÃ  Gaveston',\n",
              " 'uit_001985': 'sá»± chá» dáº¥u tÃ­ch cá»±c cho Äáº¥t nÆ°á»c Anh mÃ  trong thá»i gian dÃ i chÆ°a Äáº¡t ÄÆ°á»£c',\n",
              " 'uit_001986': 'lÆ°á»i nhÃ¡c vÃ  thiáº¿u nÄng lá»±c, hay ÄÆ¡n giáº£n chá» lÃ  má»t nhÃ  cai trá» báº¥t Äáº¯c dÄ© vÃ  hoÃ n toÃ n tháº¥t báº¡i',\n",
              " 'uit_001987': 'má»i quan há» Äá»ng tÃ­nh luyáº¿n Ã¡i Äá»n ÄoÃ¡n cá»§a hai ngÆ°á»i ÄÃ n Ã´ng',\n",
              " 'uit_001988': 'má»t nhÃ  lÃ£nh Äáº¡o quÃ¢n sá»± tÃ i nÄng',\n",
              " 'uit_001989': 'má»t nhÃ  lÃ£nh Äáº¡o quÃ¢n sá»± tÃ i nÄng',\n",
              " 'uit_001990': 'VÆ°Æ¡ng quá»c Castile',\n",
              " 'uit_001991': 'má»t vá» vua ÄÃ¡ng sá»£ vÃ  ÄÃ¡ng kÃ­nh',\n",
              " 'uit_001992': 'Ã´ng thá» hiá»n kháº£ nÄng kiá»m soÃ¡t quyá»n hÃ nh rá»ng lá»n cá»§a cÃ¡c bÃ¡ tÆ°á»c trong hÃ ng ngÅ© giá»i quÃ½ tá»c Anh',\n",
              " 'uit_001993': 'Eleanor xá»© Castile',\n",
              " 'uit_001994': 'tuyÃªn bá» bÃ¡ quyá»n Äá»i vá»i nÆ°á»c nÃ y',\n",
              " 'uit_001995': 'tuyÃªn bá» bÃ¡ quyá»n Äá»i vá»i nÆ°á»c nÃ y',\n",
              " 'uit_001996': 'trÃªn cÆ°Æ¡ng vá» chÆ° háº§u cáº§n tá» lÃ²ng tháº§n phá»¥c há»',\n",
              " 'uit_001997': 'CÃ¡c vua PhÃ¡p nháº¥n máº¡nh ráº±ng quá»c vÆ°Æ¡ng Anh trÃªn cÆ°Æ¡ng vá» chÆ° háº§u cáº§n tá» lÃ²ng tháº§n phá»¥c há», tuy nhiÃªn Ã´ng cho ráº±ng yÃªu cáº§u ÄÃ³ lÃ  má»t sá»± lÄng máº¡ Äáº¿n lÃ²ng kiÃªu hÃ£nh cá»§a nhÃ  vua vÃ  váº¥n Äá» nÃ y váº«n chÆ°a ÄÆ°á»£c giáº£i quyáº¿t triá»t Äá»',\n",
              " 'uit_001998': 'Viá»c Edward cai trá» vÃ¹ng Gascony',\n",
              " 'uit_001999': 'nÄm 1307',\n",
              " 'uit_002000': 'sá»± thá»ng trá» cá»§a Anh á» Scotland',\n",
              " 'uit_002001': 'Viá»c Edward cai trá» vÃ¹ng Gascony',\n",
              " 'uit_002002': 'Ã´ng ÄÃ¡nh thuáº¿ náº·ng vÃ  yÃªu cáº§u cung cáº¥p nhiá»u nguá»n lá»±c phá»¥c vá»¥ chiáº¿n tranh',\n",
              " 'uit_002003': 'Ã´ng ÄÃ¡nh thuáº¿ náº·ng vÃ  yÃªu cáº§u cung cáº¥p nhiá»u nguá»n lá»±c phá»¥c vá»¥ chiáº¿n tranh',\n",
              " 'uit_002004': 'nÄm 1307',\n",
              " 'uit_002005': 'Edward xá»© Caernarfon',\n",
              " 'uit_002006': 'ngÃ y 25 thÃ¡ng 4 nÄm 1284',\n",
              " 'uit_002007': 'LÃ¢u ÄÃ i Caernarfon',\n",
              " 'uit_002008': 'Edward xá»© Caernarfon',\n",
              " 'uit_002009': 'ngÃ y 25 thÃ¡ng 4 nÄm 1284',\n",
              " 'uit_002010': 'LÃ¢u ÄÃ i Caernarfon á» miá»n Báº¯c xá»© Wales',\n",
              " 'uit_002011': 'ngÃ y 25 thÃ¡ng 4 nÄm 1284',\n",
              " 'uit_002012': 'nÃ³ lÃ  má»t Äá»a Äiá»m cÃ³ tÃ­nh biá»u tÆ°á»£ng quan trá»ng Äá»i vá»i ngÆ°á»i Wales báº£n Äá»a, gáº¯n liá»n vá»i lá»ch sá»­ Äáº¿ quá»c La MÃ£, vÃ  lÃ¢u ÄÃ i Caernarfon cÅ©ng lÃ  nÆ¡i Äáº·t vÆ°Æ¡ng quyá»n má»i á» miá»n Báº¯c xá»© Wales',\n",
              " 'uit_002013': 'Eleanor',\n",
              " 'uit_002014': 'Alphonso',\n",
              " 'uit_002015': 'dÃ¹ng tÃªn tiáº¿ng Norman vÃ  Castilla',\n",
              " 'uit_002016': 'Alice de Leygrave',\n",
              " 'uit_002017': 'Alice de Leygrave',\n",
              " 'uit_002018': 'TÃªn Edward cÃ³ xuáº¥t xá»© tá»« tiáº¿ng Anh, liÃªn tÆ°á»ng Äáº¿n ThÃ¡nh ngÆ°á»i Anglo-Saxon lÃ  Edward xÆ°ng tá»i, vÃ  ÄÆ°á»£c lá»±a chá»n bá»i cha Ã´ng thay cho truyá»n thá»ng dÃ¹ng tÃªn tiáº¿ng Norman vÃ  Castilla ÄÃ£ ÄÆ°á»£c Äáº·t cho cÃ¡c anh trai cá»§a Edward',\n",
              " 'uit_002019': 'Mariota hoáº·c Mary Maunsel',\n",
              " 'uit_002020': 'má»¥c sÆ° Giles',\n",
              " 'uit_002021': 'má»¥c sÆ° Giles xá»© Oudenarde',\n",
              " 'uit_002022': 'lÃ m quÃ¢n sÆ° cho riÃªng Ã´ng',\n",
              " 'uit_002023': 'rÃ¨n luyá»n, huáº¥n luyá»n cÆ°á»¡i ngá»±a vÃ  ká»¹ nÄng quÃ¢n sá»± cho Edward',\n",
              " 'uit_002024': 'chi tiÃªu trong tÆ° tháº¥t',\n",
              " 'uit_002025': 'tiáº¿ng PhÃ¡p Anglo-Norman',\n",
              " 'uit_002026': 'cÃ¡c chi tiÃªu trong tÆ° tháº¥t cá»§a Edward',\n",
              " 'uit_002027': 'ná»n giÃ¡o dá»¥c CÃ´ng giÃ¡o tá»« cÃ¡c tu sÄ© dÃ²ng Äa Minh',\n",
              " 'uit_002028': 'William xá»© Blyborough',\n",
              " 'uit_002029': 'Guy Ferre',\n",
              " 'uit_002030': 'má»t tay cÆ°á»¡i ngá»±a giá»i',\n",
              " 'uit_002031': 'nháº¡c Wales vÃ  ÄÃ n crwth má»i vá»«a ÄÆ°á»£c phÃ¡t minh, vÃ  phong cáº§m',\n",
              " 'uit_002032': 'Äáº¥u thÆ°Æ¡ng',\n",
              " 'uit_002033': 'nháº¡c Wales vÃ  ÄÃ n crwth má»i vá»«a ÄÆ°á»£c phÃ¡t minh, vÃ  phong cáº§m',\n",
              " 'uit_002034': 'Ã´ng khÃ´ng cÃ³ nÄng khiáº¿u hoáº·c lÃ  táº¡i Ã´ng khÃ´ng ÄÆ°á»£c phÃ©p tham gia Äá» Äáº£m báº£o an toÃ n',\n",
              " 'uit_002035': 'giÃºp duy trÃ¬ ná»n hÃ²a bÃ¬nh lÃ¢u dÃ i giá»¯a Anh vá»i PhÃ¡p',\n",
              " 'uit_002036': 'NÄm 1290',\n",
              " 'uit_002037': 'Hiá»p Æ°á»c Birgham',\n",
              " 'uit_002038': 'NÄm 1290',\n",
              " 'uit_002039': 'Margaret',\n",
              " 'uit_002040': 'Margaret cá»§a Na Uy',\n",
              " 'uit_002041': 'Hiá»p Æ°á»c Birgham',\n",
              " 'uit_002042': 'NÄm 1290',\n",
              " 'uit_002043': 'Margaret táº¡ tháº¿ cÃ¹ng nÄm',\n",
              " 'uit_002044': 'Margaret táº¡ tháº¿',\n",
              " 'uit_002045': 'giÃºp duy trÃ¬ ná»n hÃ²a bÃ¬nh lÃ¢u dÃ i giá»¯a Anh vá»i PhÃ¡p',\n",
              " 'uit_002046': 'Edward',\n",
              " 'uit_002047': 'Edward',\n",
              " 'uit_002048': 'quan há» tá»t Äáº¹p',\n",
              " 'uit_002049': 'sá»± há» trá»£ vá» tÃ i chÃ­nh vÃ  cÃ¡c chá»©c danh',\n",
              " 'uit_002050': '1301',\n",
              " 'uit_002051': 'Isabella',\n",
              " 'uit_002052': 'NhÃ  vua tham gia chiáº¿n dá»ch Flanders chá»ng láº¡i Philippe IV',\n",
              " 'uit_002053': 'Isabella',\n",
              " 'uit_002054': 'LÃ£nh Äá»a BÃ¡ tÆ°á»c Chester vÃ  nhá»¯ng vÃ¹ng Äáº¥t trÃªn kháº¯p Báº¯c Wales',\n",
              " 'uit_002055': 'LÃ£nh Äá»a BÃ¡ tÆ°á»c Chester vÃ  nhá»¯ng vÃ¹ng Äáº¥t trÃªn kháº¯p Báº¯c Wales',\n",
              " 'uit_002056': 'chá» huy háº­u quÃ¢n trong cuá»c bao vÃ¢y Caerlaverock',\n",
              " 'uit_002057': 'sá»± tháº§n phá»¥c',\n",
              " 'uit_002058': 'sá»± Äá»c láº­p vá» tÃ i chÃ­nh',\n",
              " 'uit_002059': 'chá» huy háº­u quÃ¢n trong cuá»c bao vÃ¢y Caerlaverock',\n",
              " 'uit_002060': 'Ã´ng Äem con trai Äi theo',\n",
              " 'uit_002061': 'Ã´ng Äem con trai Äi theo, khiáº¿n cho Edward trá» thÃ nh chá» huy háº­u quÃ¢n trong cuá»c bao vÃ¢y Caerlaverock',\n",
              " 'uit_002062': 'sá»± Äá»c láº­p vá» tÃ i chÃ­nh',\n",
              " 'uit_002063': 'LÃ£nh Äá»a BÃ¡ tÆ°á»c Chester vÃ  nhá»¯ng vÃ¹ng Äáº¥t trÃªn kháº¯p Báº¯c Wales',\n",
              " 'uit_002064': 'huy Äá»ng má»t lá»±c lÆ°á»£ng quÃ¢n viá»n chinh má»i',\n",
              " 'uit_002065': 'Robert the Bruce',\n",
              " 'uit_002066': 'sá»± trá»«ng pháº¡t, sá»± tráº£ thÃ¹ khá»§ng khiáº¿p',\n",
              " 'uit_002067': 'huy Äá»ng má»t lá»±c lÆ°á»£ng quÃ¢n viá»n chinh má»i',\n",
              " 'uit_002068': 'sá»± trá»«ng pháº¡t, sá»± tráº£ thÃ¹ khá»§ng khiáº¿p',\n",
              " 'uit_002069': 'Robert the Bruce',\n",
              " 'uit_002070': 'ÄÃ m phÃ¡n vá» ngÃ y cÆ°á»i cá»§a Ã´ng vá»i Isabella',\n",
              " 'uit_002071': 'cÃ¡c cuá»c ÄÃ m phÃ¡n vá» ngÃ y cÆ°á»i cá»§a Ã´ng vá»i Isabella tiáº¿p tá»¥c',\n",
              " 'uit_002072': 'con trai Ã´ng',\n",
              " 'uit_002073': 'Piers Gaveston',\n",
              " 'uit_002074': 'hiá»p sÄ© trong gia trang cá»§a NhÃ  vua cÃ³ Äáº¥t phong giÃ¡p vá»i Gascony',\n",
              " 'uit_002075': 'ngÆ°á»i báº¡n thÃ¢n thiáº¿t',\n",
              " 'uit_002076': 'NhÃ  vua ÄÃ¡p láº¡i má»t cÃ¡ch giáº­n dá»¯, ÄÃ¡nh Äáº­p vÃ  giáº­t tÃ³c con trai mÃ¬nh, trÆ°á»c khi ÄÆ°a Gaveston Äi lÆ°u ÄÃ y',\n",
              " 'uit_002077': 'báº¡n thÃ¢n thiáº¿t',\n",
              " 'uit_002078': 'khÃ´ng rÃµ lÃ½ do',\n",
              " 'uit_002079': 'Eleanor de Clare',\n",
              " 'uit_002080': 'lÃªn Ã¡n quyáº¿t liá»t',\n",
              " 'uit_002081': 'Cáº£ Edward vÃ  Gaveston Äá»u cÃ³ quan há» vá»i vá»£ há», vÃ  Äá»u cÃ³ con; Edward cÅ©ng cÃ³ má»t Äá»©a con ngoáº¡i hÃ´n, vÃ  cÃ³ thá» ÄÃ£ cÃ³ cuá»c tÃ¬nh vá»i cÃ´ chÃ¡u gÃ¡i, Eleanor de Clare',\n",
              " 'uit_002082': 'Eleanor de Clare',\n",
              " 'uit_002083': 'ráº¥t phá»©c táº¡p bá»i nhá»¯ng báº±ng chá»©ng xÃ¡c Äá»nh vá» má»i quan há» chi tiáº¿t thá»±c sá»± cÃ²n khÃ¡ Ã­t á»i',\n",
              " 'uit_002084': 'Adam Orleton',\n",
              " 'uit_002085': 'mÃ´ táº£ cÃ¡ch mÃ  Edward \"cáº£m tháº¥y nhÆ° tÃ¬nh yÃªu\" dÃ nh cho Gaveston',\n",
              " 'uit_002086': 'bá» buá»c tá»i',\n",
              " 'uit_002087': 'GiÃ¡m má»¥c Winchester',\n",
              " 'uit_002088': 'Adam Orleton',\n",
              " 'uit_002089': 'Adam Orleton',\n",
              " 'uit_002090': '1320',\n",
              " 'uit_002091': 'cÃ¡ch mÃ  Edward \"cáº£m tháº¥y nhÆ° tÃ¬nh yÃªu\" dÃ nh cho Gaveston, ráº±ng \"Ã´ng dÃ­nh dÃ¡ng vÃ o má»t giao Æ°á»c khÃ´ng thay Äá»i, vÃ  rÃ ng buá»c chÃ­nh Ã´ng vá»i Ã´ng ta trÆ°á»c táº¥t cáº£ nhá»¯ng ngÆ°á»i khÃ¡c vá»i má»t tÃ¬nh yÃªu báº¥t kháº£ phÃ¢n li, vá»¯ng bá»n quyáº¿n rÅ© vÃ  buá»c cháº·t báº±ng má»t má»i rÃ ng\"',\n",
              " 'uit_002092': '1308',\n",
              " 'uit_002093': 'GiÃ¡o hoÃ ng Boniface VIII vÃ  Hiá»p sÄ© Templar',\n",
              " 'uit_002094': 'lÃ½ cho chÃ¡nh trá»',\n",
              " 'uit_002095': 'GiÃ¡o hoÃ ng Boniface VIII vÃ  Hiá»p sÄ© Templar',\n",
              " 'uit_002096': 'cha vÃ  cha vá»£ cá»§a Edward',\n",
              " 'uit_002097': 'cha vÃ  cha vá»£',\n",
              " 'uit_002098': 'má»t pháº§n tá»« lÃ½ cho chÃ¡nh trá»',\n",
              " 'uit_002099': 'nhÆ° anh em, vÃ  nhá»¯ng chÃº thÃ­ch dá»©t khoÃ¡t ráº±ng Edward coi Gaveston lÃ  ngÆ°á»i anh nuÃ´i cá»§a Ã´ng',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Raise\n",
        "import json\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Chuáº©n hÃ³a cÃ¢u tráº£ lá»i: loáº¡i bá» kÃ½ tá»± thá»«a, viáº¿t thÆ°á»ng.\"\"\"\n",
        "    import re\n",
        "    s = s.lower()\n",
        "    # s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)  # Loáº¡i bá» cÃ¡c tá»« khÃ´ng cáº§n thiáº¿t\n",
        "    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)  # Loáº¡i bá» dáº¥u cÃ¢u\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()  # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a\n",
        "    return s\n",
        "\n",
        "def compute_scores(gold_answers, predicted_answers):\n",
        "    exact_matches = 0\n",
        "    total_f1 = 0\n",
        "    n = len(gold_answers)\n",
        "\n",
        "    for id_, gold in gold_answers.items():\n",
        "        pred = predicted_answers.get(id_, \"\")\n",
        "\n",
        "        # # Chuáº©n hÃ³a cÃ¢u tráº£ lá»i\n",
        "        gold = normalize_answer(gold)\n",
        "        pred = normalize_answer(pred)\n",
        "\n",
        "        # Exact Match\n",
        "        if gold == pred:\n",
        "            exact_matches += 1\n",
        "\n",
        "        # F1 Score\n",
        "        gold_tokens = gold.split()\n",
        "        pred_tokens = pred.split()\n",
        "        common_tokens = set(gold_tokens) & set(pred_tokens)\n",
        "        num_common = len(common_tokens)\n",
        "\n",
        "        if num_common == 0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            precision = num_common / len(pred_tokens)\n",
        "            recall = num_common / len(gold_tokens)\n",
        "            f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "        total_f1 += f1\n",
        "\n",
        "    exact_match = exact_matches / n\n",
        "    f1_score_avg = total_f1 / n\n",
        "\n",
        "    return exact_match, f1_score_avg\n",
        "\n",
        "# Load dá»¯ liá»u tá»« file JSON\n",
        "with open(\"/content/ground_truth.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ground_truth = json.load(f)\n",
        "\n",
        "with open(\"/content/predict_predictions.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    predictions = json.load(f)\n",
        "\n",
        "# TÃ­nh toÃ¡n EM vÃ  F1\n",
        "em, f1 = compute_scores(ground_truth, predictions)\n",
        "\n",
        "print(f\"Exact Match: {em * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Gm-Q_xAaTKL",
        "outputId": "639c3b08-7bab-470e-db0b-4210dbd1fb06"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match: 42.48%\n",
            "F1 Score: 64.62%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/transformers/evaluate_v2.py /content/ground_truth.json /content/transformers/results/predictions/predict_predictions.json --out-file eval_results.json"
      ],
      "metadata": {
        "id": "syvc0mChcg7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exact Match : 42.16%\n",
        "\n",
        "# F1 Score : 64.32%\n",
        "\n",
        "# Total : 3712"
      ],
      "metadata": {
        "id": "Jt894chlbr7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import argparse\n",
        "# import collections\n",
        "# import json\n",
        "# import re\n",
        "# import string\n",
        "\n",
        "# def parse_args():\n",
        "#     parser = argparse.ArgumentParser('Evaluation script for custom QA dataset.')\n",
        "#     parser.add_argument('data_file', metavar='data.json', help='Input ground truth JSON file.')\n",
        "#     parser.add_argument('pred_file', metavar='pred.json', help='Model predictions JSON file.')\n",
        "#     parser.add_argument('--out-file', '-o', metavar='eval.json', help='Write accuracy metrics to file (default is stdout).')\n",
        "#     return parser.parse_args()\n",
        "\n",
        "# def normalize_answer(s):\n",
        "#     \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "#     def remove_articles(text):\n",
        "#         regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "#         return re.sub(regex, ' ', text)\n",
        "\n",
        "#     def white_space_fix(text):\n",
        "#         return ' '.join(text.split())\n",
        "\n",
        "#     def remove_punc(text):\n",
        "#         exclude = set(string.punctuation)\n",
        "#         return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "#     def lower(text):\n",
        "#         return text.lower()\n",
        "\n",
        "#     return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "# def compute_exact(a_gold, a_pred):\n",
        "#     return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "# def compute_f1(a_gold, a_pred):\n",
        "#     gold_toks = normalize_answer(a_gold).split()\n",
        "#     pred_toks = normalize_answer(a_pred).split()\n",
        "#     common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
        "#     num_same = sum(common.values())\n",
        "\n",
        "#     if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "#         return int(gold_toks == pred_toks)\n",
        "\n",
        "#     if num_same == 0:\n",
        "#         return 0\n",
        "\n",
        "#     precision = 1.0 * num_same / len(pred_toks)\n",
        "#     recall = 1.0 * num_same / len(gold_toks)\n",
        "#     return (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "# def evaluate(data_file, pred_file):\n",
        "#     with open(data_file, 'r', encoding='utf-8') as f:\n",
        "#         ground_truth = json.load(f)\n",
        "\n",
        "#     with open(pred_file, 'r', encoding='utf-8') as f:\n",
        "#         predictions = json.load(f)\n",
        "\n",
        "#     exact_scores = {}\n",
        "#     f1_scores = {}\n",
        "\n",
        "#     for qid, gold_answer in ground_truth.items():\n",
        "#         if qid not in predictions:\n",
        "#             print(f\"Missing prediction for {qid}\")\n",
        "#             continue\n",
        "\n",
        "#         pred_answer = predictions[qid]\n",
        "#         exact_scores[qid] = compute_exact(gold_answer, pred_answer)\n",
        "#         f1_scores[qid] = compute_f1(gold_answer, pred_answer)\n",
        "\n",
        "#     total = len(ground_truth)\n",
        "#     exact_match = 100.0 * sum(exact_scores.values()) / total\n",
        "#     f1 = 100.0 * sum(f1_scores.values()) / total\n",
        "\n",
        "#     return {\n",
        "#         'exact': exact_match,\n",
        "#         'f1': f1,\n",
        "#         'total': total\n",
        "#     }\n",
        "\n",
        "# def main():\n",
        "#     args = parse_args()\n",
        "#     metrics = evaluate(args.data_file, args.pred_file)\n",
        "\n",
        "#     if args.out_file:\n",
        "#         with open(args.out_file, 'w', encoding='utf-8') as f:\n",
        "#             json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
        "#     else:\n",
        "#         print(json.dumps(metrics, indent=2, ensure_ascii=False))\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     main()\n"
      ],
      "metadata": {
        "id": "OB18NgfkcCZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "twpl-jALDzSf",
        "outputId": "a40bc551-3563-4276-9898-773d23e6011c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    }
  ]
}