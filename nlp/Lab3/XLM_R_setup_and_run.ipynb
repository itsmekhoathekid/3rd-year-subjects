{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awikz7lriwXO",
        "outputId": "3a21224d-c47c-4c80-d691-fccabe0ce3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EGiDy8pwg_Uu-JRKHutu-gfDe0tXbpp2\n",
            "To: /content/ViQuAD2.0.zip\n",
            "100% 14.2M/14.2M [00:00<00:00, 64.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1EGiDy8pwg_Uu-JRKHutu-gfDe0tXbpp2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git\n",
        "# Báº­t sparse-checkout\n",
        "!git clone --depth 1 --filter=blob:none --sparse https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!git sparse-checkout set examples/pytorch/question-answering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gja7Xr_pi2NE",
        "outputId": "45421076-66b7-4f78-ad74-2a0c6e389a61"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 855, done.\u001b[K\n",
            "remote: Counting objects: 100% (855/855), done.\u001b[K\n",
            "remote: Compressing objects: 100% (821/821), done.\u001b[K\n",
            "remote: Total 855 (delta 1), reused 280 (delta 1), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (855/855), 208.06 KiB | 4.16 MiB/s, done.\n",
            "Resolving deltas: 100% (1/1), done.\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 15 (delta 0), reused 3 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (15/15), 55.31 KiB | 3.25 MiB/s, done.\n",
            "/content/transformers\n",
            "remote: Enumerating objects: 10, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 10 (delta 4), reused 4 (delta 3), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (10/10), 37.27 KiB | 6.21 MiB/s, done.\n",
            "Resolving deltas: 100% (4/4), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/ViQuAD2.0.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iut1gaM8i5sR",
        "outputId": "0ebfd707-6830-4037-d053-7e3f5815c8a5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/ViQuAD2.0.zip\n",
            "  inflating: ViQuAD2.0/test/ground_truth_private_test.json  \n",
            "  inflating: ViQuAD2.0/train/train.json  \n",
            "  inflating: ViQuAD2.0/dev/dev.json  \n",
            "  inflating: ViQuAD2.0/test/Private_Test_ref.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/transformers/examples/pytorch/question-answering/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRFlb5Poi5_Y",
        "outputId": "4656075d-e5bc-40ef-8e85-1a33baf406ca"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (1.2.1)\n",
            "Collecting datasets>=1.8.0 (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (2.5.1+cu121)\n",
            "Collecting evaluate (from -r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 4))\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.27.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.12.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.67.1)\n",
            "Collecting xxhash (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2))\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.11.10)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.3.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.8.0->-r /content/transformers/examples/pytorch/question-answering/requirements.txt (line 2)) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 evaluate-0.4.3 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall transformers -y\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AqjBWXYi8Ji",
        "outputId": "07bdb337-0762-47dd-8b48-2bf9ac3a500d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: transformers 4.47.1\n",
            "Uninstalling transformers-4.47.1:\n",
            "  Successfully uninstalled transformers-4.47.1\n",
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-gtxunx_3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-gtxunx_3\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit e5fd865ebae062b7cf03a81b8c6affeb39f30bec\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.27.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.0.dev0) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.0.dev0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.48.0.dev0) (2024.12.14)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.48.0.dev0-py3-none-any.whl size=10329101 sha256=503997c155567193daf58825cdc2b520fdf5db8c16d725d09fa6d3518c36e0a9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xaxg2t74/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "Successfully installed transformers-4.48.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "  --model_name_or_path xlm-roberta-base \\\n",
        "  --train_file /content/transformers/ViQuAD2.0/train/train.json \\\n",
        "  --validation_file /content/transformers/ViQuAD2.0/dev/dev.json \\\n",
        "  --test_file /content/transformers/ViQuAD2.0/test/Private_Test_ref.json \\\n",
        "  --output_dir ./results \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --do_predict \\\n",
        "  --overwrite_output_dir \\\n",
        "  --per_device_train_batch_size 16 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 3 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kh2S8_DTi9Yd",
        "outputId": "ac1705b3-26bb-4764-9ea9-3177a4061a13"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-04 04:28:03.140093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-04 04:28:03.159999: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-04 04:28:03.165737: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-04 04:28:03.179643: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-04 04:28:04.318616: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/04/2025 04:28:06 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "01/04/2025 04:28:06 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./results/runs/Jan04_04-28-06_2e76e5855e86,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=./results,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=16,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./results,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-7d7ac5ea573e91b3\n",
            "01/04/2025 04:28:06 - INFO - datasets.builder - Using custom data configuration default-7d7ac5ea573e91b3\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "01/04/2025 04:28:06 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "01/04/2025 04:28:06 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "01/04/2025 04:28:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "01/04/2025 04:28:06 - INFO - datasets.builder - Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "01/04/2025 04:28:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
            "[INFO|configuration_utils.py:696] 2025-01-04 04:28:06,910 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-04 04:28:06,912 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "tokenizer_config.json: 100% 25.0/25.0 [00:00<00:00, 122kB/s]\n",
            "[INFO|configuration_utils.py:696] 2025-01-04 04:28:07,172 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-04 04:28:07,173 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "sentencepiece.bpe.model: 100% 5.07M/5.07M [00:00<00:00, 11.9MB/s]\n",
            "tokenizer.json: 100% 9.10M/9.10M [00:00<00:00, 48.3MB/s]\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-04 04:28:08,577 >> loading file sentencepiece.bpe.model from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-04 04:28:08,577 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-04 04:28:08,577 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-04 04:28:08,577 >> loading file special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-04 04:28:08,577 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2034] 2025-01-04 04:28:08,577 >> loading file chat_template.jinja from cache at None\n",
            "[INFO|configuration_utils.py:696] 2025-01-04 04:28:08,578 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-04 04:28:08,579 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"xlm-roberta-base\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "model.safetensors: 100% 1.12G/1.12G [00:08<00:00, 137MB/s]\n",
            "[INFO|modeling_utils.py:3893] 2025-01-04 04:28:18,381 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--xlm-roberta-base/snapshots/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/model.safetensors\n",
            "[INFO|modeling_utils.py:4838] 2025-01-04 04:28:18,504 >> Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForQuestionAnswering: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLMRobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[WARNING|modeling_utils.py:4850] 2025-01-04 04:28:18,504 >> Some weights of XLMRobertaForQuestionAnswering were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on train dataset:   0% 0/22765 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-230279636ad95719.arrow\n",
            "01/04/2025 04:28:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-230279636ad95719.arrow\n",
            "Running tokenizer on train dataset: 100% 22765/22765 [00:22<00:00, 1016.45 examples/s]\n",
            "Running tokenizer on validation dataset:   0% 0/5692 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-508b747d4c6676cb.arrow\n",
            "01/04/2025 04:28:42 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-508b747d4c6676cb.arrow\n",
            "Running tokenizer on validation dataset: 100% 5692/5692 [00:06<00:00, 895.35 examples/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/7301 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1ae1e744319723a9.arrow\n",
            "01/04/2025 04:28:48 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7d7ac5ea573e91b3/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1ae1e744319723a9.arrow\n",
            "Running tokenizer on prediction dataset: 100% 7301/7301 [00:08<00:00, 854.33 examples/s]\n",
            "Downloading builder script: 100% 4.53k/4.53k [00:00<00:00, 18.4MB/s]\n",
            "Downloading extra modules: 100% 3.32k/3.32k [00:00<00:00, 12.9MB/s]\n",
            "[INFO|trainer.py:2369] 2025-01-04 04:28:59,551 >> ***** Running training *****\n",
            "[INFO|trainer.py:2370] 2025-01-04 04:28:59,551 >>   Num examples = 24,319\n",
            "[INFO|trainer.py:2371] 2025-01-04 04:28:59,551 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2372] 2025-01-04 04:28:59,552 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:2375] 2025-01-04 04:28:59,552 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:2376] 2025-01-04 04:28:59,552 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2377] 2025-01-04 04:28:59,552 >>   Total optimization steps = 4,560\n",
            "[INFO|trainer.py:2378] 2025-01-04 04:28:59,552 >>   Number of trainable parameters = 277,454,594\n",
            "[INFO|integration_utils.py:811] 2025-01-04 04:28:59,557 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/transformers/wandb/run-20250104_042942-urwr53wa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m./results\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/21522798-uit/huggingface\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ð View run at \u001b[34m\u001b[4mhttps://wandb.ai/21522798-uit/huggingface/runs/urwr53wa\u001b[0m\n",
            "{'loss': 2.588, 'grad_norm': 22.20242691040039, 'learning_rate': 2.6710526315789474e-05, 'epoch': 0.33}\n",
            " 11% 500/4560 [09:51<1:21:03,  1.20s/it][INFO|trainer.py:3911] 2025-01-04 04:39:35,362 >> Saving model checkpoint to ./results/checkpoint-500\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 04:39:35,365 >> Configuration saved in ./results/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 04:39:49,849 >> Model weights saved in ./results/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 04:39:49,851 >> tokenizer config file saved in ./results/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 04:39:49,851 >> Special tokens file saved in ./results/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 1.7389, 'grad_norm': 36.918399810791016, 'learning_rate': 2.3421052631578947e-05, 'epoch': 0.66}\n",
            " 22% 1000/4560 [20:19<1:10:18,  1.18s/it][INFO|trainer.py:3911] 2025-01-04 04:50:03,206 >> Saving model checkpoint to ./results/checkpoint-1000\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 04:50:03,209 >> Configuration saved in ./results/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 04:50:13,546 >> Model weights saved in ./results/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 04:50:13,548 >> tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 04:50:13,548 >> Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 1.604, 'grad_norm': 27.905296325683594, 'learning_rate': 2.013157894736842e-05, 'epoch': 0.99}\n",
            " 33% 1500/4560 [30:40<1:00:29,  1.19s/it][INFO|trainer.py:3911] 2025-01-04 05:00:23,934 >> Saving model checkpoint to ./results/checkpoint-1500\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 05:00:23,936 >> Configuration saved in ./results/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 05:00:30,532 >> Model weights saved in ./results/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 05:00:30,533 >> tokenizer config file saved in ./results/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 05:00:30,534 >> Special tokens file saved in ./results/checkpoint-1500/special_tokens_map.json\n",
            "{'loss': 1.3462, 'grad_norm': 19.71335220336914, 'learning_rate': 1.6842105263157893e-05, 'epoch': 1.32}\n",
            " 44% 2000/4560 [40:56<50:46,  1.19s/it][INFO|trainer.py:3911] 2025-01-04 05:10:40,656 >> Saving model checkpoint to ./results/checkpoint-2000\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 05:10:40,658 >> Configuration saved in ./results/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 05:10:49,926 >> Model weights saved in ./results/checkpoint-2000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 05:10:49,928 >> tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 05:10:49,928 >> Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
            "{'loss': 1.3151, 'grad_norm': 26.555479049682617, 'learning_rate': 1.355263157894737e-05, 'epoch': 1.64}\n",
            " 55% 2500/4560 [51:16<40:59,  1.19s/it][INFO|trainer.py:3911] 2025-01-04 05:20:59,807 >> Saving model checkpoint to ./results/checkpoint-2500\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 05:20:59,809 >> Configuration saved in ./results/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 05:21:08,918 >> Model weights saved in ./results/checkpoint-2500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 05:21:08,920 >> tokenizer config file saved in ./results/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 05:21:08,920 >> Special tokens file saved in ./results/checkpoint-2500/special_tokens_map.json\n",
            "{'loss': 1.2949, 'grad_norm': 23.728483200073242, 'learning_rate': 1.0263157894736843e-05, 'epoch': 1.97}\n",
            " 66% 3000/4560 [1:01:35<30:52,  1.19s/it][INFO|trainer.py:3911] 2025-01-04 05:31:18,981 >> Saving model checkpoint to ./results/checkpoint-3000\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 05:31:18,983 >> Configuration saved in ./results/checkpoint-3000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 05:31:31,521 >> Model weights saved in ./results/checkpoint-3000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 05:31:31,523 >> tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 05:31:31,524 >> Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
            "{'loss': 1.0615, 'grad_norm': 23.944061279296875, 'learning_rate': 6.973684210526316e-06, 'epoch': 2.3}\n",
            " 77% 3500/4560 [1:12:06<21:05,  1.19s/it][INFO|trainer.py:3911] 2025-01-04 05:41:50,375 >> Saving model checkpoint to ./results/checkpoint-3500\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 05:41:50,376 >> Configuration saved in ./results/checkpoint-3500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 05:41:55,680 >> Model weights saved in ./results/checkpoint-3500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 05:41:55,682 >> tokenizer config file saved in ./results/checkpoint-3500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 05:41:55,682 >> Special tokens file saved in ./results/checkpoint-3500/special_tokens_map.json\n",
            "{'loss': 1.0192, 'grad_norm': 22.83578109741211, 'learning_rate': 3.6842105263157892e-06, 'epoch': 2.63}\n",
            " 88% 4000/4560 [1:22:21<11:07,  1.19s/it][INFO|trainer.py:3911] 2025-01-04 05:52:05,017 >> Saving model checkpoint to ./results/checkpoint-4000\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 05:52:05,019 >> Configuration saved in ./results/checkpoint-4000/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 05:52:15,173 >> Model weights saved in ./results/checkpoint-4000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 05:52:15,175 >> tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 05:52:15,175 >> Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
            "{'loss': 1.0062, 'grad_norm': 36.85730743408203, 'learning_rate': 3.9473684210526315e-07, 'epoch': 2.96}\n",
            " 99% 4500/4560 [1:32:39<01:11,  1.19s/it][INFO|trainer.py:3911] 2025-01-04 06:02:23,193 >> Saving model checkpoint to ./results/checkpoint-4500\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 06:02:23,195 >> Configuration saved in ./results/checkpoint-4500/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 06:02:30,166 >> Model weights saved in ./results/checkpoint-4500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 06:02:30,168 >> tokenizer config file saved in ./results/checkpoint-4500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 06:02:30,168 >> Special tokens file saved in ./results/checkpoint-4500/special_tokens_map.json\n",
            "100% 4560/4560 [1:34:12<00:00,  1.13s/it][INFO|trainer.py:3911] 2025-01-04 06:03:56,133 >> Saving model checkpoint to ./results/checkpoint-4560\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 06:03:56,135 >> Configuration saved in ./results/checkpoint-4560/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 06:04:41,914 >> Model weights saved in ./results/checkpoint-4560/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 06:04:41,917 >> tokenizer config file saved in ./results/checkpoint-4560/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 06:04:41,920 >> Special tokens file saved in ./results/checkpoint-4560/special_tokens_map.json\n",
            "[INFO|trainer.py:2643] 2025-01-04 06:05:18,074 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 5778.5232, 'train_samples_per_second': 12.626, 'train_steps_per_second': 0.789, 'train_loss': 1.4349681185002912, 'epoch': 3.0}\n",
            "100% 4560/4560 [1:35:34<00:00,  1.26s/it]\n",
            "[INFO|trainer.py:3911] 2025-01-04 06:05:18,083 >> Saving model checkpoint to ./results\n",
            "[INFO|configuration_utils.py:420] 2025-01-04 06:05:18,086 >> Configuration saved in ./results/config.json\n",
            "[INFO|modeling_utils.py:2977] 2025-01-04 06:05:25,277 >> Model weights saved in ./results/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2489] 2025-01-04 06:05:25,279 >> tokenizer config file saved in ./results/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2498] 2025-01-04 06:05:25,280 >> Special tokens file saved in ./results/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  total_flos               = 13315650GF\n",
            "  train_loss               =      1.435\n",
            "  train_runtime            = 1:36:18.52\n",
            "  train_samples            =      24319\n",
            "  train_samples_per_second =     12.626\n",
            "  train_steps_per_second   =      0.789\n",
            "01/04/2025 06:05:25 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:917] 2025-01-04 06:05:25,356 >> The following columns in the evaluation set don't have a corresponding argument in `XLMRobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `XLMRobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:4227] 2025-01-04 06:05:25,359 >> \n",
            "***** Running Evaluation *****\n",
            "[INFO|trainer.py:4229] 2025-01-04 06:05:25,359 >>   Num examples = 6083\n",
            "[INFO|trainer.py:4232] 2025-01-04 06:05:25,359 >>   Batch size = 8\n",
            "100% 760/761 [02:16<00:00,  5.56it/s]01/04/2025 06:07:48 - INFO - utils_qa - Post-processing 5692 example predictions split into 6083 features.\n",
            "\n",
            "  0% 0/5692 [00:00<?, ?it/s]\u001b[A\n",
            "  0% 18/5692 [00:00<00:32, 175.15it/s]\u001b[A\n",
            "  1% 39/5692 [00:00<00:29, 193.14it/s]\u001b[A\n",
            "  1% 59/5692 [00:00<00:29, 188.50it/s]\u001b[A\n",
            "  1% 78/5692 [00:00<00:30, 182.64it/s]\u001b[A\n",
            "  2% 98/5692 [00:00<00:29, 187.40it/s]\u001b[A\n",
            "  2% 117/5692 [00:00<00:29, 188.11it/s]\u001b[A\n",
            "  2% 137/5692 [00:00<00:29, 191.51it/s]\u001b[A\n",
            "  3% 160/5692 [00:00<00:27, 202.56it/s]\u001b[A\n",
            "  3% 181/5692 [00:00<00:28, 194.40it/s]\u001b[A\n",
            "  4% 201/5692 [00:01<00:28, 190.00it/s]\u001b[A\n",
            "  4% 222/5692 [00:01<00:28, 195.22it/s]\u001b[A\n",
            "  4% 244/5692 [00:01<00:27, 200.01it/s]\u001b[A\n",
            "  5% 265/5692 [00:01<00:27, 197.72it/s]\u001b[A\n",
            "  5% 285/5692 [00:01<00:28, 189.50it/s]\u001b[A\n",
            "  5% 305/5692 [00:01<00:28, 191.29it/s]\u001b[A\n",
            "  6% 327/5692 [00:01<00:27, 197.20it/s]\u001b[A\n",
            "  6% 347/5692 [00:01<00:27, 191.60it/s]\u001b[A\n",
            "  6% 368/5692 [00:01<00:27, 195.88it/s]\u001b[A\n",
            "  7% 388/5692 [00:02<00:27, 193.34it/s]\u001b[A\n",
            "  7% 408/5692 [00:02<00:27, 192.67it/s]\u001b[A\n",
            "  8% 428/5692 [00:02<00:27, 192.41it/s]\u001b[A\n",
            "  8% 448/5692 [00:02<00:27, 191.40it/s]\u001b[A\n",
            "  8% 468/5692 [00:02<00:27, 189.50it/s]\u001b[A\n",
            "  9% 487/5692 [00:02<00:28, 182.03it/s]\u001b[A\n",
            "  9% 506/5692 [00:02<00:28, 180.58it/s]\u001b[A\n",
            "  9% 526/5692 [00:02<00:27, 185.27it/s]\u001b[A\n",
            " 10% 545/5692 [00:02<00:27, 184.61it/s]\u001b[A\n",
            " 10% 564/5692 [00:02<00:28, 179.31it/s]\u001b[A\n",
            " 10% 583/5692 [00:03<00:29, 175.73it/s]\u001b[A\n",
            " 11% 601/5692 [00:03<00:28, 175.78it/s]\u001b[A\n",
            " 11% 622/5692 [00:03<00:27, 183.15it/s]\u001b[A\n",
            " 11% 641/5692 [00:03<00:27, 184.30it/s]\u001b[A\n",
            " 12% 660/5692 [00:03<00:27, 182.31it/s]\u001b[A\n",
            " 12% 681/5692 [00:03<00:26, 186.70it/s]\u001b[A\n",
            " 13% 717/5692 [00:03<00:21, 236.41it/s]\u001b[A\n",
            " 13% 752/5692 [00:03<00:18, 268.49it/s]\u001b[A\n",
            " 14% 784/5692 [00:03<00:17, 283.34it/s]\u001b[A\n",
            " 14% 819/5692 [00:04<00:16, 301.23it/s]\u001b[A\n",
            " 15% 851/5692 [00:04<00:15, 305.85it/s]\u001b[A\n",
            " 15% 882/5692 [00:04<00:16, 296.97it/s]\u001b[A\n",
            " 16% 918/5692 [00:04<00:15, 313.27it/s]\u001b[A\n",
            " 17% 953/5692 [00:04<00:14, 321.81it/s]\u001b[A\n",
            " 17% 987/5692 [00:04<00:14, 327.09it/s]\u001b[A\n",
            " 18% 1024/5692 [00:04<00:13, 338.76it/s]\u001b[A\n",
            " 19% 1062/5692 [00:04<00:13, 348.68it/s]\u001b[A\n",
            " 19% 1099/5692 [00:04<00:12, 353.70it/s]\u001b[A\n",
            " 20% 1135/5692 [00:04<00:13, 350.31it/s]\u001b[A\n",
            " 21% 1173/5692 [00:05<00:12, 356.62it/s]\u001b[A\n",
            " 21% 1209/5692 [00:05<00:12, 352.69it/s]\u001b[A\n",
            " 22% 1245/5692 [00:05<00:12, 344.56it/s]\u001b[A\n",
            " 22% 1280/5692 [00:05<00:12, 343.08it/s]\u001b[A\n",
            " 23% 1317/5692 [00:05<00:12, 349.57it/s]\u001b[A\n",
            " 24% 1354/5692 [00:05<00:12, 354.85it/s]\u001b[A\n",
            " 24% 1391/5692 [00:05<00:12, 358.16it/s]\u001b[A\n",
            " 25% 1427/5692 [00:05<00:12, 346.06it/s]\u001b[A\n",
            " 26% 1464/5692 [00:05<00:11, 352.90it/s]\u001b[A\n",
            " 26% 1500/5692 [00:05<00:11, 353.33it/s]\u001b[A\n",
            " 27% 1536/5692 [00:06<00:12, 335.04it/s]\u001b[A\n",
            " 28% 1571/5692 [00:06<00:12, 337.94it/s]\u001b[A\n",
            " 28% 1606/5692 [00:06<00:11, 341.25it/s]\u001b[A\n",
            " 29% 1642/5692 [00:06<00:11, 344.92it/s]\u001b[A\n",
            " 29% 1677/5692 [00:06<00:11, 337.27it/s]\u001b[A\n",
            " 30% 1711/5692 [00:06<00:11, 334.74it/s]\u001b[A\n",
            " 31% 1746/5692 [00:06<00:11, 338.44it/s]\u001b[A\n",
            " 31% 1781/5692 [00:06<00:11, 335.12it/s]\u001b[A\n",
            " 32% 1815/5692 [00:06<00:12, 302.96it/s]\u001b[A\n",
            " 32% 1846/5692 [00:07<00:13, 293.64it/s]\u001b[A\n",
            " 33% 1879/5692 [00:07<00:12, 302.89it/s]\u001b[A\n",
            " 34% 1910/5692 [00:07<00:12, 301.46it/s]\u001b[A\n",
            " 34% 1944/5692 [00:07<00:12, 311.82it/s]\u001b[A\n",
            " 35% 1978/5692 [00:07<00:11, 317.06it/s]\u001b[A\n",
            " 35% 2010/5692 [00:07<00:11, 313.23it/s]\u001b[A\n",
            " 36% 2045/5692 [00:07<00:11, 323.46it/s]\u001b[A\n",
            " 37% 2080/5692 [00:07<00:10, 328.88it/s]\u001b[A\n",
            " 37% 2113/5692 [00:07<00:12, 297.96it/s]\u001b[A\n",
            " 38% 2144/5692 [00:08<00:12, 294.54it/s]\u001b[A\n",
            " 38% 2179/5692 [00:08<00:11, 307.18it/s]\u001b[A\n",
            " 39% 2214/5692 [00:08<00:11, 315.04it/s]\u001b[A\n",
            " 39% 2247/5692 [00:08<00:10, 318.25it/s]\u001b[A\n",
            " 40% 2283/5692 [00:08<00:10, 326.40it/s]\u001b[A\n",
            " 41% 2317/5692 [00:08<00:10, 328.94it/s]\u001b[A\n",
            " 41% 2353/5692 [00:08<00:10, 333.29it/s]\u001b[A\n",
            " 42% 2387/5692 [00:08<00:10, 330.37it/s]\u001b[A\n",
            " 43% 2421/5692 [00:08<00:09, 329.15it/s]\u001b[A\n",
            " 43% 2454/5692 [00:08<00:09, 326.18it/s]\u001b[A\n",
            " 44% 2489/5692 [00:09<00:09, 331.40it/s]\u001b[A\n",
            " 44% 2525/5692 [00:09<00:09, 337.00it/s]\u001b[A\n",
            " 45% 2560/5692 [00:09<00:09, 339.37it/s]\u001b[A\n",
            " 46% 2596/5692 [00:09<00:09, 343.46it/s]\u001b[A\n",
            " 46% 2631/5692 [00:09<00:08, 341.36it/s]\u001b[A\n",
            " 47% 2667/5692 [00:09<00:08, 339.62it/s]\u001b[A\n",
            " 47% 2703/5692 [00:09<00:08, 345.51it/s]\u001b[A\n",
            " 48% 2740/5692 [00:09<00:08, 351.44it/s]\u001b[A\n",
            " 49% 2776/5692 [00:09<00:08, 350.11it/s]\u001b[A\n",
            " 49% 2813/5692 [00:09<00:08, 352.20it/s]\u001b[A\n",
            " 50% 2849/5692 [00:10<00:08, 345.34it/s]\u001b[A\n",
            " 51% 2885/5692 [00:10<00:08, 347.23it/s]\u001b[A\n",
            " 51% 2920/5692 [00:10<00:08, 337.39it/s]\u001b[A\n",
            " 52% 2954/5692 [00:10<00:08, 335.85it/s]\u001b[A\n",
            " 53% 2989/5692 [00:10<00:07, 338.37it/s]\u001b[A\n",
            " 53% 3023/5692 [00:10<00:07, 337.41it/s]\u001b[A\n",
            " 54% 3061/5692 [00:10<00:07, 349.44it/s]\u001b[A\n",
            " 54% 3096/5692 [00:10<00:07, 338.54it/s]\u001b[A\n",
            " 55% 3132/5692 [00:10<00:07, 343.74it/s]\u001b[A\n",
            " 56% 3169/5692 [00:11<00:07, 350.26it/s]\u001b[A\n",
            " 56% 3205/5692 [00:11<00:07, 348.43it/s]\u001b[A\n",
            " 57% 3240/5692 [00:11<00:07, 347.40it/s]\u001b[A\n",
            " 58% 3275/5692 [00:11<00:07, 332.49it/s]\u001b[A\n",
            " 58% 3310/5692 [00:11<00:07, 334.66it/s]\u001b[A\n",
            " 59% 3346/5692 [00:11<00:06, 340.99it/s]\u001b[A\n",
            " 59% 3381/5692 [00:11<00:06, 331.53it/s]\u001b[A\n",
            " 60% 3415/5692 [00:11<00:06, 330.31it/s]\u001b[A\n",
            " 61% 3449/5692 [00:11<00:06, 331.64it/s]\u001b[A\n",
            " 61% 3483/5692 [00:11<00:06, 319.92it/s]\u001b[A\n",
            " 62% 3516/5692 [00:12<00:06, 321.93it/s]\u001b[A\n",
            " 62% 3553/5692 [00:12<00:06, 335.64it/s]\u001b[A\n",
            " 63% 3590/5692 [00:12<00:06, 342.58it/s]\u001b[A\n",
            " 64% 3625/5692 [00:12<00:06, 328.38it/s]\u001b[A\n",
            " 64% 3659/5692 [00:12<00:06, 328.82it/s]\u001b[A\n",
            " 65% 3694/5692 [00:12<00:05, 333.41it/s]\u001b[A\n",
            " 66% 3730/5692 [00:12<00:05, 338.86it/s]\u001b[A\n",
            " 66% 3767/5692 [00:12<00:05, 345.71it/s]\u001b[A\n",
            " 67% 3802/5692 [00:12<00:05, 344.25it/s]\u001b[A\n",
            " 67% 3837/5692 [00:13<00:05, 336.69it/s]\u001b[A\n",
            " 68% 3875/5692 [00:13<00:05, 349.00it/s]\u001b[A\n",
            " 69% 3912/5692 [00:13<00:05, 353.82it/s]\u001b[A\n",
            " 69% 3948/5692 [00:13<00:04, 350.59it/s]\u001b[A\n",
            " 70% 3984/5692 [00:13<00:05, 341.38it/s]\u001b[A\n",
            " 71% 4020/5692 [00:13<00:04, 346.37it/s]\u001b[A\n",
            " 71% 4055/5692 [00:13<00:05, 313.48it/s]\u001b[A\n",
            " 72% 4087/5692 [00:13<00:05, 275.63it/s]\u001b[A\n",
            " 72% 4116/5692 [00:14<00:06, 246.04it/s]\u001b[A\n",
            " 73% 4142/5692 [00:14<00:06, 230.05it/s]\u001b[A\n",
            " 73% 4166/5692 [00:14<00:07, 212.16it/s]\u001b[A\n",
            " 74% 4188/5692 [00:14<00:07, 205.24it/s]\u001b[A\n",
            " 74% 4209/5692 [00:14<00:07, 195.04it/s]\u001b[A\n",
            " 74% 4229/5692 [00:14<00:07, 189.52it/s]\u001b[A\n",
            " 75% 4250/5692 [00:14<00:07, 192.42it/s]\u001b[A\n",
            " 75% 4272/5692 [00:14<00:07, 199.44it/s]\u001b[A\n",
            " 75% 4293/5692 [00:14<00:06, 202.02it/s]\u001b[A\n",
            " 76% 4314/5692 [00:15<00:06, 202.98it/s]\u001b[A\n",
            " 76% 4335/5692 [00:15<00:06, 200.96it/s]\u001b[A\n",
            " 77% 4356/5692 [00:15<00:06, 198.66it/s]\u001b[A\n",
            " 77% 4376/5692 [00:15<00:07, 184.44it/s]\u001b[A\n",
            " 77% 4396/5692 [00:15<00:06, 187.82it/s]\u001b[A\n",
            " 78% 4416/5692 [00:15<00:06, 189.58it/s]\u001b[A\n",
            " 78% 4436/5692 [00:15<00:06, 186.29it/s]\u001b[A\n",
            " 78% 4455/5692 [00:15<00:06, 186.60it/s]\u001b[A\n",
            " 79% 4476/5692 [00:15<00:06, 188.99it/s]\u001b[A\n",
            " 79% 4496/5692 [00:16<00:06, 190.93it/s]\u001b[A\n",
            " 79% 4517/5692 [00:16<00:06, 195.50it/s]\u001b[A\n",
            " 80% 4537/5692 [00:16<00:06, 186.14it/s]\u001b[A\n",
            " 80% 4558/5692 [00:16<00:05, 192.65it/s]\u001b[A\n",
            " 80% 4578/5692 [00:16<00:05, 194.75it/s]\u001b[A\n",
            " 81% 4598/5692 [00:16<00:05, 186.57it/s]\u001b[A\n",
            " 81% 4618/5692 [00:16<00:05, 188.75it/s]\u001b[A\n",
            " 81% 4638/5692 [00:16<00:05, 190.39it/s]\u001b[A\n",
            " 82% 4658/5692 [00:16<00:05, 184.73it/s]\u001b[A\n",
            " 82% 4677/5692 [00:16<00:05, 178.51it/s]\u001b[A\n",
            " 83% 4697/5692 [00:17<00:05, 182.68it/s]\u001b[A\n",
            " 83% 4716/5692 [00:17<00:05, 182.78it/s]\u001b[A\n",
            " 83% 4735/5692 [00:17<00:05, 181.92it/s]\u001b[A\n",
            " 84% 4755/5692 [00:17<00:05, 184.71it/s]\u001b[A\n",
            " 84% 4775/5692 [00:17<00:04, 187.95it/s]\u001b[A\n",
            " 84% 4794/5692 [00:17<00:04, 180.38it/s]\u001b[A\n",
            " 85% 4813/5692 [00:17<00:04, 180.90it/s]\u001b[A\n",
            " 85% 4832/5692 [00:17<00:04, 173.59it/s]\u001b[A\n",
            " 85% 4858/5692 [00:17<00:04, 197.63it/s]\u001b[A\n",
            " 86% 4895/5692 [00:18<00:03, 245.49it/s]\u001b[A\n",
            " 87% 4930/5692 [00:18<00:02, 274.06it/s]\u001b[A\n",
            " 87% 4967/5692 [00:18<00:02, 301.75it/s]\u001b[A\n",
            " 88% 5001/5692 [00:18<00:02, 309.06it/s]\u001b[A\n",
            " 88% 5037/5692 [00:18<00:02, 322.18it/s]\u001b[A\n",
            " 89% 5075/5692 [00:18<00:01, 337.35it/s]\u001b[A\n",
            " 90% 5109/5692 [00:18<00:01, 330.64it/s]\u001b[A\n",
            " 90% 5146/5692 [00:18<00:01, 339.56it/s]\u001b[A\n",
            " 91% 5183/5692 [00:18<00:01, 344.66it/s]\u001b[A\n",
            " 92% 5219/5692 [00:18<00:01, 346.52it/s]\u001b[A\n",
            " 92% 5254/5692 [00:19<00:01, 344.63it/s]\u001b[A\n",
            " 93% 5289/5692 [00:19<00:01, 344.79it/s]\u001b[A\n",
            " 94% 5324/5692 [00:19<00:01, 341.38it/s]\u001b[A\n",
            " 94% 5359/5692 [00:19<00:01, 332.57it/s]\u001b[A\n",
            " 95% 5393/5692 [00:19<00:00, 334.29it/s]\u001b[A\n",
            " 95% 5431/5692 [00:19<00:00, 346.99it/s]\u001b[A\n",
            " 96% 5466/5692 [00:19<00:00, 334.14it/s]\u001b[A\n",
            " 97% 5505/5692 [00:19<00:00, 348.98it/s]\u001b[A\n",
            " 97% 5542/5692 [00:19<00:00, 352.30it/s]\u001b[A\n",
            " 98% 5579/5692 [00:20<00:00, 355.39it/s]\u001b[A\n",
            " 99% 5615/5692 [00:20<00:00, 351.01it/s]\u001b[A\n",
            " 99% 5651/5692 [00:20<00:00, 346.20it/s]\u001b[A\n",
            "100% 5692/5692 [00:20<00:00, 279.71it/s]\n",
            "01/04/2025 06:08:09 - INFO - utils_qa - Saving predictions to ./results/eval_predictions.json.\n",
            "01/04/2025 06:08:09 - INFO - utils_qa - Saving nbest_preds to ./results/eval_nbest_predictions.json.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 715, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 672, in main\n",
            "    metrics = trainer.evaluate()\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 73, in evaluate\n",
            "    metrics = self.compute_metrics(eval_preds)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 634, in compute_metrics\n",
            "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/evaluate/module.py\", line 467, in compute\n",
            "    output = self._compute(**inputs, **compute_kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/squad.py\", line 110, in _compute\n",
            "    score = compute_score(dataset=dataset, predictions=pred_dict)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 67, in compute_score\n",
            "    exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 52, in metric_max_over_ground_truths\n",
            "    return max(scores_for_ground_truths)\n",
            "ValueError: max() arg is an empty sequence\n",
            "\u001b[1;34mwandb\u001b[0m: ð View run \u001b[33m./results\u001b[0m at: \u001b[34mhttps://wandb.ai/21522798-uit/huggingface/runs/urwr53wa\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250104_042942-urwr53wa/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fC3_oE0p6t0y",
        "outputId": "4680c236-550d-4b48-d9ce-4dd9a6640897"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/transformers/examples/pytorch/question-answering/run_qa.py \\\n",
        "    --model_name_or_path /content/transformers/results/checkpoint-4560 \\\n",
        "    --test_file /content/transformers/ViQuAD2.0/test/Private_Test_ref.json \\\n",
        "    --do_predict \\\n",
        "    --output_dir /content/transformers/results/predictions \\\n",
        "    --max_seq_length 384 \\\n",
        "    --doc_stride 128 \\\n",
        "    --pad_to_max_length \\\n",
        "    --per_device_eval_batch_size 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwnhqOA4kQT0",
        "outputId": "99f1f302-5df3-4b74-bca8-4a836b1074fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-01-04 06:09:10.367767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2025-01-04 06:09:10.387598: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2025-01-04 06:09:10.393574: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-01-04 06:09:10.408390: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-01-04 06:09:11.657033: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "01/04/2025 06:09:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "01/04/2025 06:09:14 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "average_tokens_across_devices=False,\n",
            "batch_eval_metrics=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=True,\n",
            "do_train=False,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_do_concat_batches=True,\n",
            "eval_on_start=False,\n",
            "eval_steps=None,\n",
            "eval_strategy=no,\n",
            "eval_use_gather_object=False,\n",
            "evaluation_strategy=None,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=None,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_for_metrics=[],\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=/content/transformers/results/predictions/runs/Jan04_06-09-14_2e76e5855e86,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=/content/transformers/results/predictions,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=16,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard', 'wandb'],\n",
            "restore_callback_states_from_checkpoint=False,\n",
            "resume_from_checkpoint=None,\n",
            "run_name=/content/transformers/results/predictions,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torch_empty_cache_steps=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_liger_kernel=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Using custom data configuration default-28d194c0c95f7714\n",
            "01/04/2025 06:09:14 - INFO - datasets.builder - Using custom data configuration default-28d194c0c95f7714\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "01/04/2025 06:09:14 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Generating dataset json (/root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "01/04/2025 06:09:14 - INFO - datasets.builder - Generating dataset json (/root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "01/04/2025 06:09:14 - INFO - datasets.builder - Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092...\n",
            "Downloading took 0.0 min\n",
            "01/04/2025 06:09:14 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "01/04/2025 06:09:14 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating test split\n",
            "01/04/2025 06:09:14 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 7301 examples [00:00, 11932.00 examples/s]\n",
            "Unable to verify splits sizes.\n",
            "01/04/2025 06:09:15 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "01/04/2025 06:09:15 - INFO - datasets.builder - Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:694] 2025-01-04 06:09:15,505 >> loading configuration file /content/transformers/results/checkpoint-4560/config.json\n",
            "[INFO|configuration_utils.py:768] 2025-01-04 06:09:15,509 >> Model config XLMRobertaConfig {\n",
            "  \"_name_or_path\": \"/content/transformers/results/checkpoint-4560\",\n",
            "  \"architectures\": [\n",
            "    \"XLMRobertaForQuestionAnswering\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"model_type\": \"xlm-roberta\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.48.0.dev0\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250002\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-04 06:09:15,513 >> loading file sentencepiece.bpe.model\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-04 06:09:15,513 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-04 06:09:15,513 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-04 06:09:15,513 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-04 06:09:15,513 >> loading file tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2032] 2025-01-04 06:09:15,513 >> loading file chat_template.jinja\n",
            "[INFO|modeling_utils.py:3890] 2025-01-04 06:09:16,506 >> loading weights file /content/transformers/results/checkpoint-4560/model.safetensors\n",
            "[INFO|modeling_utils.py:4848] 2025-01-04 06:09:16,571 >> All model checkpoint weights were used when initializing XLMRobertaForQuestionAnswering.\n",
            "\n",
            "[INFO|modeling_utils.py:4856] 2025-01-04 06:09:16,571 >> All the weights of XLMRobertaForQuestionAnswering were initialized from the model checkpoint at /content/transformers/results/checkpoint-4560.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaForQuestionAnswering for predictions without further training.\n",
            "Running tokenizer on prediction dataset:   0% 0/7301 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0c5188847a4644ba.arrow\n",
            "01/04/2025 06:09:17 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/json/default-28d194c0c95f7714/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-0c5188847a4644ba.arrow\n",
            "Running tokenizer on prediction dataset: 100% 7301/7301 [00:07<00:00, 1004.35 examples/s]\n",
            "01/04/2025 06:09:30 - INFO - __main__ - *** Predict ***\n",
            "[INFO|trainer.py:917] 2025-01-04 06:09:30,553 >> The following columns in the test set don't have a corresponding argument in `XLMRobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `XLMRobertaForQuestionAnswering.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:4227] 2025-01-04 06:09:30,558 >> \n",
            "***** Running Prediction *****\n",
            "[INFO|trainer.py:4229] 2025-01-04 06:09:30,558 >>   Num examples = 7695\n",
            "[INFO|trainer.py:4232] 2025-01-04 06:09:30,558 >>   Batch size = 16\n",
            "100% 481/481 [02:43<00:00,  2.99it/s]01/04/2025 06:12:24 - INFO - utils_qa - Post-processing 7301 example predictions split into 7695 features.\n",
            "\n",
            "  0% 0/7301 [00:00<?, ?it/s]\u001b[A\n",
            "  1% 37/7301 [00:00<00:19, 363.99it/s]\u001b[A\n",
            "  1% 77/7301 [00:00<00:18, 384.65it/s]\u001b[A\n",
            "  2% 118/7301 [00:00<00:18, 392.28it/s]\u001b[A\n",
            "  2% 158/7301 [00:00<00:18, 384.62it/s]\u001b[A\n",
            "  3% 197/7301 [00:00<00:25, 274.90it/s]\u001b[A\n",
            "  3% 229/7301 [00:00<00:30, 234.93it/s]\u001b[A\n",
            "  4% 256/7301 [00:01<00:37, 190.25it/s]\u001b[A\n",
            "  4% 294/7301 [00:01<00:30, 228.71it/s]\u001b[A\n",
            "  5% 331/7301 [00:01<00:26, 260.10it/s]\u001b[A\n",
            "  5% 370/7301 [00:01<00:23, 291.79it/s]\u001b[A\n",
            "  6% 409/7301 [00:01<00:21, 316.00it/s]\u001b[A\n",
            "  6% 447/7301 [00:01<00:20, 332.46it/s]\u001b[A\n",
            "  7% 488/7301 [00:01<00:19, 353.27it/s]\u001b[A\n",
            "  7% 530/7301 [00:01<00:18, 371.72it/s]\u001b[A\n",
            "  8% 569/7301 [00:01<00:18, 360.63it/s]\u001b[A\n",
            "  8% 607/7301 [00:01<00:18, 359.39it/s]\u001b[A\n",
            "  9% 644/7301 [00:02<00:19, 347.07it/s]\u001b[A\n",
            "  9% 684/7301 [00:02<00:18, 359.55it/s]\u001b[A\n",
            " 10% 725/7301 [00:02<00:17, 371.50it/s]\u001b[A\n",
            " 10% 764/7301 [00:02<00:17, 374.30it/s]\u001b[A\n",
            " 11% 804/7301 [00:02<00:17, 379.40it/s]\u001b[A\n",
            " 12% 843/7301 [00:02<00:19, 335.97it/s]\u001b[A\n",
            " 12% 879/7301 [00:02<00:18, 340.26it/s]\u001b[A\n",
            " 13% 917/7301 [00:02<00:18, 349.08it/s]\u001b[A\n",
            " 13% 953/7301 [00:02<00:19, 331.76it/s]\u001b[A\n",
            " 14% 987/7301 [00:03<00:19, 319.82it/s]\u001b[A\n",
            " 14% 1027/7301 [00:03<00:18, 339.64it/s]\u001b[A\n",
            " 15% 1062/7301 [00:03<00:18, 339.83it/s]\u001b[A\n",
            " 15% 1098/7301 [00:03<00:17, 345.11it/s]\u001b[A\n",
            " 16% 1133/7301 [00:03<00:18, 338.36it/s]\u001b[A\n",
            " 16% 1168/7301 [00:03<00:18, 332.29it/s]\u001b[A\n",
            " 17% 1205/7301 [00:03<00:17, 341.79it/s]\u001b[A\n",
            " 17% 1243/7301 [00:03<00:17, 351.73it/s]\u001b[A\n",
            " 18% 1283/7301 [00:03<00:16, 364.15it/s]\u001b[A\n",
            " 18% 1320/7301 [00:04<00:16, 352.13it/s]\u001b[A\n",
            " 19% 1357/7301 [00:04<00:16, 356.74it/s]\u001b[A\n",
            " 19% 1398/7301 [00:04<00:15, 372.16it/s]\u001b[A\n",
            " 20% 1438/7301 [00:04<00:15, 377.73it/s]\u001b[A\n",
            " 20% 1476/7301 [00:04<00:16, 356.53it/s]\u001b[A\n",
            " 21% 1513/7301 [00:04<00:16, 358.16it/s]\u001b[A\n",
            " 21% 1550/7301 [00:04<00:18, 309.44it/s]\u001b[A\n",
            " 22% 1583/7301 [00:04<00:20, 273.78it/s]\u001b[A\n",
            " 22% 1612/7301 [00:04<00:22, 256.58it/s]\u001b[A\n",
            " 22% 1639/7301 [00:05<00:23, 236.87it/s]\u001b[A\n",
            " 23% 1664/7301 [00:05<00:24, 230.46it/s]\u001b[A\n",
            " 23% 1688/7301 [00:05<00:24, 226.72it/s]\u001b[A\n",
            " 23% 1711/7301 [00:05<00:25, 216.73it/s]\u001b[A\n",
            " 24% 1733/7301 [00:05<00:25, 214.42it/s]\u001b[A\n",
            " 24% 1757/7301 [00:05<00:25, 219.63it/s]\u001b[A\n",
            " 24% 1780/7301 [00:05<00:25, 219.49it/s]\u001b[A\n",
            " 25% 1804/7301 [00:05<00:24, 224.51it/s]\u001b[A\n",
            " 25% 1827/7301 [00:05<00:24, 222.28it/s]\u001b[A\n",
            " 25% 1850/7301 [00:06<00:25, 212.10it/s]\u001b[A\n",
            " 26% 1872/7301 [00:06<00:25, 212.42it/s]\u001b[A\n",
            " 26% 1896/7301 [00:06<00:24, 217.90it/s]\u001b[A\n",
            " 26% 1918/7301 [00:06<00:24, 215.47it/s]\u001b[A\n",
            " 27% 1940/7301 [00:06<00:25, 213.01it/s]\u001b[A\n",
            " 27% 1962/7301 [00:06<00:25, 210.94it/s]\u001b[A\n",
            " 27% 1984/7301 [00:06<00:25, 206.93it/s]\u001b[A\n",
            " 27% 2005/7301 [00:06<00:25, 204.97it/s]\u001b[A\n",
            " 28% 2026/7301 [00:06<00:26, 202.51it/s]\u001b[A\n",
            " 28% 2047/7301 [00:07<00:26, 201.43it/s]\u001b[A\n",
            " 28% 2068/7301 [00:07<00:26, 195.44it/s]\u001b[A\n",
            " 29% 2089/7301 [00:07<00:26, 198.51it/s]\u001b[A\n",
            " 29% 2111/7301 [00:07<00:25, 201.95it/s]\u001b[A\n",
            " 29% 2132/7301 [00:07<00:25, 202.73it/s]\u001b[A\n",
            " 29% 2153/7301 [00:07<00:25, 202.66it/s]\u001b[A\n",
            " 30% 2174/7301 [00:07<00:25, 204.03it/s]\u001b[A\n",
            " 30% 2195/7301 [00:07<00:24, 205.16it/s]\u001b[A\n",
            " 30% 2216/7301 [00:07<00:25, 199.22it/s]\u001b[A\n",
            " 31% 2236/7301 [00:08<00:25, 198.09it/s]\u001b[A\n",
            " 31% 2272/7301 [00:08<00:20, 244.52it/s]\u001b[A\n",
            " 32% 2303/7301 [00:08<00:18, 263.43it/s]\u001b[A\n",
            " 32% 2341/7301 [00:08<00:16, 295.88it/s]\u001b[A\n",
            " 33% 2378/7301 [00:08<00:15, 316.47it/s]\u001b[A\n",
            " 33% 2414/7301 [00:08<00:14, 327.72it/s]\u001b[A\n",
            " 34% 2451/7301 [00:08<00:14, 338.61it/s]\u001b[A\n",
            " 34% 2487/7301 [00:08<00:14, 343.83it/s]\u001b[A\n",
            " 35% 2525/7301 [00:08<00:13, 351.99it/s]\u001b[A\n",
            " 35% 2561/7301 [00:08<00:13, 354.17it/s]\u001b[A\n",
            " 36% 2597/7301 [00:09<00:13, 337.22it/s]\u001b[A\n",
            " 36% 2631/7301 [00:09<00:14, 323.00it/s]\u001b[A\n",
            " 36% 2664/7301 [00:09<00:14, 323.13it/s]\u001b[A\n",
            " 37% 2703/7301 [00:09<00:13, 340.35it/s]\u001b[A\n",
            " 38% 2741/7301 [00:09<00:13, 350.12it/s]\u001b[A\n",
            " 38% 2777/7301 [00:09<00:14, 321.90it/s]\u001b[A\n",
            " 38% 2810/7301 [00:09<00:13, 322.67it/s]\u001b[A\n",
            " 39% 2843/7301 [00:09<00:14, 315.15it/s]\u001b[A\n",
            " 39% 2881/7301 [00:09<00:13, 333.33it/s]\u001b[A\n",
            " 40% 2915/7301 [00:10<00:13, 332.65it/s]\u001b[A\n",
            " 40% 2951/7301 [00:10<00:12, 339.37it/s]\u001b[A\n",
            " 41% 2990/7301 [00:10<00:12, 352.42it/s]\u001b[A\n",
            " 41% 3026/7301 [00:10<00:12, 342.37it/s]\u001b[A\n",
            " 42% 3061/7301 [00:10<00:12, 333.76it/s]\u001b[A\n",
            " 42% 3098/7301 [00:10<00:12, 341.70it/s]\u001b[A\n",
            " 43% 3134/7301 [00:10<00:12, 345.17it/s]\u001b[A\n",
            " 43% 3169/7301 [00:10<00:12, 337.89it/s]\u001b[A\n",
            " 44% 3203/7301 [00:10<00:12, 338.24it/s]\u001b[A\n",
            " 44% 3240/7301 [00:10<00:11, 347.07it/s]\u001b[A\n",
            " 45% 3275/7301 [00:11<00:11, 340.50it/s]\u001b[A\n",
            " 45% 3316/7301 [00:11<00:11, 355.87it/s]\u001b[A\n",
            " 46% 3352/7301 [00:11<00:11, 335.99it/s]\u001b[A\n",
            " 46% 3386/7301 [00:11<00:12, 322.07it/s]\u001b[A\n",
            " 47% 3424/7301 [00:11<00:11, 336.35it/s]\u001b[A\n",
            " 47% 3458/7301 [00:11<00:11, 336.92it/s]\u001b[A\n",
            " 48% 3498/7301 [00:11<00:10, 352.79it/s]\u001b[A\n",
            " 48% 3534/7301 [00:11<00:10, 346.44it/s]\u001b[A\n",
            " 49% 3569/7301 [00:11<00:10, 346.00it/s]\u001b[A\n",
            " 49% 3605/7301 [00:12<00:10, 347.89it/s]\u001b[A\n",
            " 50% 3640/7301 [00:12<00:10, 341.40it/s]\u001b[A\n",
            " 50% 3677/7301 [00:12<00:10, 349.56it/s]\u001b[A\n",
            " 51% 3713/7301 [00:12<00:10, 344.78it/s]\u001b[A\n",
            " 51% 3748/7301 [00:12<00:10, 332.33it/s]\u001b[A\n",
            " 52% 3783/7301 [00:12<00:10, 335.63it/s]\u001b[A\n",
            " 52% 3822/7301 [00:12<00:09, 350.56it/s]\u001b[A\n",
            " 53% 3858/7301 [00:12<00:11, 293.63it/s]\u001b[A\n",
            " 53% 3889/7301 [00:12<00:11, 297.71it/s]\u001b[A\n",
            " 54% 3921/7301 [00:13<00:11, 300.44it/s]\u001b[A\n",
            " 54% 3956/7301 [00:13<00:10, 312.42it/s]\u001b[A\n",
            " 55% 3988/7301 [00:13<00:10, 312.88it/s]\u001b[A\n",
            " 55% 4023/7301 [00:13<00:10, 321.81it/s]\u001b[A\n",
            " 56% 4059/7301 [00:13<00:09, 332.27it/s]\u001b[A\n",
            " 56% 4097/7301 [00:13<00:09, 345.87it/s]\u001b[A\n",
            " 57% 4135/7301 [00:13<00:08, 355.53it/s]\u001b[A\n",
            " 57% 4173/7301 [00:13<00:08, 361.18it/s]\u001b[A\n",
            " 58% 4210/7301 [00:13<00:08, 349.56it/s]\u001b[A\n",
            " 58% 4249/7301 [00:13<00:08, 359.17it/s]\u001b[A\n",
            " 59% 4287/7301 [00:14<00:08, 363.48it/s]\u001b[A\n",
            " 59% 4326/7301 [00:14<00:08, 369.05it/s]\u001b[A\n",
            " 60% 4363/7301 [00:14<00:09, 304.89it/s]\u001b[A\n",
            " 60% 4398/7301 [00:14<00:09, 315.04it/s]\u001b[A\n",
            " 61% 4437/7301 [00:14<00:08, 334.12it/s]\u001b[A\n",
            " 61% 4476/7301 [00:14<00:08, 348.12it/s]\u001b[A\n",
            " 62% 4515/7301 [00:14<00:07, 357.81it/s]\u001b[A\n",
            " 62% 4553/7301 [00:14<00:07, 363.49it/s]\u001b[A\n",
            " 63% 4593/7301 [00:14<00:07, 372.97it/s]\u001b[A\n",
            " 63% 4631/7301 [00:15<00:07, 373.94it/s]\u001b[A\n",
            " 64% 4671/7301 [00:15<00:06, 379.74it/s]\u001b[A\n",
            " 65% 4711/7301 [00:15<00:06, 383.17it/s]\u001b[A\n",
            " 65% 4751/7301 [00:15<00:06, 386.00it/s]\u001b[A\n",
            " 66% 4790/7301 [00:15<00:06, 379.35it/s]\u001b[A\n",
            " 66% 4831/7301 [00:15<00:06, 386.49it/s]\u001b[A\n",
            " 67% 4870/7301 [00:15<00:06, 382.14it/s]\u001b[A\n",
            " 67% 4909/7301 [00:15<00:06, 379.64it/s]\u001b[A\n",
            " 68% 4948/7301 [00:15<00:06, 380.49it/s]\u001b[A\n",
            " 68% 4987/7301 [00:15<00:06, 374.91it/s]\u001b[A\n",
            " 69% 5025/7301 [00:16<00:06, 355.49it/s]\u001b[A\n",
            " 69% 5063/7301 [00:16<00:06, 360.36it/s]\u001b[A\n",
            " 70% 5100/7301 [00:16<00:06, 362.05it/s]\u001b[A\n",
            " 70% 5142/7301 [00:16<00:05, 376.76it/s]\u001b[A\n",
            " 71% 5180/7301 [00:16<00:05, 366.76it/s]\u001b[A\n",
            " 71% 5217/7301 [00:16<00:05, 367.21it/s]\u001b[A\n",
            " 72% 5254/7301 [00:16<00:05, 367.43it/s]\u001b[A\n",
            " 73% 5295/7301 [00:16<00:05, 378.35it/s]\u001b[A\n",
            " 73% 5333/7301 [00:16<00:05, 378.68it/s]\u001b[A\n",
            " 74% 5373/7301 [00:17<00:05, 384.53it/s]\u001b[A\n",
            " 74% 5412/7301 [00:17<00:04, 384.43it/s]\u001b[A\n",
            " 75% 5451/7301 [00:17<00:04, 378.87it/s]\u001b[A\n",
            " 75% 5489/7301 [00:17<00:04, 375.69it/s]\u001b[A\n",
            " 76% 5529/7301 [00:17<00:04, 381.03it/s]\u001b[A\n",
            " 76% 5568/7301 [00:17<00:04, 368.70it/s]\u001b[A\n",
            " 77% 5606/7301 [00:17<00:04, 369.93it/s]\u001b[A\n",
            " 77% 5644/7301 [00:17<00:04, 368.91it/s]\u001b[A\n",
            " 78% 5681/7301 [00:17<00:04, 366.44it/s]\u001b[A\n",
            " 78% 5718/7301 [00:17<00:04, 343.73it/s]\u001b[A\n",
            " 79% 5753/7301 [00:18<00:04, 327.97it/s]\u001b[A\n",
            " 79% 5787/7301 [00:18<00:05, 282.86it/s]\u001b[A\n",
            " 80% 5817/7301 [00:18<00:05, 257.03it/s]\u001b[A\n",
            " 80% 5844/7301 [00:18<00:06, 240.81it/s]\u001b[A\n",
            " 80% 5869/7301 [00:18<00:06, 232.83it/s]\u001b[A\n",
            " 81% 5893/7301 [00:18<00:06, 226.92it/s]\u001b[A\n",
            " 81% 5916/7301 [00:18<00:06, 225.31it/s]\u001b[A\n",
            " 81% 5939/7301 [00:18<00:06, 224.48it/s]\u001b[A\n",
            " 82% 5962/7301 [00:19<00:06, 210.18it/s]\u001b[A\n",
            " 82% 5985/7301 [00:19<00:06, 213.00it/s]\u001b[A\n",
            " 82% 6008/7301 [00:19<00:05, 216.16it/s]\u001b[A\n",
            " 83% 6030/7301 [00:19<00:05, 216.80it/s]\u001b[A\n",
            " 83% 6053/7301 [00:19<00:05, 218.56it/s]\u001b[A\n",
            " 83% 6075/7301 [00:19<00:05, 213.09it/s]\u001b[A\n",
            " 84% 6097/7301 [00:19<00:05, 213.26it/s]\u001b[A\n",
            " 84% 6120/7301 [00:19<00:05, 215.70it/s]\u001b[A\n",
            " 84% 6142/7301 [00:19<00:05, 206.00it/s]\u001b[A\n",
            " 84% 6163/7301 [00:20<00:05, 203.33it/s]\u001b[A\n",
            " 85% 6184/7301 [00:20<00:05, 204.69it/s]\u001b[A\n",
            " 85% 6205/7301 [00:20<00:05, 203.13it/s]\u001b[A\n",
            " 85% 6226/7301 [00:20<00:05, 201.15it/s]\u001b[A\n",
            " 86% 6247/7301 [00:20<00:05, 203.02it/s]\u001b[A\n",
            " 86% 6269/7301 [00:20<00:04, 207.61it/s]\u001b[A\n",
            " 86% 6290/7301 [00:20<00:05, 191.24it/s]\u001b[A\n",
            " 86% 6311/7301 [00:20<00:05, 194.98it/s]\u001b[A\n",
            " 87% 6332/7301 [00:20<00:04, 198.32it/s]\u001b[A\n",
            " 87% 6352/7301 [00:21<00:04, 196.14it/s]\u001b[A\n",
            " 87% 6372/7301 [00:21<00:04, 196.42it/s]\u001b[A\n",
            " 88% 6392/7301 [00:21<00:04, 185.60it/s]\u001b[A\n",
            " 88% 6413/7301 [00:21<00:04, 190.94it/s]\u001b[A\n",
            " 88% 6435/7301 [00:21<00:04, 198.71it/s]\u001b[A\n",
            " 88% 6458/7301 [00:21<00:04, 206.73it/s]\u001b[A\n",
            " 89% 6494/7301 [00:21<00:03, 249.93it/s]\u001b[A\n",
            " 89% 6527/7301 [00:21<00:02, 270.65it/s]\u001b[A\n",
            " 90% 6560/7301 [00:21<00:02, 287.63it/s]\u001b[A\n",
            " 90% 6596/7301 [00:21<00:02, 307.00it/s]\u001b[A\n",
            " 91% 6633/7301 [00:22<00:02, 325.16it/s]\u001b[A\n",
            " 91% 6671/7301 [00:22<00:01, 340.22it/s]\u001b[A\n",
            " 92% 6709/7301 [00:22<00:01, 351.17it/s]\u001b[A\n",
            " 92% 6749/7301 [00:22<00:01, 363.61it/s]\u001b[A\n",
            " 93% 6786/7301 [00:22<00:01, 355.14it/s]\u001b[A\n",
            " 93% 6822/7301 [00:22<00:01, 343.22it/s]\u001b[A\n",
            " 94% 6860/7301 [00:22<00:01, 352.68it/s]\u001b[A\n",
            " 94% 6896/7301 [00:22<00:01, 331.62it/s]\u001b[A\n",
            " 95% 6932/7301 [00:22<00:01, 339.35it/s]\u001b[A\n",
            " 95% 6968/7301 [00:22<00:00, 343.69it/s]\u001b[A\n",
            " 96% 7003/7301 [00:23<00:00, 325.79it/s]\u001b[A\n",
            " 96% 7036/7301 [00:23<00:00, 316.35it/s]\u001b[A\n",
            " 97% 7072/7301 [00:23<00:00, 328.47it/s]\u001b[A\n",
            " 97% 7106/7301 [00:23<00:00, 311.15it/s]\u001b[A\n",
            " 98% 7141/7301 [00:23<00:00, 318.44it/s]\u001b[A\n",
            " 98% 7176/7301 [00:23<00:00, 323.08it/s]\u001b[A\n",
            " 99% 7209/7301 [00:23<00:00, 299.19it/s]\u001b[A\n",
            " 99% 7242/7301 [00:23<00:00, 306.39it/s]\u001b[A\n",
            "100% 7301/7301 [00:24<00:00, 303.08it/s]\n",
            "01/04/2025 06:12:48 - INFO - utils_qa - Saving predictions to /content/transformers/results/predictions/predict_predictions.json.\n",
            "01/04/2025 06:12:48 - INFO - utils_qa - Saving nbest_preds to /content/transformers/results/predictions/predict_nbest_predictions.json.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 715, in <module>\n",
            "    main()\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 683, in main\n",
            "    results = trainer.predict(predict_dataset, predict_examples)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/trainer_qa.py\", line 130, in predict\n",
            "    metrics = self.compute_metrics(predictions)\n",
            "  File \"/content/transformers/examples/pytorch/question-answering/run_qa.py\", line 634, in compute_metrics\n",
            "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/evaluate/module.py\", line 467, in compute\n",
            "    output = self._compute(**inputs, **compute_kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/squad.py\", line 110, in _compute\n",
            "    score = compute_score(dataset=dataset, predictions=pred_dict)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 67, in compute_score\n",
            "    exact_match += metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\n",
            "  File \"/root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--squad/b4e2dbca455821c7367faa26712f378254b69040ebaab90b64bdeb465e4a304d/compute_score.py\", line 52, in metric_max_over_ground_truths\n",
            "    return max(scores_for_ground_truths)\n",
            "ValueError: max() arg is an empty sequence\n",
            "100% 481/481 [03:19<00:00,  2.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "def get_ground_truth_format(path):\n",
        "  with open(path, 'r') as f:\n",
        "    json_data = json.load(f)\n",
        "  res = {}\n",
        "  for i in range(len(json_data[\"data\"])):\n",
        "    data = json_data[\"data\"][i][\"paragraphs\"]\n",
        "    for qa in data:\n",
        "      for content in qa[\"qas\"]:\n",
        "        if content[\"is_impossible\"] == 0:\n",
        "          res[content[\"id\"]] = content[\"answers\"][-1][\"text\"]\n",
        "        else:\n",
        "          res[content[\"id\"]] = content[\"plausible_answers\"][-1][\"text\"]\n",
        "  with open('/content/ground_truth.json', 'w') as f:\n",
        "    json.dump(res, f)\n",
        "  return res"
      ],
      "metadata": {
        "id": "cYEX63QcsRd6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAChT-zK8kc9",
        "outputId": "1d8a7be1-70e7-4129-ca34-39d61c73fd8a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_ground_truth_format('/content/transformers/ViQuAD2.0/test/ground_truth_private_test.json')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqBs36rLsT33",
        "outputId": "cab482ac-c01f-4e21-f885-8494398bce23"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'uit_000001': 'ThÃ¡i BÃ¬nh DÆ°Æ¡ng',\n",
              " 'uit_000002': 'trong thung lÅ©ng Trung tÃ¢m',\n",
              " 'uit_000003': '411,000 km2 (160,000 mi2)',\n",
              " 'uit_000004': 'Báº¯c Fork',\n",
              " 'uit_000005': 'California',\n",
              " 'uit_000006': 'tiá»u bang Baja California',\n",
              " 'uit_000007': 'trong thung lÅ©ng Trung tÃ¢m',\n",
              " 'uit_000008': '411,000 km2',\n",
              " 'uit_000009': 'tiá»u bang Baja California',\n",
              " 'uit_000010': 'sÃ¡t hay gáº§n bá» biá»n ThÃ¡i BÃ¬nh DÆ°Æ¡ng',\n",
              " 'uit_000011': 'dÃ£y nÃºi ÄÃ¡ granit Sierra Nevada',\n",
              " 'uit_000012': 'vá»«a lÃ  cá»­a sÃ´ng quan trá»ng há» trá»£ há» sinh thÃ¡i nÆ°á»c máº·n vÃ  vá»«a lÃ  nguá»n nÆ°á»c chá»§ yáº¿u cá»§a pháº§n lá»n dÃ¢n cÆ° tiá»u bang',\n",
              " 'uit_000013': 'dÃ£y nÃºi ÄÃ¡ granit Sierra Nevada',\n",
              " 'uit_000014': 'vá»«a lÃ  cá»­a sÃ´ng quan trá»ng há» trá»£ há» sinh thÃ¡i nÆ°á»c máº·n vÃ  vá»«a lÃ  nguá»n nÆ°á»c chá»§ yáº¿u cá»§a pháº§n lá»n dÃ¢n cÆ° tiá»u bang',\n",
              " 'uit_000015': 'Tehachapi',\n",
              " 'uit_000016': 'viá»c trá»ng trá»t bá» tÃ n phÃ¡',\n",
              " 'uit_000017': 'nhiá»t Äá» tháº¥p gáº§n Äiá»m ÄÃ´ng trong mÃ¹a ÄÃ´ng',\n",
              " 'uit_000018': 'vÃ i con sÃ´ng ÄÃ£ Äá»§ rá»ng vÃ  sÃ¢u Äá» cho vÃ i thÃ nh phá» ná»i Äá»a (nháº¥t lÃ  Stockton) ÄÆ°á»£c trá» thÃ nh háº£i cáº£ng',\n",
              " 'uit_000019': 'há» trá»£ há» thá»ng nÆ°á»c cá»§a má»t sá» thÃ nh phá», nhÆ°ng chá»§ yáº¿u cung cáº¥p cho viá»c tÆ°á»i tiÃªu nÃ´ng nghiá»p',\n",
              " 'uit_000020': '4,421 mÃ©t',\n",
              " 'uit_000021': '4,421 mÃ©t (14,505 feet)',\n",
              " 'uit_000022': 'chim biá»n',\n",
              " 'uit_000023': 'Clear',\n",
              " 'uit_000024': 'há» Clear',\n",
              " 'uit_000025': 'cá»±c nam',\n",
              " 'uit_000026': 'há» Tahoe',\n",
              " 'uit_000027': 'cá»±c nam',\n",
              " 'uit_000028': 'phÃ­a tÃ¢y báº¯c tiá»u bang vÃ  triá»n phÃ­a tÃ¢y dÃ£y Sierra Nevada',\n",
              " 'uit_000029': 'khoáº£ng 35%',\n",
              " 'uit_000030': 'phÃ­a tÃ¢y báº¯c tiá»u bang vÃ  triá»n phÃ­a tÃ¢y dÃ£y Sierra Nevada',\n",
              " 'uit_000031': 'Alaska',\n",
              " 'uit_000032': 'dá»c theo nhá»¯ng dÃ£y nÃºi California gáº§n bá» biá»n hÆ¡n, vÃ  cáº£ nhá»¯ng Äá»i tháº¥p dÆ°á»i chÃ¢n dÃ£y Sierra Nevada',\n",
              " 'uit_000033': 'thÃ´ng',\n",
              " 'uit_000034': 'Alaska',\n",
              " 'uit_000035': '35%',\n",
              " 'uit_000036': 'dá»c theo nhá»¯ng dÃ£y nÃºi California gáº§n bá» biá»n hÆ¡n, vÃ  cáº£ nhá»¯ng Äá»i tháº¥p dÆ°á»i chÃ¢n dÃ£y Sierra Nevada',\n",
              " 'uit_000037': 'biá»n Salton',\n",
              " 'uit_000038': 'ráº¥t cao',\n",
              " 'uit_000039': 'ÄÃ´ng nam',\n",
              " 'uit_000040': '25%',\n",
              " 'uit_000041': 'lÃ  thung lÅ©ng Cháº¿t, lÃ  nÆ¡i cÃ³ Badwater Flat â Äiá»m tháº¥p nháº¥t vÃ  nÃ³ng nháº¥t cá»§a Báº¯c Má»¹',\n",
              " 'uit_000042': 'lÃ  nÆ¡i cÃ³ Badwater Flat â Äiá»m tháº¥p nháº¥t vÃ  nÃ³ng nháº¥t cá»§a Báº¯c Má»¹',\n",
              " 'uit_000043': 'Ã­t hÆ¡n 322 km (200 dáº·m)',\n",
              " 'uit_000044': 'biá»n Salton',\n",
              " 'uit_000045': '25%',\n",
              " 'uit_000046': 'Viá»c buÃ´n bÃ¡n, hÃ´n nhÃ¢n khÃ¡c dÃ¢n tá»c, vÃ  liÃªn minh quÃ¢n sá»±',\n",
              " 'uit_000047': 'Viá»c buÃ´n bÃ¡n, hÃ´n nhÃ¢n khÃ¡c dÃ¢n tá»c, vÃ  liÃªn minh quÃ¢n sá»±',\n",
              " 'uit_000048': 'nhÃ³m, bá» láº¡c, tiá»u bá» láº¡c, vÃ  cÃ¡c cá»ng Äá»ng lá»n hÆ¡n trÃªn bá» biá»n dá»i dÃ o tÃ i nguyÃªn',\n",
              " 'uit_000049': 'Äi sÄn thÃº rá»«ng vÃ  hÃ¡i lÆ°á»£m nhá»¯ng quáº£ háº¡ch, quáº£ Äáº§u, vÃ  quáº£ má»ng',\n",
              " 'uit_000050': 'má»t trong nhá»¯ng vÃ¹ng Äa dáº¡ng vá» vÄn hÃ³a vÃ  ngÃ´n ngá»¯ nháº¥t á» Báº¯c Má»¹ thá»i thá» dÃ¢n',\n",
              " 'uit_000051': 'sÄn nhá»¯ng con thÃº biá»n, cÃ¢u cÃ¡ há»i vÃ  thu nháº·t tÃ´m cua',\n",
              " 'uit_000052': 'má»t trong nhá»¯ng vÃ¹ng Äa dáº¡ng vá» vÄn hÃ³a vÃ  ngÃ´n ngá»¯ nháº¥t á» Báº¯c Má»¹ thá»i thá» dÃ¢n',\n",
              " 'uit_000053': 'Äi sÄn thÃº rá»«ng vÃ  hÃ¡i lÆ°á»£m nhá»¯ng quáº£ háº¡ch, quáº£ Äáº§u, vÃ  quáº£ má»ng',\n",
              " 'uit_000054': 'sÄn thÃº rá»«ng vÃ  hÃ¡i lÆ°á»£m nhá»¯ng quáº£ háº¡ch, quáº£ Äáº§u, vÃ  quáº£ má»ng',\n",
              " 'uit_000055': 'theo hÃ²n Äáº£o láº¡c viÃªn California trong Las sergas de EsplandiÃ¡n (CÃ¡c truyá»n phiÃªu lÆ°u cá»§a Splandian)',\n",
              " 'uit_000056': 'California',\n",
              " 'uit_000057': 'vÃ¹ng tÃ¢y báº¯c cá»§a Äáº¿ quá»c TÃ¢y Ban Nha, tá»©c lÃ  bÃ¡n Äáº£o Baja California (Háº¡ California)',\n",
              " 'uit_000058': 'Alta California (ThÆ°á»£ng California)',\n",
              " 'uit_000059': 'Alta California',\n",
              " 'uit_000060': 'nhá»¯ng ranh giá»i cá»§a biá»n Cortez vÃ  bá» biá»n ThÃ¡i BÃ¬nh DÆ°Æ¡ng chÆ°a ÄÆ°á»£c thÃ¡m hiá»m Äáº§y Äá»§',\n",
              " 'uit_000061': 'nhá»¯ng ranh giá»i cá»§a biá»n Cortez vÃ  bá» biá»n ThÃ¡i BÃ¬nh DÆ°Æ¡ng chÆ°a ÄÆ°á»£c thÃ¡m hiá»m Äáº§y Äá»§',\n",
              " 'uit_000062': 'Garci RodrÃ­guez de Montalvo',\n",
              " 'uit_000063': 'theo hÃ²n Äáº£o láº¡c viÃªn California trong Las sergas de EsplandiÃ¡n',\n",
              " 'uit_000064': '1821',\n",
              " 'uit_000065': 'cuá»i tháº¿ ká»· 18',\n",
              " 'uit_000066': 'CÃ¡c tráº¡i ráº¥t lá»n nuÃ´i bÃ²',\n",
              " 'uit_000067': 'ngÆ°á»i thá» dÃ¢n',\n",
              " 'uit_000068': 'CÃ¡c tráº¡i ráº¥t lá»n nuÃ´i bÃ²',\n",
              " 'uit_000069': 'Mexico giÃ nh ÄÆ°á»£c Äá»c láº­p trong cuá»c Chiáº¿n tranh Äá»c láº­p Mexico',\n",
              " 'uit_000070': 'khi Mexico giÃ nh ÄÆ°á»£c Äá»c láº­p trong cuá»c Chiáº¿n tranh Äá»c láº­p Mexico (1810â1821)',\n",
              " 'uit_000071': 'CÃ¡c thÆ°Æ¡ng gia vÃ  thá»±c dÃ¢n báº¯t Äáº§u Äáº¿n tá»« Hoa Ká»³',\n",
              " 'uit_000072': 'muá»i vÃ  bá» chÃ©t',\n",
              " 'uit_000073': 'khÃ´ng cÃ³ nhiá»u ngÆ°á»i sinh sá»ng',\n",
              " 'uit_000074': 'xÃ¢y dá»±ng má»t sá» phÃ¡o ÄÃ i (presidio)',\n",
              " 'uit_000075': 'sá»t vÃ ng, sá»t rÃ©t, vÃ  dá»ch háº¡ch gÃ¢y ra bá»i muá»i vÃ  bá» chÃ©t',\n",
              " 'uit_000076': 'bÃ¹ng ná» cÃ¡c bá»nh sá»t vÃ ng, sá»t rÃ©t, vÃ  dá»ch háº¡ch gÃ¢y ra bá»i muá»i vÃ  bá» chÃ©t',\n",
              " 'uit_000077': 'do Sa hoÃ ng khÃ´ng quan tÃ¢m',\n",
              " 'uit_000078': 'phÃ­a nam',\n",
              " 'uit_000079': 'Baja California vÃ  Baja California Sur',\n",
              " 'uit_000080': 'má»t con gáº¥u vÃ ng vÃ  má»t ngÃ´i sao',\n",
              " 'uit_000081': 'Baja California vÃ  Baja California Sur',\n",
              " 'uit_000082': 'Thiáº¿u tÆ°á»ng John D. Sloat cá»§a Háº£i quÃ¢n Hoa Ká»³ tiáº¿n vÃ o vá»nh San Francisco vÃ  tuyÃªn bá» chá»§ quyá»n cá»§a Hoa Ká»³ Äá»i vá»i California',\n",
              " 'uit_000083': '1846â1848',\n",
              " 'uit_000084': 'Alta California',\n",
              " 'uit_000085': 'Thiáº¿u tÆ°á»ng John D. Sloat cá»§a Háº£i quÃ¢n Hoa Ká»³ tiáº¿n vÃ o vá»nh San Francisco vÃ  tuyÃªn bá» chá»§ quyá»n cá»§a Hoa Ká»³ Äá»i vá»i California',\n",
              " 'uit_000086': 'Alta California',\n",
              " 'uit_000087': 'ÄÆ°á»ng xe lá»­a xuyÃªn lá»¥c Äá»a Äáº§u tiÃªn ÄÆ°á»£c hoÃ n thÃ nh',\n",
              " 'uit_000088': 'náº¿u tÆ°á»i Äáº¥t vÃ o nhá»¯ng thÃ¡ng hÃ¨ khÃ´ cáº¡n, Äáº¥t ÄÃ³ ráº¥t há»£p Äá» trá»ng cÃ¢y Än quáº£ vÃ  lÃ m nÃ´ng nghiá»p',\n",
              " 'uit_000089': 'Äi theo cÃ¡c chuyáº¿n ÄÆ°á»ng biá»n dÃ i hoáº·c Äi báº±ng xe ngá»±a hay Äi bá» ráº¥t khÃ³ khÄn trÃªn nhá»¯ng con ÄÆ°á»ng Äáº¥t',\n",
              " 'uit_000090': 'cÃ¢y Än quáº£ vÃ  lÃ m nÃ´ng nghiá»p nÃ³i chung',\n",
              " 'uit_000091': 'cam quÃ½t',\n",
              " 'uit_000092': 'viá»c Äi láº¡i láº¡i giá»¯a miá»n TÃ¢y vÃ  cÃ¡c trung tÃ¢m á» miá»n ÄÃ´ng tá»n thÃ¬ giá» vÃ  nguy hiá»m',\n",
              " 'uit_000093': 'tá»n thÃ¬ giá» vÃ  nguy hiá»m',\n",
              " 'uit_000094': 'CÃ¡c loáº¡i cÃ¢y giá»ng cam quÃ½t',\n",
              " 'uit_000095': 'má»t trong nhá»¯ng Äá»a Äiá»m cÃ³ nhiá»u loáº¡i ngÆ°á»i nháº¥t trÃªn tháº¿ giá»i',\n",
              " 'uit_000096': 'California',\n",
              " 'uit_000097': 'California',\n",
              " 'uit_000098': 'ÄÆ°á»ng Lincoln vÃ  Xa lá» 66',\n",
              " 'uit_000099': 'hoÃ n thÃ nh nhá»¯ng con ÄÆ°á»ng xuyÃªn lá»¥c Äá»a lá»n',\n",
              " 'uit_000100': 'sá»± di trÃº Äáº¿n California tÄng nhanh',\n",
              " 'uit_000101': 'ká»¹ thuáº­t vÃ  vÄn hÃ³a, vÃ  lÃ  trung tÃ¢m quá»c táº¿ vá» cÃ´ng ty ká»¹ thuáº­t, ngÃ nh cÃ´ng nghiá»p Äiá»n áº£nh vÃ  truyá»n hÃ¬nh, cÃ´ng nghiá»p Ã¢m nháº¡c',\n",
              " 'uit_000102': 'gáº§n má»t triá»u',\n",
              " 'uit_000103': 'gáº§n má»t triá»u',\n",
              " 'uit_000104': 'California trá» thÃ nh má»t trong nhá»¯ng Äá»a Äiá»m cÃ³ nhiá»u loáº¡i ngÆ°á»i nháº¥t trÃªn tháº¿ giá»i',\n",
              " 'uit_000105': '751.419 ngÆ°á»i',\n",
              " 'uit_000106': 'khoáº£ng 36.132.147 ngÆ°á»i',\n",
              " 'uit_000107': 'thá»© 13',\n",
              " 'uit_000108': 'khoáº£ng 36.132.147 ngÆ°á»i',\n",
              " 'uit_000109': '4 triá»u dÃ¢n',\n",
              " 'uit_000110': '4 triá»u',\n",
              " 'uit_000111': '6,7%',\n",
              " 'uit_000112': 'thá»© 13',\n",
              " 'uit_000113': 'California',\n",
              " 'uit_000114': 'mÃ¡y tÃ­nh vÃ  cÃ´ng nghá» cao',\n",
              " 'uit_000115': '13%',\n",
              " 'uit_000116': 'California',\n",
              " 'uit_000117': 'thá»© sÃ¡u',\n",
              " 'uit_000118': 'thá»© sÃ¡u',\n",
              " 'uit_000119': 'vá» mÃ¡y tÃ­nh vÃ  cÃ´ng nghá» cao',\n",
              " 'uit_000120': 'nÃ´ng nghiá»p, hÃ ng khÃ´ng vÅ© trá»¥, giáº£i trÃ­, cÃ´ng nghiá»p nháº¹, vÃ  du lá»ch',\n",
              " 'uit_000121': 'nÃ´ng nghiá»p, hÃ ng khÃ´ng vÅ© trá»¥, giáº£i trÃ­, cÃ´ng nghiá»p nháº¹, vÃ  du lá»ch',\n",
              " 'uit_000122': 'Hollywood',\n",
              " 'uit_000123': 'Hollywood',\n",
              " 'uit_000124': 'nhá»¯ng trÆ°á»ng trung há»c cÆ¡ sá» cÃ³ lá»p tÃ¹y chá»n vá»i chÆ°Æ¡ng trÃ¬nh táº­p trung vÃ o cÃ¡ch há»c',\n",
              " 'uit_000125': 'khoáº£ng 14â18 tuá»',\n",
              " 'uit_000126': '16 tuá»i',\n",
              " 'uit_000127': '6 tuá»i',\n",
              " 'uit_000128': 'cÃ¡ch há»c, lá»ch sá»­, vÃ  xÃ£ há»i',\n",
              " 'uit_000129': 'nghá» nghiá»p, ngÃ´n ngá»¯, vÃ  khoa há»c nhÃ¢n vÄn',\n",
              " 'uit_000130': 'khoáº£ng 14â18 tuá»i',\n",
              " 'uit_000131': 'nhá»¯ng lá»p tÃ¹y chá»n vá» nghá» nghiá»p, ngÃ´n ngá»¯',\n",
              " 'uit_000132': 'nhá»¯ng trÆ°á»ng trung há»c cÆ¡ sá»',\n",
              " 'uit_000133': 'Bá» NÄng lÆ°á»£ng Hoa Ká»³',\n",
              " 'uit_000134': 'há» thá»ng Viá»n Äáº¡i há»c California (UC)',\n",
              " 'uit_000135': 'nháº­n 12,5% cá»§a nhá»¯ng há»c sinh cao Äiá»m nháº¥t vÃ  thá»±c hiá»n nghiÃªn cá»©u sau Äáº¡i há»c',\n",
              " 'uit_000136': 'sinh cao Äiá»m nháº¥t vÃ  thá»±c hiá»n nghiÃªn cá»©u sau Äáº¡i há»c',\n",
              " 'uit_000137': 'má»t trong nhá»¯ng há» thá»ng viá»n Äáº¡i há»c cÃ´ng láº­p hÃ ng Äáº§u cá»§a Hoa Ká»³',\n",
              " 'uit_000138': 'sinh viÃªn sau Äáº¡i há»c ngÃ nh y',\n",
              " 'uit_000139': 'PhÃ²ng thÃ­ nghiá»m Quá»c gia Lawrence táº¡i Livermore, PhÃ²ng thÃ­ nghiá»m Quá»c gia Lawrence táº¡i Berkeley, vÃ  PhÃ²ng thÃ­ nghiá»m Quá»c gia Los Alamos',\n",
              " 'uit_000140': 'nhá»¯ng sinh viÃªn sau Äáº¡i há»c ngÃ nh y',\n",
              " 'uit_000141': 'ÄÆ°á»£c coi nhÆ° má»t trong nhá»¯ng há» thá»ng viá»n Äáº¡i há»c cÃ´ng láº­p hÃ ng Äáº§u cá»§a Hoa Ká»³',\n",
              " 'uit_000142': 'hÆ¡n 400.000 sinh viÃªn',\n",
              " 'uit_000143': 'CÃ¡n bá» ThÆ° viá»n Tiá»u bang Kevin Star',\n",
              " 'uit_000144': 'pháº§n ba há»c sinh trung há»c phá» thÃ´ng cao Äiá»m nháº¥t',\n",
              " 'uit_000145': 'Äáº¡i há»c',\n",
              " 'uit_000146': 'Äáº¡i há»c',\n",
              " 'uit_000147': 'nháº­n pháº§n ba há»c sinh trung há»c phá» thÃ´ng cao Äiá»m nháº¥t',\n",
              " 'uit_000148': 'CSU-Long Beach, CSU-Fresno, San Diego State University, vÃ  San Jose State University',\n",
              " 'uit_000149': 'khoa há»c á»©ng dá»¥ng',\n",
              " 'uit_000150': 'khoa há»c á»©ng dá»¥ng',\n",
              " 'uit_000151': 'hÆ¡n 400.000',\n",
              " 'uit_000152': 'khoáº£ng 4,6 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000153': 'tinh vÃ¢n Máº·t Trá»i',\n",
              " 'uit_000154': 'khoáº£ng 150 triá»u kilÃ´mÃ©t',\n",
              " 'uit_000155': 'Khi Máº·t Trá»i ngÃ y cÃ ng Äáº·c láº¡i, nÃ³ nÃ³ng lÃªn, pháº£n á»©ng háº¡t nhÃ¢n bÃ¹ng ná» vÃ  táº¡o nÃªn giÃ³ Máº·t Trá»i',\n",
              " 'uit_000156': 'tinh vÃ¢n Máº·t Trá»i',\n",
              " 'uit_000157': 'khoáº£ng 4,6 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000158': 'lá»±c háº¥p dáº«n vÃ  quÃ¡n tÃ­nh',\n",
              " 'uit_000159': '150 triá»u kilÃ´mÃ©t',\n",
              " 'uit_000160': 'lá»±c háº¥p dáº«n vÃ  quÃ¡n tÃ­nh',\n",
              " 'uit_000161': 'tinh vÃ¢n Máº·t Trá»i',\n",
              " 'uit_000162': 'Sao Hoáº£',\n",
              " 'uit_000163': '23,5Â°',\n",
              " 'uit_000164': 'Theia',\n",
              " 'uit_000165': 'khoáº£ng 4.533 tá»· nÄm (cÃ³ láº½ 0 giá» 05 phÃºt ÄÃªm theo giá» cÃ¡i Äá»ng há» cá»§a chÃºng ta)',\n",
              " 'uit_000166': '23,5Â°',\n",
              " 'uit_000167': '150 triá»u km',\n",
              " 'uit_000168': 'Theia',\n",
              " 'uit_000169': 'Theia',\n",
              " 'uit_000170': 'Sao Hoáº£',\n",
              " 'uit_000171': 'amoniac, mÃªtan, hÆ¡i nÆ°á»c, cacbon ÄiÃ´xÃ­t, vÃ  nitÆ¡, cÅ©ng nhÆ° má»t lÆ°á»£ng nhá» cÃ¡c cháº¥t khÃ­',\n",
              " 'uit_000172': 'HÆ¡i nÆ°á»c thoÃ¡t ra tá»« lá»p vá» khi cÃ¡c khÃ­ gas bá» nÃºi lá»­a phun lÃªn',\n",
              " 'uit_000173': 'amoniac, mÃªtan, hÆ¡i nÆ°á»c, cacbon ÄiÃ´xÃ­t, vÃ  nitÆ¡, cÅ©ng nhÆ° má»t lÆ°á»£ng nhá» cÃ¡c cháº¥t khÃ­',\n",
              " 'uit_000174': 'lá»p ozone',\n",
              " 'uit_000175': 'ThÃ¡i Viá»n Cá»',\n",
              " 'uit_000176': 'giÃ³ máº·t trá»i vÃ  chÃ­nh nhiá»t lÆ°á»£ng cá»§a TrÃ¡i Äáº¥t',\n",
              " 'uit_000177': 'nhá»¯ng cuá»c va cháº¡m cá»§a sao bÄng',\n",
              " 'uit_000178': 'vÅ© trá»¥',\n",
              " 'uit_000179': 'má»t phÃ¢n tá»­ (hay tháº­m chÃ­ lÃ  má»t thá»© gÃ¬ khÃ¡c) ÄÃ£ cÃ³ kháº£ nÄng tá»± phÃ¢n chia thÃ nh cÃ¡c báº£n sao cá»§a chÃ­nh nÃ³',\n",
              " 'uit_000180': 'cÃ¡c dÃ²ng dÃµi sau ÄÃ³ cÃ³ thá» khai thÃ¡c cÃ¡c nguyÃªn liá»u khÃ¡c, hay cÃ³ láº½ lÃ  há»c cÃ¡ch tiáº¿n triá»n cá»§a cÃ¡c kiá»u dÃ²ng dÃµi khÃ¡c, vÃ  trá» nÃªn ÄÃ´ng Äáº£o hÆ¡n',\n",
              " 'uit_000181': 'tá»« vÅ© trá»¥',\n",
              " 'uit_000182': 'khoáº£ng 4 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000183': 'thÃºc Äáº©y cÃ¡c pháº£n á»©ng hÃ³a há»c táº¡o thÃ nh báº£n sao cá»§a chÃ­nh nÃ³',\n",
              " 'uit_000184': 'may máº¯n',\n",
              " 'uit_000185': 'DNA',\n",
              " 'uit_000186': 'DNA',\n",
              " 'uit_000187': 'protein hiá»n Äáº¡i cá»§a cÃ¡c acid nucleic, phospholipid, tinh thá», hay tháº­m chÃ­ cÃ¡c há» lÆ°á»£ng tá»­',\n",
              " 'uit_000188': 'cÃ³ tÃ­nh cháº¥t ká»³ dá» thÃºc Äáº©y cÃ¡c pháº£n á»©ng hÃ³a há»c táº¡o thÃ nh báº£n sao cá»§a chÃ­nh nÃ³, vÃ  tiáº¿n trÃ¬nh phÃ¡t triá»n thá»±c sá»± báº¯t Äáº§u',\n",
              " 'uit_000189': 'phÃ¢n tá»­ phospholipid',\n",
              " 'uit_000190': 'enzym',\n",
              " 'uit_000191': 'CÃ¡c protein',\n",
              " 'uit_000192': 'RNA',\n",
              " 'uit_000193': 'trao Äá»i thÃ´ng tin vÃ  tá»ng há»£p protein, vÃ  cÃ¡c enzyme lÃ m xÃºc tÃ¡c cho pháº£n á»©ng',\n",
              " 'uit_000194': 'ÄiÃ´xÃ­t cacbon vÃ  nÆ°á»c',\n",
              " 'uit_000195': 'thá»© ba',\n",
              " 'uit_000196': 'háº¥p thá»¥ Ã¡nh sÃ¡ng máº·t trá»i nhÆ° má»t nguá»n nÄng lÆ°á»£ng',\n",
              " 'uit_000197': 'biáº¿n Äá»i lá»n',\n",
              " 'uit_000198': 'Ãxy lÃ  cháº¥t Äá»c',\n",
              " 'uit_000199': 'ÄiÃ´xÃ­t cacbon vÃ  nÆ°á»c',\n",
              " 'uit_000200': 'Ãxy lÃ  cháº¥t Äá»c Äá»i vá»i nhiá»u dáº¡ng sá»ng vÃ o thá»i ká»³ nÃ y',\n",
              " 'uit_000201': 'khÃ­ Ã´xy',\n",
              " 'uit_000202': 'thá»i ká»³ khÃ­ quyá»n thá»© ba',\n",
              " 'uit_000203': 'Archarea vÃ  Eukarya',\n",
              " 'uit_000204': 'chloroplast',\n",
              " 'uit_000205': 'táº¿ bÃ o nhá» tÃ¬m cÃ¡ch kÃ½ sinh trÃªn táº¿ bÃ o lá»n',\n",
              " 'uit_000206': 'mitochondria',\n",
              " 'uit_000207': 'PhÃ©p phÃ¢n loáº¡i hiá»n Äáº¡i',\n",
              " 'uit_000208': 'Archarea vÃ  Eukarya',\n",
              " 'uit_000209': 'Vá»±c Bacteria',\n",
              " 'uit_000210': 'sá»± phÃ¢n chia giá»¯a má»t táº­p ÄoÃ n vá»i cÃ¡c táº¿ bÃ o',\n",
              " 'uit_000211': 'táº¥t cáº£ cÃ¡c táº¿ bÃ o Äá»u mang tÃ­nh toÃ n nÄng (totipotent)',\n",
              " 'uit_000212': 'khoáº£ng 750 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000213': 'Khoáº£ng 1.1 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000214': 'táº£o lá»¥c',\n",
              " 'uit_000215': 'eukaryotes',\n",
              " 'uit_000216': 'cÃ³ láº½ lÃ  táº£o lá»¥c',\n",
              " 'uit_000217': 'khoáº£ng 750 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000218': 'Khoáº£ng 1.1 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000219': 'khoáº£ng 530 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000220': 'khoáº£ng 600 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000221': 'cÃ¡c sinh váº­t ÄÆ¡n bÃ o Äi lÃªn máº·t Äáº¥t sáº½ cÃ³ cÆ¡ há»i sá»ng sÃ³t cao hÆ¡n, vÃ  cÃ¡c sinh váº­t chÆ°a cÃ³ nhÃ¢n ÄÃ£ báº¯t Äáº§u sinh sÃ´i vÃ  trá» nÃªn thÃ­ch á»©ng tá»t hÆ¡n vá»i mÃ´i trÆ°á»ng sá»ng bÃªn ngoÃ i Äáº¡i dÆ°Æ¡ng',\n",
              " 'uit_000222': 'CÃ¡',\n",
              " 'uit_000223': 'CÃ¡, nhá»¯ng Äá»ng váº­t cÃ³ xÆ°Æ¡ng sá»ng',\n",
              " 'uit_000224': '50 triá»u nÄm',\n",
              " 'uit_000225': 'khoáº£ng 600 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000226': '50 triá»u nÄm',\n",
              " 'uit_000227': 'chÃ¢n Äá»t',\n",
              " 'uit_000228': '530 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000229': 'cÃ¡c vÃ¢y',\n",
              " 'uit_000230': '530 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000231': '1 tá»· nÄm trÆ°á»c',\n",
              " 'uit_000232': 'Khoáº£ng 365 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000233': 'sá»± láº¡nh Äi toÃ n cáº§u',\n",
              " 'uit_000234': 'Pangea',\n",
              " 'uit_000235': 'Jura',\n",
              " 'uit_000236': 'Pangea',\n",
              " 'uit_000237': '10 kilÃ´mÃ©t',\n",
              " 'uit_000238': '340 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000239': '340 triá»u nÄm trÆ°á»c',\n",
              " 'uit_000240': 'khoáº£ng thá»i gian phÃ¢n tÃ¡ch giá»¯a ká»· Permi vÃ  Trias',\n",
              " 'uit_000241': 'Pangaea',\n",
              " 'uit_000242': 'hai triá»u nÄm trÆ°á»c',\n",
              " 'uit_000243': 'loÃ i tinh tinh',\n",
              " 'uit_000244': 'kháº£ nÄng Äá»©ng tháº³ng',\n",
              " 'uit_000245': 'chÃ¢u Phi',\n",
              " 'uit_000246': 'kháº£ nÄng Äá»©ng tháº³ng',\n",
              " 'uit_000247': 'NgÆ°á»i Cro-Magnon',\n",
              " 'uit_000248': 'NgÆ°á»i Cro-Magnon',\n",
              " 'uit_000249': 'chÃ¢u Phi',\n",
              " 'uit_000250': 'hai triá»u nÄm trÆ°á»c',\n",
              " 'uit_000251': 'thá» hiá»n Äá»©c tin tÃ´n giÃ¡o',\n",
              " 'uit_000252': 'Thá»i Äiá»m nÃ o ÄÃ³ trong khoáº£ng 8500 tá»i 7000 trÆ°á»c CÃ´ng NguyÃªn',\n",
              " 'uit_000253': 'sÄn báº¯t - hÃ¡i lÆ°á»£m',\n",
              " 'uit_000254': 'LÆ°á»¡ng HÃ ',\n",
              " 'uit_000255': 'táº§ng lá»p cai trá» vÃ  tháº§y cÃºng xuáº¥t hiá»n, tiáº¿p ÄÃ³ lÃ  sá»± phÃ¢n cÃ´ng lao Äá»ng',\n",
              " 'uit_000256': 'Sumer vÃ¹ng Trung ÄÃ´ng',\n",
              " 'uit_000257': 'má»t hÃ¬nh thá»©c tÃ¡i truyá»n táº£i thÃ´ng tin má»i',\n",
              " 'uit_000258': 'sÄn báº¯t - hÃ¡i lÆ°á»£m',\n",
              " 'uit_000259': 'nhá»¯ng thÆ° khá» vÃ  cÃ¡c thÆ° viá»n trá» thÃ nh nÆ¡i lÆ°u giá»¯ nhá»¯ng hiá»u biáº¿t cá»§a nhÃ¢n loáº¡i cÅ©ng nhÆ° tÄng cÆ°á»ng sá»± chuyá»n giao vÄn hÃ³a vÃ  thÃ´ng tin',\n",
              " 'uit_000260': 'Sá»± phÃ¡t minh ra chá»¯ viáº¿t',\n",
              " 'uit_000261': 'tÃ­nh tÃ² mÃ² vÃ  giÃ¡o dá»¥c',\n",
              " 'uit_000262': 'tÃ­nh tÃ² mÃ² vÃ  giÃ¡o dá»¥c khiáº¿n má»i ngÆ°á»i nhanh chÃ³ng cÃ³ ÄÆ°á»£c sá»± hiá»u biáº¿t vÃ  khÃ´n ngoan',\n",
              " 'uit_000263': 'phÃ¡t triá»n tá»i cá»±c Äiá»m',\n",
              " 'uit_000264': 'cho phÃ©p cÃ¡c xÃ£ há»i phá»©c táº¡p hÆ¡n xuáº¥t hiá»n',\n",
              " 'uit_000265': 'phÃ¡t triá»n tá»i cá»±c Äiá»m',\n",
              " 'uit_000266': 'Há»i quá»c liÃªn',\n",
              " 'uit_000267': 'Tá»« nÄm 1914 Äáº¿n 1918',\n",
              " 'uit_000268': 'Há»i quá»c liÃªn',\n",
              " 'uit_000269': 'váº­n táº£i vÃ  thÃ´ng tin phÃ¡t triá»n',\n",
              " 'uit_000270': 'Italia',\n",
              " 'uit_000271': 'LiÃªn minh chÃ¢u Ãu',\n",
              " 'uit_000272': 'khoáº£ng nÄm 1500',\n",
              " 'uit_000273': 'Tá»« nÄm 1914 Äáº¿n 1918',\n",
              " 'uit_000274': 'Italia',\n",
              " 'uit_000275': 'sá»± tuyá»t chá»§ng hÃ ng loáº¡t vÃ  sá»± áº¥m lÃªn toÃ n cáº§u',\n",
              " 'uit_000276': 'Sá»± thay Äá»i tiáº¿p tá»¥c diá»n ra vá»i tá»c Äá» ngÃ y cÃ ng nhanh trong pháº§n nghÃ¬n giÃ¢y cuá»i cÃ¹ng cá»§a 24 giá» tÆ°á»ng tÆ°á»£ng',\n",
              " 'uit_000277': 'sá»± tuyá»t chá»§ng hÃ ng loáº¡t vÃ  sá»± áº¥m lÃªn toÃ n cáº§u',\n",
              " 'uit_000278': 'LiÃªn bang xÃ´ viáº¿t',\n",
              " 'uit_000279': 'LiÃªn bang xÃ´ viáº¿t',\n",
              " 'uit_000280': 'nÄm 2000',\n",
              " 'uit_000281': 'LiÃªn bang xÃ´ viáº¿t',\n",
              " 'uit_000282': 'Yuri Gagarin',\n",
              " 'uit_000283': 'NÄm',\n",
              " 'uit_000284': 'nÄm 2000',\n",
              " 'uit_000285': 'hiá»n diá»n thÆ°á»ng xuyÃªn trÃªn vÅ© trá»¥ hay tháº­m chÃ­ chiáº¿m lÃ m thuá»c Äá»a nhá»¯ng tháº¿ giá»i xa xÃ´i',\n",
              " 'uit_000286': 'hiá»n diá»n thÆ°á»ng xuyÃªn trÃªn vÅ© trá»¥ hay tháº­m chÃ­ chiáº¿m lÃ m thuá»c Äá»a nhá»¯ng tháº¿ giá»i xa xÃ´i',\n",
              " 'uit_000287': 'Yuri Gagarin',\n",
              " 'uit_000513': 'bá»©c xáº¡ Hawking',\n",
              " 'uit_000514': 'sá»± thá»ng nháº¥t giá»¯a thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t vÃ  cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000515': 'lÃ  má»t nhÃ  váº­t lÃ½ lÃ½ thuyáº¿t, vÅ© trá»¥ há»c, tÃ¡c giáº£ viáº¿t sÃ¡ch khoa há»c thÆ°á»ng thá»©c ngÆ°á»i Anh, nguyÃªn GiÃ¡m Äá»c NghiÃªn cá»©u táº¡i Trung tÃ¢m VÅ© trá»¥ há»c lÃ½ thuyáº¿t thuá»c Äáº¡i há»c Cambridge',\n",
              " 'uit_000516': 'Hawking lÃ  ngÆ°á»i Äáº§u tiÃªn khá»i Äáº§u má»t ná»n vÅ© trá»¥ há»c dá»±a trÃªn sá»± thá»ng nháº¥t giá»¯a thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t vÃ  cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000517': 'Roger Penrose',\n",
              " 'uit_000518': 'lÃ  má»t nhÃ  váº­t lÃ½ lÃ½ thuyáº¿t, vÅ© trá»¥ há»c, tÃ¡c giáº£ viáº¿t sÃ¡ch khoa há»c thÆ°á»ng thá»©c ngÆ°á»i Anh, nguyÃªn GiÃ¡m Äá»c NghiÃªn cá»©u táº¡i Trung tÃ¢m VÅ© trá»¥ há»c lÃ½ thuyáº¿t thuá»c Äáº¡i há»c Cambridge',\n",
              " 'uit_000519': 'bá»©c xáº¡ Hawking',\n",
              " 'uit_000520': '21 tuá»i',\n",
              " 'uit_000521': 'cÄn bá»nh ALS sáº½ khiáº¿n Ã´ng chá» sá»ng thÃªm ÄÆ°á»£c vÃ i nÄm',\n",
              " 'uit_000522': 'kháº£ nÄng nÃ³i chuyá»n',\n",
              " 'uit_000523': 'chá»©ng bá»nh xÆ¡ cá»©ng teo cÆ¡',\n",
              " 'uit_000524': 'Ã´ng chá» sá»ng thÃªm ÄÆ°á»£c vÃ i nÄm',\n",
              " 'uit_000525': 'giao tiáº¿p thÃ´ng qua má»t thiáº¿t bá» táº¡o giá»ng nÃ³i ÄÆ°á»£c gáº¯n trá»±c tiáº¿p vÃ o chiáº¿c xe lÄn cá»§a Ã´ng',\n",
              " 'uit_000526': 'thÃ´ng qua má»t thiáº¿t bá» táº¡o giá»ng nÃ³i ÄÆ°á»£c gáº¯n trá»±c tiáº¿p vÃ o chiáº¿c xe lÄn cá»§a Ã´ng',\n",
              " 'uit_000527': 'xÆ¡ cá»©ng teo cÆ¡',\n",
              " 'uit_000528': 'tháº§y dáº¡y toÃ¡n ná»i tiáº¿ng Dikran Tahta',\n",
              " 'uit_000529': 'Hawking duy trÃ¬ ÄÆ°á»£c má»t nhÃ³m báº¡n thÃ¢n mÃ  Ã´ng thÆ°á»ng tham gia chÆ¡i bÃ i, lÃ m phÃ¡o hoa, cÃ¡c mÃ´ hÃ¬nh phi cÆ¡ vÃ  tÃ u thuyá»n, cÅ©ng nhÆ° tháº£o luáº­n vá» CÆ¡ Äá»c giÃ¡o vÃ  nÄng lá»±c ngoáº¡i cáº£m. Tá»« 1958, vá»i sá»± giÃºp Äá»¡ cá»§a tháº§y dáº¡y toÃ¡n ná»i tiáº¿ng Dikran Tahta, há» xÃ¢y dá»±ng má»t mÃ¡y tÃ­nh vá»i cÃ¡c linh kiá»n láº¥y tá»« Äá»ng há», má»t mÃ¡y tá»ng ÄÃ i Äiá»n thoáº¡i cÅ© vÃ  cÃ¡c thiáº¿t bá» tÃ¡i cháº¿ khÃ¡c',\n",
              " 'uit_000530': 'tháº§y dáº¡y toÃ¡n ná»i tiáº¿ng Dikran Tahta',\n",
              " 'uit_000531': 'ráº¥t Äá» cao giÃ¡ trá» cá»§a viá»c há»c hÃ nh',\n",
              " 'uit_000532': 'Hawking duy trÃ¬ ÄÆ°á»£c má»t nhÃ³m báº¡n thÃ¢n mÃ  Ã´ng thÆ°á»ng tham gia chÆ¡i bÃ i, lÃ m phÃ¡o hoa, cÃ¡c mÃ´ hÃ¬nh phi cÆ¡ vÃ  tÃ u thuyá»n, cÅ©ng nhÆ° tháº£o luáº­n vá» CÆ¡ Äá»c giÃ¡o vÃ  nÄng lá»±c ngoáº¡i cáº£m',\n",
              " 'uit_000533': 'bá» á»m vÃ o ÄÃºng ngÃ y thi láº¥y há»c bá»ng',\n",
              " 'uit_000534': 'lo ngáº¡i ráº±ng khÃ´ng cÃ³ máº¥y viá»c lÃ m cho má»t sinh viÃªn ngÃ nh toÃ¡n ra trÆ°á»ng',\n",
              " 'uit_000535': 'Einstein',\n",
              " 'uit_000536': 'máº·c dÃ¹ Äiá»m sá» khÃ´ng tá»t nhÆ°ng cáº£ giÃ¡o viÃªn vÃ  báº¡n bÃ¨ Äá»u tháº¥y ÄÆ°á»£c tá» cháº¥t thiÃªn tÃ i cá»§a Ã´ng',\n",
              " 'uit_000537': 'cÃ¡c mÃ´n khoa há»c tá»± nhiÃªn',\n",
              " 'uit_000538': 'vÃ¬ lo ngáº¡i ráº±ng khÃ´ng cÃ³ máº¥y viá»c lÃ m cho má»t sinh viÃªn ngÃ nh toÃ¡n ra trÆ°á»ng',\n",
              " 'uit_000539': 'sá»± trá» náº£i cá»§a Ã´ng',\n",
              " 'uit_000540': 'lÃ¡i Äá»i Äua theo nhá»¯ng hÆ°á»ng nguy hiá»m thÆ°á»ng dáº«n tá»i thuyá»n bá» hÆ° háº¡i',\n",
              " 'uit_000541': 'pháº¥n Äáº¥u vÃ  trá» thÃ nh má»t sinh viÃªn ÄÆ°á»£c quÃ½ máº¿n, hoáº¡t bÃ¡t, dÃ­ dá»m, há»©ng thÃº vá»i nháº¡c cá» Äiá»n vÃ  tiá»u thuyáº¿t viá»n tÆ°á»ng',\n",
              " 'uit_000542': 'Äá»i vá»i cáº­u ta chá» cáº§n biáº¿t Äiá»u gÃ¬ ÄÃ³ cÃ³ thá» thá»±c hiá»n, vÃ  cáº­u cÃ³ thá» lÃ m nÃ³ mÃ  khÃ´ng cáº§n pháº£i ngÃ³ xem nhá»¯ng ngÆ°á»i khÃ¡c ÄÃ£ lÃ m tháº¿ nÃ o',\n",
              " 'uit_000543': 'Hawking pháº¥n Äáº¥u vÃ  trá» thÃ nh má»t sinh viÃªn ÄÆ°á»£c quÃ½ máº¿n, hoáº¡t bÃ¡t, dÃ­ dá»m, há»©ng thÃº vá»i nháº¡c cá» Äiá»n vÃ  tiá»u thuyáº¿t viá»n tÆ°á»ng',\n",
              " 'uit_000544': 'tÃ¡o báº¡o',\n",
              " 'uit_000545': 'Ã´ng Ã­t tuá»i hÆ¡n pháº§n lá»n sinh viÃªn',\n",
              " 'uit_000546': 'lÃ¡i Äá»i Äua theo nhá»¯ng hÆ°á»ng nguy hiá»m thÆ°á»ng dáº«n tá»i thuyá»n bá» hÆ° háº¡i',\n",
              " 'uit_000547': 'Äá»i vá»i cáº­u ta chá» cáº§n biáº¿t Äiá»u gÃ¬ ÄÃ³ cÃ³ thá» thá»±c hiá»n, vÃ  cáº­u cÃ³ thá» lÃ m nÃ³ mÃ  khÃ´ng cáº§n pháº£i ngÃ³ xem nhá»¯ng ngÆ°á»i khÃ¡c ÄÃ£ lÃ m tháº¿ nÃ o',\n",
              " 'uit_000548': '1000 giá»',\n",
              " 'uit_000549': 'káº¿t quáº£ náº±m á» ÄÃºng Äiá»m sá» ranh giá»i giá»¯a háº¡ng nháº¥t vÃ  háº¡ng nhÃ¬',\n",
              " 'uit_000550': 'Äá» phÃ¢n háº¡ng',\n",
              " 'uit_000551': 'pháº£i cÃ³ má»t báº±ng danh dá»± háº¡ng nháº¥t',\n",
              " 'uit_000552': 'káº¿t quáº£ náº±m á» ÄÃºng Äiá»m sá» ranh giá»i giá»¯a háº¡ng nháº¥t vÃ  háº¡ng nhÃ¬',\n",
              " 'uit_000553': 'chá» tráº£ lá»i nhá»¯ng cÃ¢u há»i váº­t lÃ½ lÃ½ thuyáº¿t vÃ  bá» qua nhá»¯ng cÃ¢u ÄÃ²i há»i kiáº¿n thá»©c thá»±c táº¿',\n",
              " 'uit_000554': 'phÃ¢n háº¡ng',\n",
              " 'uit_000555': 'Náº¿u cÃ¡c vá» trao cho tÃ´i háº¡ng Nháº¥t, tÃ´i sáº½ tá»i Cambridge. Náº¿u tÃ´i nháº­n háº¡ng NhÃ¬, tÃ´i sáº½ á» láº¡i Oxford, vÃ¬ váº­y tÃ´i hi vá»ng cÃ¡c vá» cho tÃ´i háº¡ng Nháº¥t',\n",
              " 'uit_000556': 'bá» xem lÃ  má»t sinh viÃªn lÆ°á»i nhÃ¡c vÃ  khÃ³ tÃ­nh',\n",
              " 'uit_000557': 'báº¯t Äáº§u vÃ o há»c báº­c trÃªn Äáº¡i há»c táº¡i Trinity Hall (Äáº¡i há»c Cambridge)',\n",
              " 'uit_000558': 'thÃ¡ng 10 nÄm 1962',\n",
              " 'uit_000559': 'Hawking rÆ¡i vÃ o tráº§m uáº¥t; máº·c dÃ¹ cÃ¡c bÃ¡c sÄ© khuyÃªn Ã´ng tiáº¿p tá»¥c há»c hÃ nh, Ã´ng cáº£m tháº¥y cháº³ng cÃ²n máº¥y Ã½ nghÄ©a',\n",
              " 'uit_000560': 'rÆ¡i vÃ o tráº§m uáº¥t',\n",
              " 'uit_000561': 'Jane Wilde, báº¡n cá»§a em gÃ¡i Ã´ng',\n",
              " 'uit_000562': 'Jane Wilde',\n",
              " 'uit_000563': 'ngÆ°á»£c ngáº¡o',\n",
              " 'uit_000564': 'CÃ¡c ká»³ dá» vÃ  HÃ¬nh há»c cá»§a KhÃ´ng-Thá»i gian',\n",
              " 'uit_000565': 'giáº£i dÃ nh cho nghiÃªn cá»©u toÃ¡n há»c xuáº¥t sáº¯c nháº¥t hÃ ng nÄm cá»§a Cambridge',\n",
              " 'uit_000566': 'CÃ¡c ká»³ dá» vÃ  HÃ¬nh há»c cá»§a KhÃ´ng-Thá»i gian',\n",
              " 'uit_000567': 'cÃ¡c lÃ½ thuyáº¿t Äang thá»nh hÃ nh liÃªn quan tá»i sá»± khai sinh vÅ© trá»¥: thuyáº¿t Vá»¥ Ná» Lá»n vÃ  thuyáº¿t vÅ© trá»¥ tÄ©nh táº¡i',\n",
              " 'uit_000568': 'Ã¡p dá»¥ng Ã½ tÆ°á»ng tÆ°Æ¡ng tá»± cho toÃ n thá» vÅ© trá»¥',\n",
              " 'uit_000569': 'cÃ¡c lÃ½ thuyáº¿t Äang thá»nh hÃ nh liÃªn quan tá»i sá»± khai sinh vÅ© trá»¥',\n",
              " 'uit_000570': 'Äá»nh lÃ½ vá» kÃ¬ dá» khÃ´ng-thá»i gian trong tÃ¢m cÃ¡c há» Äen cá»§a Roger Penrose',\n",
              " 'uit_000571': 'DÆ°á»i áº£nh hÆ°á»ng cá»§a Äá»nh lÃ½ vá» kÃ¬ dá» khÃ´ng-thá»i gian trong tÃ¢m cÃ¡c há» Äen cá»§a Roger Penrose, Hawking Ã¡p dá»¥ng Ã½ tÆ°á»ng tÆ°Æ¡ng tá»± cho toÃ n thá» vÅ© trá»¥',\n",
              " 'uit_000572': 'vá» nhÃ¬ trong cuá»c thi cá»§a Quá»¹ NghiÃªn cá»©u Lá»±c Háº¥p dáº«n nÄm 1968',\n",
              " 'uit_000573': 'vÅ© trá»¥ tá»± nÃ³ cÃ³ thá» khá»i Äáº§u tá»« má»t kÃ¬ dá»',\n",
              " 'uit_000574': 'cÃ¡c quan niá»m vá» Äá»nh lÃ½ Äiá»m kÃ¬ dá»',\n",
              " 'uit_000575': 'cÃ¡c quan niá»m vá» Äá»nh lÃ½ Äiá»m kÃ¬ dá» mÃ  Ã´ng khÃ¡m phÃ¡ trong luáº­n Ã¡n tiáº¿n sÄ©',\n",
              " 'uit_000576': 'cÃ´ng bá» má»t phÃ©p chá»©ng minh ráº±ng náº¿u vÅ© trá»¥ tuÃ¢n theo lÃ½ thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t vÃ  phÃ¹ há»£p vá»i báº¥t ká»³ mÃ´ hÃ¬nh nÃ o vá» vÅ© trá»¥ há»c váº­t lÃ½ phÃ¡t triá»n bá»i Alexander Friedmann, thÃ¬ nÃ³ pháº£i khá»i Äáº§u tá»« má»t kÃ¬ dá»',\n",
              " 'uit_000577': 'cÃ´ng bá» má»t phÃ©p chá»©ng minh ráº±ng náº¿u vÅ© trá»¥ tuÃ¢n theo lÃ½ thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t vÃ  phÃ¹ há»£p vá»i báº¥t ká»³ mÃ´ hÃ¬nh nÃ o vá» vÅ© trá»¥ há»c váº­t lÃ½ phÃ¡t triá»n bá»i Alexander Friedmann, thÃ¬ nÃ³ pháº£i khá»i Äáº§u tá»« má»t kÃ¬ dá»',\n",
              " 'uit_000578': 'vá» nhÃ¬ trong cuá»c thi cá»§a Quá»¹ NghiÃªn cá»©u Lá»±c Háº¥p dáº«n nÄm 1968',\n",
              " 'uit_000579': 'trÆ°á»c háº¿t nhÆ° má»t nhÃ  khoa há»c, thá»© Äáº¿n nhÆ° má»t nhÃ  vÄn phá» biáº¿n khoa há»c, vÃ , trong má»i cÃ¡ch mÃ  nÃ³ ÄÃ¡ng ká», má»t ngÆ°á»i bÃ¬nh thÆ°á»ng vá»i cÃ¹ng nhá»¯ng ham muá»n, nghá» lá»±c, Æ°á»c mÆ¡ vÃ  tham vá»ng nhÆ° nhá»¯ng ngÆ°á»i xung quanh',\n",
              " 'uit_000580': 'Äá»c láº­p má»t cÃ¡ch mÃ£nh liá»t vÃ  khÃ´ng báº±ng lÃ²ng cháº¥p nháº­n giÃºp Äá»¡ hay chá»u nhÆ°á»£ng bá» vÃ¬ sá»± tÃ n táº­t cá»§a mÃ¬nh',\n",
              " 'uit_000581': 'Ã´ng phÃ¡t triá»n cÃ¡c phÆ°Æ¡ng phÃ¡p thá» giÃ¡c Äá» bÃ¹ Äáº¯p, bao gá»m nhÃ¬n cÃ¡c phÆ°Æ¡ng trÃ¬nh theo cÃ¡ch hiá»u hÃ¬nh há»c',\n",
              " 'uit_000582': 'Ã´ng báº¯t Äáº§u pháº£i dÃ¹ng náº¡ng vÃ  thÆ°á»ng xuyÃªn há»§y cÃ¡c buá»i giáº£ng',\n",
              " 'uit_000583': 'trÆ°á»c háº¿t nhÆ° má»t nhÃ  khoa há»c, thá»© Äáº¿n nhÆ° má»t nhÃ  vÄn phá» biáº¿n khoa há»c, vÃ , trong má»i cÃ¡ch mÃ  nÃ³ ÄÃ¡ng ká», má»t ngÆ°á»i bÃ¬nh thÆ°á»ng vá»i cÃ¹ng nhá»¯ng ham muá»n, nghá» lá»±c, Æ°á»c mÆ¡ vÃ  tham vá»ng nhÆ° nhá»¯ng ngÆ°á»i xung quanh',\n",
              " 'uit_000584': 'Ã´ng phÃ¡t triá»n cÃ¡c phÆ°Æ¡ng phÃ¡p thá» giÃ¡c Äá» bÃ¹ Äáº¯p, bao gá»m nhÃ¬n cÃ¡c phÆ°Æ¡ng trÃ¬nh theo cÃ¡ch hiá»u hÃ¬nh há»c',\n",
              " 'uit_000585': 'Äá» giá»¯ Ã´ng láº¡i á» Caius',\n",
              " 'uit_000586': 'bá»nh táº­t cÅ©ng nhÆ° danh tiáº¿ng vá» trÃ­ tuá» vÃ  sá»± ngáº¡o ngÆ°á»£c cá»§a Ã´ng',\n",
              " 'uit_000587': 'bá»nh táº­t cÅ©ng nhÆ° danh tiáº¿ng vá» trÃ­ tuá» vÃ  sá»± ngáº¡o ngÆ°á»£c cá»§a Ã´ng',\n",
              " 'uit_000588': 'cuá»i nhá»¯ng nÄm 1960',\n",
              " 'uit_000589': 'cuá»i nhá»¯ng nÄm 1960',\n",
              " 'uit_000590': 'Carter, Werner Israel vÃ  David C. Robinson',\n",
              " 'uit_000591': 'Jacob Bekenstein, má»t nghiÃªn cá»©u sinh cá»§a John Wheeler',\n",
              " 'uit_000592': 'báº¥t ká» há» Äen ban Äáº§u táº¡o thÃ nh tá»« váº­t liá»u nÃ o, nÃ³ hoÃ n toÃ n cÃ³ thá» mÃ´ táº£ báº±ng ba tÃ­nh cháº¥t khá»i lÆ°á»£ng, Äiá»n tÃ­ch vÃ  sá»± tá»± quay',\n",
              " 'uit_000593': 'Cáº¥u trÃºc VÄ© mÃ´ cá»§a KhÃ´ng-Thá»i gian',\n",
              " 'uit_000594': 'chÃ¢n trá»i sá»± kiá»n cá»§a há» Äen khÃ´ng bao giá» cÃ³ thá» thu nhá» hÆ¡n',\n",
              " 'uit_000595': 'báº¥t ká» há» Äen ban Äáº§u táº¡o thÃ nh tá»« váº­t liá»u nÃ o, nÃ³ hoÃ n toÃ n cÃ³ thá» mÃ´ táº£ báº±ng ba tÃ­nh cháº¥t khá»i lÆ°á»£ng, Äiá»n tÃ­ch vÃ  sá»± tá»± quay',\n",
              " 'uit_000596': 'Jacob Bekenstein',\n",
              " 'uit_000597': 'kháº³ng Äá»nh ráº±ng chÃ¢n trá»i sá»± kiá»n cá»§a há» Äen khÃ´ng bao giá» cÃ³ thá» thu nhá» hÆ¡n',\n",
              " 'uit_000598': \"má»t chuyáº¿n thÄm tá»i Moskva vÃ  nhá»¯ng cuá»c tháº£o luáº­n vá»i Yakov Borisovich Zel'dovich vÃ  Alexander Starobinsky\",\n",
              " 'uit_000599': 'há» Äen phÃ¡t ra bá»©c xáº¡ - mÃ  ngÃ y nay ÄÆ°á»£c gá»i lÃ  bá»©c xáº¡ Hawking - cho Äáº¿n khi chÃºng cáº¡n kiá»t nÄng lÆ°á»£ng vÃ  bay hÆ¡i',\n",
              " 'uit_000600': 'khÃ¡m phÃ¡ nÃ y ÄÆ°á»£c cháº¥p nháº­n rá»ng rÃ£i nhÆ° má»t Äá»t phÃ¡ quan trá»ng trong váº­t lÃ½ lÃ½ thuyáº¿t',\n",
              " 'uit_000601': 'há» Äen phÃ¡t ra bá»©c xáº¡ - mÃ  ngÃ y nay ÄÆ°á»£c gá»i lÃ  bá»©c xáº¡ Hawking',\n",
              " 'uit_000602': 'theo nguyÃªn lÃ½ báº¥t Äá»nh cÃ¡c há» Äen quay phÃ¡t ra cÃ¡c háº¡t',\n",
              " 'uit_000603': 'háº¥p dáº«n lÆ°á»£ng tá»­ vÃ  cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000604': 'nhá»¯ng tÃ­nh toÃ¡n ÄÆ°á»£c kiá»m tra nhiá»u láº§n cá»§a Ã´ng cho ra nhá»¯ng phÃ¡t hiá»n mÃ¢u thuáº«n vá»i Äá»nh luáº­t cá»§a Ã´ng',\n",
              " 'uit_000605': 'háº¥p dáº«n lÆ°á»£ng tá»­ vÃ  cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000606': 'nhá»¯ng tÃ­nh toÃ¡n ÄÆ°á»£c kiá»m tra nhiá»u láº§n cá»§a Ã´ng cho ra nhá»¯ng phÃ¡t hiá»n mÃ¢u thuáº«n vá»i Äá»nh luáº­t cá»§a Ã´ng, vá»n kháº³ng Äá»nh ráº±ng cÃ¡c há» Äen khÃ´ng bao giá» co láº¡i (chá» giá»¯ nguyÃªn hoáº·c lá»n lÃªn), vÃ  á»§ng há» láº­p luáº­n cá»§a Bekenstein vá» entropy cá»§a chÃºng',\n",
              " 'uit_000607': 'má»t nghiÃªn cá»©u sinh hoáº·c sinh viÃªn háº­u tiáº¿n sÄ© sá»ng vá»i há» vÃ  giÃºp chÄm sÃ³c Ã´ng',\n",
              " 'uit_000608': 'khiáº¿n cho cÃ¡c trÃ¡ch nhiá»m gia ÄÃ¬nh rÆ¡i xuá»ng ÄÃ´i vai ngÃ y cÃ ng cháº¥t náº·ng cá»§a vá»£ Ã´ng',\n",
              " 'uit_000609': 'Bernard Carr',\n",
              " 'uit_000610': 'nghiÃªn cá»©u sinh hoáº·c sinh viÃªn háº­u tiáº¿n sÄ© sá»ng vá»i há»',\n",
              " 'uit_000611': 'cÃ¡c trÃ¡ch nhiá»m gia ÄÃ¬nh rÆ¡i xuá»ng ÄÃ´i vai ngÃ y cÃ ng cháº¥t náº·ng cá»§a vá»£ Ã´ng',\n",
              " 'uit_000612': 'Cygnus X-1 lÃ  má»t há» Äen',\n",
              " 'uit_000613': 'váº­t cháº¥t tá»i neutralino',\n",
              " 'uit_000614': 'má»t thÃ¡ng',\n",
              " 'uit_000615': 'sao tá»i (sao chá»©a má»t hÃ m lÆ°á»£ng lá»n váº­t cháº¥t tá»i neutralino) Cygnus X-1 lÃ  má»t há» Äen',\n",
              " 'uit_000616': 'há» Äen vÃ  nhá»¯ng nhÃ  váº­t lÃ½ nghiÃªn cá»©u Äá» tÃ i nÃ y',\n",
              " 'uit_000617': 'PhÃ³ GiÃ¡o sÆ°',\n",
              " 'uit_000618': 'Hawking thÆ°á»ng xuyÃªn ÄÆ°á»£c bÃ¡o chÃ­ vÃ  truyá»n hÃ¬nh má»i phá»ng váº¥n',\n",
              " 'uit_000619': 'cÃ´ng chÃºng cÃ³ sá»± quan tÃ¢m ngÃ y cÃ ng tÄng tá»i há» Äen',\n",
              " 'uit_000620': 'PhÃ³ GiÃ¡o sÆ°',\n",
              " 'uit_000621': 'thÆ°á»ng xuyÃªn ÄÆ°á»£c bÃ¡o chÃ­ vÃ  truyá»n hÃ¬nh má»i phá»ng váº¥n',\n",
              " 'uit_000622': 'Bá» khÃ­ch Äá»ng tá»« má»t cuá»c tranh luáº­n vá»i Äáº¡i há»c vá» viá»c ai sáº½ tráº£ tiá»n cho cÃ¡c bá» dá»c thoáº£i Äá» Ã´ng cÃ³ thá» Äi xe lÄn tá»i chá» lÃ m',\n",
              " 'uit_000623': 'cuá»i nhá»¯ng nÄm 1970',\n",
              " 'uit_000624': 'váº­n Äá»ng cho viá»c cáº£i thiá»n cÃ¡c lá»i Äi vÃ o há» trá»£ cho nhá»¯ng ngÆ°á»i bá» táº­t nguyá»n á» Cambridge, bao gá»m viá»c nuÃ´i cÃ¡c sinh viÃªn tÃ n táº­t trong trÆ°á»ng',\n",
              " 'uit_000625': 'chá» cÃ²n gia ÄÃ¬nh vÃ  nhá»¯ng ngÆ°á»i báº¡n thÃ¢n nháº¥t hiá»u ÄÆ°á»£c Ã´ng',\n",
              " 'uit_000626': 'Äá» giao tiáº¿p vá»i nhá»¯ng ngÆ°á»i khÃ¡c, ai ÄÃ³ hiá»u rÃµ sáº½ dá»ch lá»i Ã´ng cho ngÆ°á»i kia',\n",
              " 'uit_000627': 'Bá» khÃ­ch Äá»ng tá»« má»t cuá»c tranh luáº­n vá»i Äáº¡i há»c vá» viá»c ai sáº½ tráº£ tiá»n cho cÃ¡c bá» dá»c thoáº£i Äá» Ã´ng cÃ³ thá» Äi xe lÄn tá»i chá» lÃ m',\n",
              " 'uit_000628': 'trong khi muá»n giÃºp Äá»¡ ngÆ°á»i khÃ¡c, Ã´ng tÃ¬m cÃ¡ch tÃ¡ch báº£n thÃ¢n ra khá»i chuyá»n bá»nh táº­t vÃ  cÃ¡c khÃ³ khÄn cá»§a nÃ³',\n",
              " 'uit_000629': 'vai trÃ² cá»§a mÃ¬nh nhÆ° má»t ngÆ°á»i bÃªnh vá»±c cho quyá»n cá»§a ngÆ°á»i tÃ n táº­t',\n",
              " 'uit_000630': 'vai trÃ² cá»§a mÃ¬nh nhÆ° má»t ngÆ°á»i bÃªnh vá»±c cho quyá»n cá»§a ngÆ°á»i tÃ n táº­t',\n",
              " 'uit_000631': 'gháº¿ GiÃ¡o sÆ° ToÃ¡n há»c Lucas',\n",
              " 'uit_000632': 'má»t vá» trÃ­ danh tiáº¿ng hÃ ng Äáº§u á» Äáº¡i há»c Cambridge cÅ©ng nhÆ° trÃªn tháº¿ giá»i',\n",
              " 'uit_000633': 'tá»«ng lÃ  vá» trÃ­ cá»§a Isaac Newton vÃ  Paul Dirac',\n",
              " 'uit_000634': 'Viá»c Ã´ng thiáº¿u dáº¥n thÃ¢n vÃ o cuá»c Äáº¥u tranh',\n",
              " 'uit_000635': 'trong khi muá»n giÃºp Äá»¡ ngÆ°á»i khÃ¡c, Ã´ng tÃ¬m cÃ¡ch tÃ¡ch báº£n thÃ¢n ra khá»i chuyá»n bá»nh táº­t vÃ  cÃ¡c khÃ³ khÄn cá»§a nÃ³',\n",
              " 'uit_000636': 'suy nghÄ© theo trá»±c giÃ¡c vÃ  Æ°á»c ÄoÃ¡n hÆ¡n lÃ  nháº¥n máº¡nh vÃ o cÃ¡c phÃ©p chá»©ng minh toÃ¡n há»c',\n",
              " 'uit_000637': 'suy nghÄ© theo trá»±c giÃ¡c vÃ  Æ°á»c ÄoÃ¡n hÆ¡n lÃ  nháº¥n máº¡nh vÃ o cÃ¡c phÃ©p chá»©ng minh toÃ¡n há»c',\n",
              " 'uit_000638': 'Is the end in sight for Theoretical Physics',\n",
              " 'uit_000639': 'dÃ¹ ráº¥t miá»n cÆ°á»¡ng, má»t vÃ i dá»ch vá»¥ Äiá»u dÆ°á»¡ng táº¡i gia',\n",
              " 'uit_000640': 'SiÃªu háº¥p dáº«n N=8 nhÆ° lÃ½ thuyáº¿t hÃ ng Äáº§u nháº±m giáº£i quyáº¿t nhiá»u bÃ i toÃ¡n ná»i báº­t mÃ  cÃ¡c nhÃ  váº­t lÃ½ Äang nghiÃªn cá»©u',\n",
              " 'uit_000641': 'má»t vÃ i dá»ch vá»¥ Äiá»u dÆ°á»¡ng táº¡i gia',\n",
              " 'uit_000642': 'thÃ´ng tin cá»§a má»t há» Äen bá» máº¥t khÃ´ng thá» phá»¥c há»i khi má»t há» Äen bá»c hÆ¡i',\n",
              " 'uit_000643': 'Nghá»ch lÃ½ thÃ´ng tin há» Äen nÃ y vi pháº¡m nguyÃªn lÃ½ cÆ¡ báº£n cá»§a cÆ¡ há»c lÆ°á»£ng tá»­',\n",
              " 'uit_000644': 'ThÃ¡ng 12 nÄm 1977',\n",
              " 'uit_000645': 'nhá»¯ng nÄm 1980',\n",
              " 'uit_000646': 'khi hÃ¡t táº¡i má»t dÃ n nháº¡c nhÃ  thá»',\n",
              " 'uit_000647': 'Jane vÃ  Hellyer Jones quyáº¿t Äá»nh khÃ´ng phÃ¡ vá»¡ gia ÄÃ¬nh vÃ  má»i quan há» cá»§a há» váº«n giá»¯ trong sÃ¡ng trong má»t thá»i gian dÃ i',\n",
              " 'uit_000648': 'giá»¯a nhá»¯ng nÄm 1980',\n",
              " 'uit_000649': 'náº£y ná» tÃ¬nh cáº£m lÃ£ng máº¡n vá»i nhau',\n",
              " 'uit_000650': 'nghiÃªn cá»©u lÃ½ thuyáº¿t lÆ°á»£ng tá»­ má»i',\n",
              " 'uit_000651': 'theo sau Vá»¥ Ná» Lá»n vÅ© trá»¥ ban Äáº§u má» rá»ng cá»±c ká»³ nhanh chÃ³ng trÆ°á»c khi giáº£m tá»c Äá» thÃ nh má»t sá»± giÃ£n ná» cháº­m hÆ¡n',\n",
              " 'uit_000652': 'vÅ© trá»¥ cÃ³ thá» khÃ´ng cÃ³ biÃªn-khÃ´ng cÃ³ Äiá»m Äáº§u hay Äiá»m cuá»i',\n",
              " 'uit_000653': 'theo sau Vá»¥ Ná» Lá»n vÅ© trá»¥ ban Äáº§u má» rá»ng cá»±c ká»³ nhanh chÃ³ng trÆ°á»c khi giáº£m tá»c Äá» thÃ nh má»t sá»± giÃ£n ná» cháº­m hÆ¡n',\n",
              " 'uit_000654': 'nghiÃªn cá»©u lÃ½ thuyáº¿t lÆ°á»£ng tá»­',\n",
              " 'uit_000655': 'vÅ© trá»¥ cÃ³ thá» khÃ´ng cÃ³ biÃªn-khÃ´ng cÃ³ Äiá»m Äáº§u hay Äiá»m cuá»i',\n",
              " 'uit_000656': 'VÅ© trá»¥ NguyÃªn thá»§y',\n",
              " 'uit_000657': 'Náº¿u vÅ© trá»¥ khÃ´ng cÃ³ biÃªn mÃ  tá»± bao bá»c... thÃ¬ ChÃºa sáº½ khÃ´ng cÃ³ báº¥t ká»³ tá»± do lá»±a chá»n nÃ o vá» viá»c vÅ© trá»¥ báº¯t Äáº§u ra sao',\n",
              " 'uit_000658': 'ÄÃ³ lÃ  Äiá»m mÃ  táº¥t cáº£ cÃ¡c ÄÆ°á»ng kinh tuyáº¿n hÆ°á»ng vá» phÃ­a báº¯c gáº·p nhau vÃ  káº¿t thÃºc',\n",
              " 'uit_000659': 'ÄÆ°á»£c thay tháº¿ báº±ng má»t vÃ¹ng tÆ°Æ¡ng tá»± nhÆ° Báº¯c Cá»±c',\n",
              " 'uit_000660': 'James Hartle',\n",
              " 'uit_000661': 'khÃ´ng cÃ³ biÃªn trong khÃ´ng-thá»i gian',\n",
              " 'uit_000662': 'ÄÃ³ lÃ  Äiá»m mÃ  táº¥t cáº£ cÃ¡c ÄÆ°á»ng kinh tuyáº¿n hÆ°á»ng vá» phÃ­a báº¯c gáº·p nhau vÃ  káº¿t thÃºc',\n",
              " 'uit_000663': 'James Hartle',\n",
              " 'uit_000664': 'khÃ´ng cáº§n thiáº¿t Äá» giáº£i thÃ­ch nguá»n gá»c cá»§a vÅ© trá»¥',\n",
              " 'uit_000665': 'má»t vÅ© trá»¥ má»',\n",
              " 'uit_000666': 'sá»± tá»n táº¡i cá»§a má»t Äáº¥ng SÃ¡ng Tháº¿',\n",
              " 'uit_000667': 'nÃ³ cÅ©ng tÆ°Æ¡ng thÃ­ch vá»i má»t vÅ© trá»¥ má»',\n",
              " 'uit_000668': 'nÄm 1981 Ã´ng nháº­n Huy chÆ°Æ¡ng Franklin, vÃ  nÄm 1982 nháº­n tÆ°á»c CBE (má»t tÆ°á»c báº­c hiá»p sÄ© háº¡ng tháº¥p cá»§a Äáº¿ quá»c Anh)',\n",
              " 'uit_000669': 'tÆ°á»c CBE',\n",
              " 'uit_000670': 'vÃ o lÃºc vÅ© trá»¥ ngá»«ng dÃ£n ná» vÃ  cuá»i cÃ¹ng suy sá»¥p, thá»i gian sáº½ cháº¡y theo hÆ°á»ng ngÆ°á»£c láº¡i',\n",
              " 'uit_000671': 'cÃ´ng bá» cá»§a Don Page vÃ  Raymond Laflamme',\n",
              " 'uit_000672': 'vÃ o lÃºc vÅ© trá»¥ ngá»«ng dÃ£n ná» vÃ  cuá»i cÃ¹ng suy sá»¥p, thá»i gian sáº½ cháº¡y theo hÆ°á»ng ngÆ°á»£c láº¡i',\n",
              " 'uit_000673': 'A Brief History of Time',\n",
              " 'uit_000674': 'Bantam Books',\n",
              " 'uit_000675': 'nháº­n má»t khoáº£n tiá»n Äáº·t cá»c lá»n cho tÃ¡c pháº©m',\n",
              " 'uit_000676': 'Bantam Books',\n",
              " 'uit_000677': 'A Brief History of Time',\n",
              " 'uit_000678': 'thanh toÃ¡n hÃ³a ÄÆ¡n, nÃªn dÆ°á»i nhu cáº§u trang tráº£i chi phÃ­ viá»c há»c hÃ nh cá»§a con cÃ¡i vÃ  sinh hoáº¡t gia ÄÃ¬nh',\n",
              " 'uit_000679': 'CÃ¡c y tÃ¡ ÄÆ°á»£c thuÃª suá»t ba ca Äá» chÄm sÃ³c Ã´ng hai mÆ°Æ¡i bá»n tiáº¿ng Äá»ng há» má»i ngÃ y',\n",
              " 'uit_000680': 'cÃ³ thá» Äe dá»a tÃ­nh máº¡ng',\n",
              " 'uit_000681': 'CÆ¡ quan ChÄm sÃ³c Sá»©c khá»e Anh',\n",
              " 'uit_000682': 'chÄm sÃ³c Äiá»u dÆ°á»¡ng suá»t ngÃ y ÄÃªm vÃ  loáº¡i bá» nÄng lá»±c phÃ¡t Ã¢m Ã­t á»i cÃ²n láº¡i cá»§a Ã´ng',\n",
              " 'uit_000683': 'chÄm sÃ³c Ã´ng hai mÆ°Æ¡i bá»n tiáº¿ng Äá»ng há» má»i ngÃ y',\n",
              " 'uit_000684': 'má»t quá»¹ á» Hoa Ká»³',\n",
              " 'uit_000685': 'sá»­ dá»¥ng má»t cÃ´ng táº¯c Ã´ng chá»n cÃ¡c cá»¥m tá»«, tá»«, hoáº·c chá»¯ cÃ¡i tá»« má»t bá» nhá» chá»©a khoáº£ng 2500-3000 lá»±a chá»n ÄÆ°á»£c quÃ©t qua bá»i mÃ¡y',\n",
              " 'uit_000686': 'Giá» tÃ´i ÄÃ¢m ra giao tiáº¿p tá»t hÆ¡n lÃ  trÆ°á»c khi tÃ´i máº¥t giá»ng nÃ³i',\n",
              " 'uit_000687': 'Equalizer',\n",
              " 'uit_000688': 'Giá» tÃ´i ÄÃ¢m ra giao tiáº¿p tá»t hÆ¡n lÃ  trÆ°á»c khi tÃ´i máº¥t giá»ng nÃ³i',\n",
              " 'uit_000689': 'sá»­ dá»¥ng má»t cÃ´ng táº¯c Ã´ng chá»n cÃ¡c cá»¥m tá»«, tá»«, hoáº·c chá»¯ cÃ¡i tá»« má»t bá» nhá» chá»©a khoáº£ng 2500-3000 lá»±a chá»n ÄÆ°á»£c quÃ©t qua bá»i mÃ¡y',\n",
              " 'uit_000690': 'thÃºc Äáº©y Ã´ng pháº£i giáº£i thÃ­ch cÃ¡c Ã½ tÆ°á»ng má»t cÃ¡ch rÃµ rÃ ng trong ngÃ´n ngá»¯ khÃ´ng mang tÃ­nh ká»¹ thuáº­t',\n",
              " 'uit_000691': 'yÃªu cáº§u trá»£ lÃ½ giÃºp Ã´ng hoÃ n thÃ nh viá»c viáº¿t \"LÆ°á»£c sá»­ Thá»i gian\"',\n",
              " 'uit_000692': 'trá» thÃ nh má»t thÃ nh cÃ´ng phi thÆ°á»ng, nhanh chÃ³ng vÆ°Æ¡n lÃªn Äáº§u cÃ¡c danh sÃ¡ch bÃ¡n cháº¡y nháº¥t á» cáº£ hai quá»c gia vÃ  duy trÃ¬ vá» trÃ­ khÃ´ng chá» nhiá»u tuáº§n mÃ  nhiá»u nÄm liÃªn tá»¥c',\n",
              " 'uit_000693': 'trá» thÃ nh má»t thÃ nh cÃ´ng phi thÆ°á»ng, nhanh chÃ³ng vÆ°Æ¡n lÃªn Äáº§u cÃ¡c danh sÃ¡ch bÃ¡n cháº¡y nháº¥t á» cáº£ hai quá»c gia vÃ  duy trÃ¬ vá» trÃ­ khÃ´ng chá» nhiá»u tuáº§n mÃ  nhiá»u nÄm liÃªn tá»¥c',\n",
              " 'uit_000694': 'yÃªu cáº§u trá»£ lÃ½ giÃºp Ã´ng hoÃ n thÃ nh viá»c viáº¿t \"LÆ°á»£c sá»­ Thá»i gian\"',\n",
              " 'uit_000695': 'ÄÆ°á»£c dá»ch sang nhiá»u thá»© tiáº¿ng, vÃ  tá»i nÄm 2009 bÃ¡n ÄÆ°á»£c Ã­t nháº¥t 9 triá»u báº£n',\n",
              " 'uit_000696': 'Master of the Universe',\n",
              " 'uit_000697': '\"LÆ°á»£c sá»­ Thá»i gian\" ÄÆ°á»£c dá»ch sang nhiá»u thá»© tiáº¿ng, vÃ  tá»i nÄm 2009 bÃ¡n ÄÆ°á»£c Ã­t nháº¥t 9 triá»u báº£n',\n",
              " 'uit_000698': 'Hawking ÄÃ£ du hÃ nh liÃªn tá»¥c Äá» quáº£ng bÃ¡ cÃ´ng trÃ¬nh cá»§a mÃ¬nh, vÃ  tham gia tiá»c tÃ¹ng vÃ  khiÃªu vÅ© tá»i táº­n ÄÃªm khuya',\n",
              " 'uit_000699': 'trong vai trÃ² ngÆ°á»i ná»i tiáº¿ng',\n",
              " 'uit_000700': 'nÄm báº±ng tiáº¿n sÄ© danh dá»±, Huy chÆ°Æ¡ng VÃ ng cá»§a Há»i ThiÃªn vÄn há»c HoÃ ng gia (1985), Huy chÆ°Æ¡ng Paul Dirac (1987) vÃ , cÃ¹ng vá»i Penrose, Giáº£i Wolf danh tiáº¿ng (1988). NÄm 1989, Ã´ng ÄÆ°á»£c Ná»¯ hoÃ ng Elizabeth II phong tÆ°á»c CH',\n",
              " 'uit_000701': 'Ã­t cÃ³ thá»i gian dÃ nh cho cÃ´ng viá»c vÃ  cÃ¡c há»c trÃ²',\n",
              " 'uit_000702': 'tÆ°á»c hiá»u dÃ¢n sá»± cao thá»© hai mÃ  má»t bÃ¬nh dÃ¢n Anh cÃ³ thá» Äáº¡t ÄÆ°á»£c, tháº¥p hÆ¡n HuÃ¢n chÆ°Æ¡ng CÃ´ng tráº¡ng-OM',\n",
              " 'uit_000703': 'khiáº¿n Ã´ng Ã­t cÃ³ thá»i gian dÃ nh cho cÃ´ng viá»c vÃ  cÃ¡c há»c trÃ²',\n",
              " 'uit_000704': 'chá»§ yáº¿u lÃ  do sá»± tÃ n táº­t cá»§a Ã´ng',\n",
              " 'uit_000705': 'sá»± tÃ n táº­t cá»§a Ã´ng',\n",
              " 'uit_000706': 'Quan Äiá»m báº¥t kháº£ tri vá» tÃ´n giÃ¡o cá»§a Hawking cÅ©ng tÆ°Æ¡ng pháº£n vá»i Äá»©c tin Ki tÃ´ giÃ¡o máº¡nh máº½ cá»§a ngÆ°á»i vá»£',\n",
              " 'uit_000707': 'Cuá»c hÃ´n nhÃ¢n giá»¯a Jane vÃ  Stephen Hawking',\n",
              " 'uit_000708': 'gÃ¢y thÃ¡ch thá»©c cho cÃ¡c Äá»ng nghiá»p vÃ  thÃ nh viÃªn gia ÄÃ¬nh',\n",
              " 'uit_000709': 'gÃ¢y thÃ¡ch thá»©c cho cÃ¡c Äá»ng nghiá»p vÃ  thÃ nh viÃªn gia ÄÃ¬nh',\n",
              " 'uit_000710': 'Äá» thÄm NhÃ n',\n",
              " 'uit_000711': 'nÄm 1990',\n",
              " 'uit_000712': 'Hawking trá» nÃªn ngÃ y cÃ ng gáº§n gÅ©i vá»i má»t trong sá» cÃ¡c y tÃ¡ cá»§a Ã´ng, Elaine Mason',\n",
              " 'uit_000713': 'nháº­n má»t cÃ´ gÃ¡i Viá»t Nam sá»ng á» LÃ ng tráº» em SOS tÃªn lÃ  Nguyá»n Thá» Thu NhÃ n lÃ m con nuÃ´i',\n",
              " 'uit_000714': 'Äá» xuáº¥t cá»§a Penrose vá» má»t \"phá»ng ÄoÃ¡n kiá»m duyá»t vÅ© trá»¥\"',\n",
              " 'uit_000715': 'Äá» xuáº¥t cá»§a Penrose vá» má»t \"phá»ng ÄoÃ¡n kiá»m duyá»t vÅ© trá»¥\"-ráº±ng khÃ´ng thá» nÃ o cÃ³ \"kÃ¬ dá» tráº§n truá»ng\" khÃ´ng che bá»i má»t chÃ¢n trá»i-lÃ  ÄÃºng',\n",
              " 'uit_000716': 'Gary Gibbons',\n",
              " 'uit_000717': 'Gary Gibbons',\n",
              " 'uit_000718': 'há» Äen vÃ  Vá»¥ Ná» Lá»n',\n",
              " 'uit_000719': 'quan niá»m vá» há» Äen cho bá»i thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t',\n",
              " 'uit_000720': 'theo hÆ°á»ng ngÆ°á»£c láº¡i, ráº±ng cÆ¡ há»c lÆ°á»£ng tá»­ Äá» xuáº¥t ráº±ng thÃ´ng tin phÃ¡t ra bá»i má»t há» Äen liÃªn quan tá»i thÃ´ng tin rÆ¡i vÃ o nÃ³ á» má»t thá»i Äiá»m trÆ°á»c Äáº¥y, quan niá»m vá» há» Äen cho bá»i thuyáº¿t tÆ°Æ¡ng Äá»i tá»ng quÃ¡t pháº£i ÄÆ°á»£c hiá»u chá»nh theo má»t cÃ¡ch nÃ o ÄÃ³',\n",
              " 'uit_000721': 'cÆ¡ há»c lÆ°á»£ng tá»­ Äá» xuáº¥t ráº±ng thÃ´ng tin phÃ¡t ra bá»i má»t há» Äen liÃªn quan tá»i thÃ´ng tin rÆ¡i vÃ o nÃ³ á» má»t thá»i Äiá»m trÆ°á»c Äáº¥y',\n",
              " 'uit_000722': 'nghá»ch lÃ½ thÃ´ng tin há» Äen',\n",
              " 'uit_000723': 'nghá»ch lÃ½ thÃ´ng tin há» Äen',\n",
              " 'uit_000724': 'Äem khoa há»c tá»i má»t lá»p cÃ´ng chÃºng rá»ng rÃ£i hÆ¡n',\n",
              " 'uit_000725': 'khoa há»c',\n",
              " 'uit_000726': 'Há» Äen vÃ  cÃ¡c VÅ© trá»¥ SÆ¡ sinh vÃ  nhá»¯ng Tiá»u luáº­n khÃ¡c',\n",
              " 'uit_000727': 'khoa há»c',\n",
              " 'uit_000728': 'Leonard Nimoy, ngÆ°á»i ÄÃ³ng vai Spock trong Star Trek, biáº¿t ÄÆ°á»£c ráº±ng Hawking há»©ng thÃº vá»i viá»c xuáº¥t hiá»n trong chÆ°Æ¡ng trÃ¬nh',\n",
              " 'uit_000729': 'biáº¿t ÄÆ°á»£c ráº±ng Hawking há»©ng thÃº vá»i viá»c xuáº¥t hiá»n trong chÆ°Æ¡ng trÃ¬nh',\n",
              " 'uit_000730': 'Keep Talking',\n",
              " 'uit_000731': 'sÃª-ri hÃ i ká»ch tÃ¬nh huá»ng The Simpsons',\n",
              " 'uit_000732': 'xuáº¥t hiá»n nÄm 1999 trong sÃª-ri hÃ i ká»ch tÃ¬nh huá»ng The Simpsons',\n",
              " 'uit_000733': 'má»t vÅ© trá»¥ há»c \"trÃªn-xuá»ng\", phÃ¡t biá»u ráº±ng vÅ© trá»¥ khÃ´ng pháº£i cÃ³ má»t tráº¡ng thÃ¡i ban Äáº§u duy nháº¥t mÃ  lÃ  nhiá»u tráº¡ng thÃ¡i, vÃ  do ÄÃ³ lÃ  khÃ´ng thÃ­ch há»£p Äá» hÃ¬nh thÃ nh má»t lÃ½ thuyáº¿t tiÃªn ÄoÃ¡n hÃ¬nh dáº¡ng hiá»n táº¡i cá»§a vÅ© trá»¥ tá»« má»t tráº¡ng thÃ¡i ban Äáº§u Äáº·c biá»t nÃ o',\n",
              " 'uit_000734': 'phÃ¡t triá»n má»t vÅ© trá»¥ há»c \"trÃªn-xuá»ng\"',\n",
              " 'uit_000735': 'VÅ© trá»¥ trong vá» háº¡t dáº»',\n",
              " 'uit_000736': 'tiÃªn ÄoÃ¡n hÃ¬nh dáº¡ng hiá»n táº¡i cá»§a vÅ© trá»¥ tá»« má»t tráº¡ng thÃ¡i ban Äáº§u Äáº·c biá»t nÃ o',\n",
              " 'uit_000737': 'thiáº¿u nhi',\n",
              " 'uit_000738': 'nÄm 2006',\n",
              " 'uit_000739': 'thiáº¿u nhi',\n",
              " 'uit_000740': 'Má»t phiÃªn báº£n hiá»u chá»nh cá»§a cuá»n sÃ¡ch trÆ°á»c ÄÃ¢y cá»§a Jane, nay mang tÃªn má»i \"HÃ nh trÃ¬nh tá»i VÃ´ háº¡n, Cuá»c Äá»i tÃ´i vá»i Stephen\", xuáº¥t hiá»n nÄm 2007',\n",
              " 'uit_000741': 'trÃ¬nh bÃ y váº­t lÃ½ lÃ½ thuyáº¿t theo cÃ¡ch dá» hiá»u vÃ  mÃ´ táº£ cÃ¡c nhÃ¢n váº­t tÆ°Æ¡ng tá»± cÃ¡c thÃ nh viÃªn gia ÄÃ¬nh Hawking',\n",
              " 'uit_000742': 'nÄm 2006',\n",
              " 'uit_000743': 'phiÃªn báº£n hiá»u chá»nh cá»§a cuá»n sÃ¡ch trÆ°á»c ÄÃ¢y cá»§a Jane, nay mang tÃªn má»i \"HÃ nh trÃ¬nh tá»i VÃ´ háº¡n, Cuá»c Äá»i tÃ´i vá»i Stephen\"',\n",
              " 'uit_000744': 'phim tÃ i liá»u cÃ³ tÃªn The Real Stephen Hawking: (2001) vÃ  \"Stephen Hawking: Profile\" (2002), má»t phim truyá»n hÃ¬nh Hawking vá» giai Äoáº¡n báº¯t Äáº§u cÄn bá»nh cá»§a Hawking (2004), cÃ¹ng má»t sÃª-ri phim tÃ i liá»u Stephen Hawking, Master of the Universe (2008)',\n",
              " 'uit_000745': 'mÃ¡y bay pháº£n lá»±c cÃ¡ nhÃ¢n',\n",
              " 'uit_000746': 'Chile, Äáº£o Phá»¥c Sinh, Nam Phi, rá»i TÃ¢y Ban Nha (Äá» nháº­n Giáº£i Fonseca nÄm 2008), Canada vÃ  nhiá»u chuyáº¿n Äi tá»i Hoa Ká»³',\n",
              " 'uit_000747': 'Chile, Äáº£o Phá»¥c Sinh, Nam Phi, rá»i TÃ¢y Ban Nha (Äá» nháº­n Giáº£i Fonseca nÄm 2008), Canada vÃ  nhiá»u chuyáº¿n Äi tá»i Hoa Ká»³',\n",
              " 'uit_000748': 'liÃªn quan tá»i sá»± tÃ n táº­t cá»§a Ã´ng',\n",
              " 'uit_000749': 'má»t loáº¡t tuyÃªn bá» gÃ¢y chÃº Ã½ vÃ  thÆ°á»ng gÃ¢y tranh cÃ£i',\n",
              " 'uit_000750': 'sá»± sá»ng trÃªn TrÃ¡i Äáº¥t bá» Äe dá»a do \"má»t cuá»c chiáº¿n tranh háº¡t nhÃ¢n Äá»t ngá»t, má»t virus ÄÆ°á»£c láº­p trÃ¬nh gien hay cÃ¡c má»i hiá»m há»a mÃ  chÃºng ta cÃ²n chÆ°a nghÄ© tá»i\"',\n",
              " 'uit_000751': 'má»t cuá»c chiáº¿n tranh háº¡t nhÃ¢n Äá»t ngá»t, má»t virus ÄÆ°á»£c láº­p trÃ¬nh gien hay cÃ¡c má»i hiá»m há»a mÃ  chÃºng ta cÃ²n chÆ°a nghÄ© tá»i',\n",
              " 'uit_000752': 'Ã´ng tá»«ng kháº³ng Äá»nh ráº±ng virus mÃ¡y tÃ­nh lÃ  má»t dáº¡ng sá»± sá»ng, ráº±ng con ngÆ°á»i nÃªn sá»­ dá»¥ng kÄ© thuáº­t di truyá»n Äá» trÃ¡nh khá»i bá» vÆ°á»£t máº·t bá»i mÃ¡y tÃ­nh, vÃ  ráº±ng ngÆ°á»i ngoÃ i hÃ nh tinh cÃ³ láº½ tá»n táº¡i vÃ  cáº§n trÃ¡nh giao tiáº¿p vá»i há» vÃ¬ há» cÃ³ thá» sáº½ chinh pháº¡t con ngÆ°á»i',\n",
              " 'uit_000753': 'Mong muá»n tÄng cÆ°á»ng má»i quan tÃ¢m cá»§a cÃ´ng chÃºng tá»i cÃ¡c chuyáº¿n bay ra ngoÃ i khÃ´ng gian vÃ  thá» hiá»n tiá»m nÄng cá»§a nhá»¯ng ngÆ°á»i tÃ n táº­t',\n",
              " 'uit_000754': 'cÃ¡c chuyáº¿n bay ra ngoÃ i khÃ´ng gian vÃ  thá» hiá»n tiá»m nÄng cá»§a nhá»¯ng ngÆ°á»i tÃ n táº­t',\n",
              " 'uit_000755': 'Ã´ng tá»«ng kháº³ng Äá»nh ráº±ng virus mÃ¡y tÃ­nh lÃ  má»t dáº¡ng sá»± sá»ng, ráº±ng con ngÆ°á»i nÃªn sá»­ dá»¥ng kÄ© thuáº­t di truyá»n Äá» trÃ¡nh khá»i bá» vÆ°á»£t máº·t bá»i mÃ¡y tÃ­nh, vÃ  ráº±ng ngÆ°á»i ngoÃ i hÃ nh tinh cÃ³ láº½ tá»n táº¡i vÃ  cáº§n trÃ¡nh giao tiáº¿p vá»i há» vÃ¬ há» cÃ³ thá» sáº½ chinh pháº¡t con ngÆ°á»i',\n",
              " 'uit_000756': 'cÃ¡c chuyáº¿n bay khÃ´ng gian vÃ  viá»c láº­p thuá»c Äá»a ngoÃ i vÅ© trá»¥',\n",
              " 'uit_000757': 'cÃ¡c chuyáº¿n bay khÃ´ng gian vÃ  viá»c láº­p thuá»c Äá»a ngoÃ i vÅ© trá»¥',\n",
              " 'uit_000758': 'bÃ y tá» sá»± á»§ng há» vá»i á»©ng cá»­ viÃªn DÃ¢n chá»§ Al Gore trong cuá»c báº§u cá»­ Tá»ng thá»ng Hoa Ká»³ nÄm 2000, gá»i Cuá»c táº¥n cÃ´ng Iraq 2003 lÃ  má»t \"tá»i Ã¡c chiáº¿n tranh\", táº©y chay má»t há»i tháº£o á» Israel do lo ngáº¡i vá» chÃ­nh sÃ¡ch cá»§a Israel Äá»i vá»i ngÆ°á»i Palestine, duy trÃ¬ chiáº¿n dá»ch lÃ¢u dÃ i cá»§a Ã´ng váº­n Äá»ng giáº£i trá»« vÅ© khÃ­ háº¡t nhÃ¢n, vÃ  á»§ng há» nghiÃªn cá»©u táº¿ bÃ o gá»c, há» thá»ng y táº¿ toÃ n cáº§u, vÃ  hÃ nh Äá»ng ngÄn cháº·n biáº¿n Äá»i khÃ­ háº­u',\n",
              " 'uit_000759': 'bÃ y tá» sá»± á»§ng há» vá»i á»©ng cá»­ viÃªn DÃ¢n chá»§ Al Gore trong cuá»c báº§u cá»­ Tá»ng thá»ng Hoa Ká»³ nÄm 2000, gá»i Cuá»c táº¥n cÃ´ng Iraq 2003 lÃ  má»t \"tá»i Ã¡c chiáº¿n tranh\", táº©y chay má»t há»i tháº£o á» Israel do lo ngáº¡i vá» chÃ­nh sÃ¡ch cá»§a Israel Äá»i vá»i ngÆ°á»i Palestine, duy trÃ¬ chiáº¿n dá»ch lÃ¢u dÃ i cá»§a Ã´ng váº­n Äá»ng giáº£i trá»« vÅ© khÃ­ háº¡t nhÃ¢n, vÃ  á»§ng há» nghiÃªn cá»©u táº¿ bÃ o gá»c, há» thá»ng y táº¿ toÃ n cáº§u, vÃ  hÃ nh Äá»ng ngÄn cháº·n biáº¿n Äá»i khÃ­ háº­u',\n",
              " 'uit_000760': 'má»t chiáº¿c xe lÄn, National Savings, British Telecom, Specsavers, Egg Banking, vÃ  Go Compare',\n",
              " 'uit_000761': 'Äáº£ng Lao Äá»ng',\n",
              " 'uit_000762': 'Äáº£ng Lao Äá»ng',\n",
              " 'uit_000763': 'National Savings, British Telecom, Specsavers, Egg Banking, vÃ  Go Compare',\n",
              " 'uit_000764': 'sá»± máº¥t mÃ¡t thÃ´ng tin cá»§a má»t há» Äen',\n",
              " 'uit_000765': 'cÃ¡c há» Äen cÃ³ nhiá»u hÆ¡n má»t tÃ´ pÃ´',\n",
              " 'uit_000766': 'Ã´ng láº­p luáº­n ráº±ng nghá»ch lÃ½ thÃ´ng tin ÄÆ°á»£c giáº£i thÃ­ch báº±ng cÃ¡ch kiá»m tra táº¥t cáº£ nhá»¯ng lá»ch sá»­ tÆ°Æ¡ng ÄÆ°Æ¡ng cá»§a vÅ© trá»¥, vá»i máº¥t mÃ¡t thÃ´ng tin trong nhá»¯ng vÅ© trá»¥ cÃ³ há» Äen sáº½ ÄÆ°á»£c triá»t tiÃªu bá»i nhá»¯ng vÅ© trá»¥ khÃ´ng cÃ³',\n",
              " 'uit_000767': 'nghá»ch lÃ½ thÃ´ng tin ÄÆ°á»£c giáº£i thÃ­ch báº±ng cÃ¡ch kiá»m tra táº¥t cáº£ nhá»¯ng lá»ch sá»­ tÆ°Æ¡ng ÄÆ°Æ¡ng cá»§a vÅ© trá»¥, vá»i máº¥t mÃ¡t thÃ´ng tin trong nhá»¯ng vÅ© trá»¥ cÃ³ há» Äen sáº½ ÄÆ°á»£c triá»t tiÃªu bá»i nhá»¯ng vÅ© trá»¥ khÃ´ng cÃ³',\n",
              " 'uit_000768': 'Hawking nhanh chÃ³ng thá»«a nháº­n thua cuá»c vÃ  nÃ³i ráº±ng Higgs nÃªn nháº­n ÄÆ°á»£c Giáº£i Nobel Váº­t lÃ½',\n",
              " 'uit_000769': 'vá» tháº¿ ná»i tiáº¿ng cá»§a Hawking Äem láº¡i cho Ã´ng ta sá»± tin cáº­y mÃ  ngÆ°á»i khÃ¡c khÃ´ng cÃ³',\n",
              " 'uit_000770': 'vá» tháº¿ ná»i tiáº¿ng cá»§a Hawking Äem láº¡i cho Ã´ng ta sá»± tin cáº­y mÃ  ngÆ°á»i khÃ¡c khÃ´ng cÃ³',\n",
              " 'uit_000771': 'thÃ¡ng 7 nÄm 2012',\n",
              " 'uit_000772': 'cÃ´ng khai vá» váº¥n Äá» nÃ y nÄm 2002 vÃ  tiáº¿p tá»¥c nÄm 2008, trong ÄÃ³ Higgs chá» trÃ­ch cÃ´ng trÃ¬nh cá»§a Hawking',\n",
              " 'uit_000773': 'Hawking ÄÃ£ kháº³ng Äá»nh dá»©t khoÃ¡t, vÃ  ÄÃ¡nh cÆ°á»£c, ráº±ng sáº½ khÃ´ng bao giá» tÃ¬m tháº¥y ÄÆ°á»£c Boson Higgs',\n",
              " 'uit_000774': 'Hawking ÄÃ£ kháº³ng Äá»nh dá»©t khoÃ¡t, vÃ  ÄÃ¡nh cÆ°á»£c, ráº±ng sáº½ khÃ´ng bao giá» tÃ¬m tháº¥y ÄÆ°á»£c Boson Higgs',\n",
              " 'uit_000775': 'tÃ¬nh tráº¡ng váº«n tá»nh tÃ¡o nhÆ°ng khÃ´ng thá» cá»­ Äá»ng báº¥t cá»© bá» pháº­n nÃ o ngoÃ i máº¯t',\n",
              " 'uit_000776': 'Ã´ng Äang há»£p tÃ¡c vá»i cÃ¡c nhÃ  nghiÃªn cá»©u vá» cÃ¡c há» thá»ng cÃ³ thá» diá»n dá»ch cÃ¡c hÃ¬nh áº£nh nÃ£o bá» hoáº·c biá»u diá»n nÃ©t máº·t thÃ nh phÆ°Æ¡ng thá»©c kÃ­ch hoáº¡t cÃ´ng táº¯c',\n",
              " 'uit_000777': 'Ã´ng báº¯t Äáº§u pháº£i Äiá»u khiá»n thiáº¿t bá» giao tiáº¿p báº±ng cá»­ Äá»ng cá»§a cÆ¡ mÃ¡ do khÃ´ng thá» sá»­ dá»¥ng tay ná»¯a, vá»i tá»c Äá» chá» má»t tá»« má»i phÃºt',\n",
              " 'uit_000778': 'Huy chÆ°Æ¡ng Copley tá»« Há»i HoÃ ng gia (2006), vinh dá»± dÃ¢n sá»± cao nháº¥t cá»§a Hoa Ká»³-HuÃ¢n chÆ°Æ¡ng Tá»± do Tá»ng thá»ng (2009), vÃ  Giáº£i thÆ°á»ng Váº­t lÃ½ CÆ¡ báº£n Nga (2012)',\n",
              " 'uit_000779': 'pháº£i Äiá»u khiá»n thiáº¿t bá» giao tiáº¿p báº±ng cá»­ Äá»ng cá»§a cÆ¡ mÃ¡ do khÃ´ng thá» sá»­ dá»¥ng tay ná»¯a, vá»i tá»c Äá» chá» má»t tá»« má»i phÃºt',\n",
              " 'uit_000780': 'Huy chÆ°Æ¡ng Copley',\n",
              " 'uit_000781': 'há»£p tÃ¡c vá»i cÃ¡c nhÃ  nghiÃªn cá»©u vá» cÃ¡c há» thá»ng cÃ³ thá» diá»n dá»ch cÃ¡c hÃ¬nh áº£nh nÃ£o bá» hoáº·c biá»u diá»n nÃ©t máº·t thÃ nh phÆ°Æ¡ng thá»©c kÃ­ch hoáº¡t cÃ´ng táº¯c',\n",
              " 'uit_000782': 'vÅ© trá»¥ ÄÆ°á»£c váº­n hÃ nh báº±ng cÃ¡c Äá»nh luáº­t khoa há»c. CÃ¡c Äá»nh luáº­t ÄÃ³ cÃ³ thá» ÄÆ°á»£c ChÃºa Trá»i ban bá», nhÆ°ng ChÃºa khÃ´ng can thiá»p Äá» phÃ¡ vá»¡ chÃºng',\n",
              " 'uit_000783': 'Curiosity trÃªn Discovery Channel',\n",
              " 'uit_000784': 'vÅ© trá»¥ ÄÆ°á»£c váº­n hÃ nh báº±ng cÃ¡c Äá»nh luáº­t khoa há»c',\n",
              " 'uit_000785': 'ThiÃªn ÄÆ°á»ng lÃ  má»t huyá»n thoáº¡i, tin ráº±ng \"khÃ´ng cÃ³ thiÃªn ÄÆ°á»ng hay tháº¿ giá»i bÃªn kia\"',\n",
              " 'uit_000786': 'ThiÃªn ÄÆ°á»ng lÃ  má»t huyá»n thoáº¡i, tin ráº±ng \"khÃ´ng cÃ³ thiÃªn ÄÆ°á»ng hay tháº¿ giá»i bÃªn kia\" vÃ  ráº±ng má»t khÃ¡i niá»m nhÆ° tháº¿ lÃ  \"má»t truyá»n cá» tÃ­ch dÃ nh cho nhá»¯ng ngÆ°á»i sá»£ bÃ³ng tá»i.\"',\n",
              " 'uit_000787': 'Curiosity trÃªn Discovery Channel',\n",
              " 'uit_000788': 'cÃ¡c nhÃ  khoa há»c \"ÄÃ£ trá» thÃ nh ngÆ°á»i mang ngá»n Äuá»c khÃ¡m phÃ¡ trong cuá»c truy táº§m tri thá»©c cá»§a chÃºng ta.\"',\n",
              " 'uit_000789': 'cÃ¡c triáº¿t gia \"khÃ´ng báº¯t ká»p vá»i nhá»¯ng tiáº¿n bá» khoa há»c hiá»n Äáº¡i\"',\n",
              " 'uit_000790': 'Khai sÃ¡ng',\n",
              " 'uit_000791': 'cÃ¡c váº¥n Äá» triáº¿t há»c cÃ³ thá» ÄÆ°á»£c khoa há»c tráº£ lá»i, Äáº·c biá»t lÃ  nhá»¯ng lÃ½ thuyáº¿t khoa há»c má»i \"dáº«n chÃºng ta tá»i má»t bá»©c tranh má»i vÃ  háº¿t sá»©c khÃ¡c biá»t vá» vÅ© trá»¥ vÃ  vá» trÃ­ cá»§a chÃºng ta trong nÃ³\"',\n",
              " 'uit_000792': 'Sá»± dÅ©ng cáº£m, can trÆ°á»ng cá»ng vá»i trÃ­ tuá», khiáº¿u hÃ i hÆ°á»c',\n",
              " 'uit_000793': 'Ãng lÃ  má»t nhÃ  khoa há»c vÃ  lÃ  má»t ngÆ°á»i ÄÃ n Ã´ng tuyá»t vá»i, ngÆ°á»i mÃ  nhá»¯ng cá»ng hiáº¿n vÃ  di sáº£n cá»§a mÃ¬nh sáº½ sá»ng mÃ£i nhiá»u nÄm ná»¯a',\n",
              " 'uit_000794': 'Ãng lÃ  má»t nhÃ  khoa há»c vÃ  lÃ  má»t ngÆ°á»i ÄÃ n Ã´ng tuyá»t vá»i, ngÆ°á»i mÃ  nhá»¯ng cá»ng hiáº¿n vÃ  di sáº£n cá»§a mÃ¬nh sáº½ sá»ng mÃ£i nhiá»u nÄm ná»¯a',\n",
              " 'uit_000795': 'VÅ© trá»¥ sáº½ cháº³ng cÃ³ nhiá»u Ã½ nghÄ©a náº¿u nhÆ° ÄÃ³ khÃ´ng pháº£i lÃ  mÃ¡i nhÃ  chá» che cho nhá»¯ng ngÆ°á»i báº¡n yÃªu thÆ°Æ¡ng',\n",
              " 'uit_000796': 'khoáº£ng cÃ¡ch Äi bá» cá»§a Hawking tá»i Bá» mÃ´n ToÃ¡n há»c á»¨ng dá»¥ng vÃ  Váº­t lÃ½ LÃ½ thuyáº¿t',\n",
              " 'uit_000797': 'thÃ¡ng 4 nÄm 1979',\n",
              " 'uit_000798': 'dá»± cÃ¡c há»i nghá» vÃ  liÃªn quan Äáº¿n váº­t lÃ½',\n",
              " 'uit_000799': 'thÃ¡ng 4 nÄm 1979',\n",
              " 'uit_000800': 'dá»± cÃ¡c há»i nghá» vÃ  liÃªn quan Äáº¿n váº­t lÃ½',\n",
              " 'uit_000801': 'bá»nh táº­t vÃ  nhá»¯ng thÃ¡ch thá»©c vá» cÆ¡ thá» cá»§a mÃ¬nh',\n",
              " 'uit_000802': 'Sá»± tÃ n táº­t cá»§a Ã´ng cÃ³ nghÄ©a lÃ  trÃ¡ch nhiá»m cá»§a gia ÄÃ¬nh Äáº·t trÃªn toÃ n bá» ÄÃ´i vai ngÆ°á»i vá»£ ngÃ y cÃ ng cáº£m tháº¥y quÃ¡ táº£i cá»§a Ã´ng',\n",
              " 'uit_000803': 'bá»nh táº­t vÃ  nhá»¯ng thÃ¡ch thá»©c vá» cÆ¡ thá» cá»§a mÃ¬nh, tháº­m chÃ­ - trong má»t tiá»n lá» ÄÆ°á»£c Äáº·t ra trong thá»i gian tÃ¡n tá»nh Jane',\n",
              " 'uit_000804': 'má»t sinh viÃªn sau Äáº¡i há»c hoáº·c sau tiáº¿n sÄ© Äáº¿n sá»ng vá»i há» vÃ  giÃºp Äá»¡ chÄm sÃ³c Hawking',\n",
              " 'uit_001209': 'KÃ¶niggrÃ¤tz',\n",
              " 'uit_001210': 'thá»ng nháº¥t nÆ°á»c Äá»©c',\n",
              " 'uit_001211': 'nÄm 1870',\n",
              " 'uit_001212': 'ngÃ y 18 thÃ¡ng 1 nÄm 1871',\n",
              " 'uit_001213': 'Thá»§ tÆ°á»ng Otto von Bismarck',\n",
              " 'uit_001214': 'Wilhelm I',\n",
              " 'uit_001215': 'thá»ng nháº¥t nÆ°á»c Äá»©c',\n",
              " 'uit_001216': 'Khoa há»c, cÃ´ng nghá», giÃ¡o dá»¥c',\n",
              " 'uit_001217': 'quÃ¢n sá»± vÃ  kinh táº¿',\n",
              " 'uit_001218': '65 triá»u ngÆ°á»i',\n",
              " 'uit_001219': '65 triá»u ngÆ°á»i',\n",
              " 'uit_001220': 'báº¡i tráº­n trong cuá»c Chiáº¿n tranh tháº¿ giá»i láº§n thá»© nháº¥t',\n",
              " 'uit_001221': 'quÃ¢n sá»± vÃ  kinh táº¿',\n",
              " 'uit_001222': 'máº«u má»±c',\n",
              " 'uit_001223': '65 triá»u ngÆ°á»i',\n",
              " 'uit_001224': 'táº¡o dá»±ng má»t guá»ng mÃ¡y',\n",
              " 'uit_001225': 'má»t vÆ°Æ¡ng quá»c Äá»c láº­p, ÄoÃ n káº¿t vÃ  hÃ¹ng máº¡nh',\n",
              " 'uit_001226': 'Tuyá»n háº§u tÆ°á»c (Elector) Friedrich Wilhelm I',\n",
              " 'uit_001227': '30.000 ngÆ°á»i',\n",
              " 'uit_001228': 'Friedrich Wilhelm I',\n",
              " 'uit_001229': '48 nÄm',\n",
              " 'uit_001230': '48 nÄm',\n",
              " 'uit_001231': 'Friedrich III',\n",
              " 'uit_001232': 'NgÃ y 18/1/1701',\n",
              " 'uit_001233': 'Friedrich II',\n",
              " 'uit_001234': 'náº¿u phong Friedrich lÃ m vua thÃ¬ cÃ¡c Tuyá»n háº§u tÆ°á»c cá»§a Hannover, Bayern vÃ  Sachsen cÅ©ng sáº½ muá»n lÃ m vua',\n",
              " 'uit_001235': 'HoÃ ng Äáº¿ cá»§a Äáº¿ quá»c La MÃ£ Tháº§n thÃ¡nh',\n",
              " 'uit_001236': 'NgÃ y 18/1/1701',\n",
              " 'uit_001237': 'náº¿u phong Friedrich lÃ m vua thÃ¬ cÃ¡c Tuyá»n háº§u tÆ°á»c cá»§a Hannover, Bayern vÃ  Sachsen cÅ©ng sáº½ muá»n lÃ m vua',\n",
              " 'uit_001238': 'Friedrich Wilhelm I',\n",
              " 'uit_001239': 'cÃ¡c cÃ´ng trÃ¬nh kiáº¿n trÃºc xa hoa phung phÃ­',\n",
              " 'uit_001240': 'cÃ¡c cÃ´ng trÃ¬nh kiáº¿n trÃºc xa hoa phung phÃ­',\n",
              " 'uit_001241': 'nhÃ  mÃ¡y lÃ m thuá»c sÃºng, lÃ² ÄÃºc Äáº¡i bÃ¡c, kho vÅ© khÃ­, doanh tráº¡i quÃ¢n Äá»i',\n",
              " 'uit_001242': 'má»¥c tiÃªu cá»§a Phá» lÃ  trá» nÃªn má»t cÆ°á»ng quá»c quÃ¢n sá»±',\n",
              " 'uit_001243': 'Vua Friedrich I',\n",
              " 'uit_001244': 'Friedrich Wilhelm I',\n",
              " 'uit_001245': 'nhÃ  mÃ¡y lÃ m thuá»c sÃºng, lÃ² ÄÃºc Äáº¡i bÃ¡c, kho vÅ© khÃ­, doanh tráº¡i quÃ¢n Äá»i',\n",
              " 'uit_001246': 'má»¥c tiÃªu cá»§a Phá» lÃ  trá» nÃªn má»t cÆ°á»ng quá»c quÃ¢n sá»±',\n",
              " 'uit_001247': 'xÃ¢y dá»±ng chá» vá»i má»¥c ÄÃ­ch quÃ¢n sá»±',\n",
              " 'uit_001248': 'Friedrich Wilhelm I',\n",
              " 'uit_001249': 'tá»nh Silesia',\n",
              " 'uit_001250': 'nÄm 1740',\n",
              " 'uit_001251': 'vÃ i thÃ¡ng',\n",
              " 'uit_001252': 'Äáº¿ quá»c La MÃ£ Tháº§n thÃ¡nh',\n",
              " 'uit_001253': 'Friedrich Äáº¡i Äáº¿',\n",
              " 'uit_001254': 'vÃ i thÃ¡ng',\n",
              " 'uit_001255': 'tá»nh Silesia',\n",
              " 'uit_001256': 'Äáº¿ quá»c La MÃ£ Tháº§n thÃ¡nh',\n",
              " 'uit_001257': 'Friedrich Äáº¡i Äáº¿',\n",
              " 'uit_001258': 'tiá»m lá»±c cá»t lÃµi',\n",
              " 'uit_001259': 'vÆ°Æ¡ng quá»c quÃ¢n sá»± kháº¯c khá»',\n",
              " 'uit_001260': 'chÃ­nh sÃ¡ch ngoáº¡i giao má»m dáº»o, sáºµn sÃ ng liÃªn minh vá»i báº¥t ká»³ tháº¿ lá»±c nÃ o',\n",
              " 'uit_001261': 'nghÃ¨o tÃºng',\n",
              " 'uit_001262': 'khÃ´ng cÃ³ Äáº¥t canh tÃ¡c, sá»ng cuá»c Äá»i vÃ´ cÃ¹ng cá»±c khá»',\n",
              " 'uit_001263': 'tiá»m lá»±c cá»t lÃµi',\n",
              " 'uit_001264': 'Äáº¥t khÃ´ cáº±n, khÃ´ng cÃ³ khoÃ¡ng sáº£n',\n",
              " 'uit_001265': 'chÃ­nh sÃ¡ch ngoáº¡i giao má»m dáº»o, sáºµn sÃ ng liÃªn minh vá»i báº¥t ká»³ tháº¿ lá»±c nÃ o',\n",
              " 'uit_001266': 'phá»¥c tÃ¹ng, lÃ m viá»c vÃ  hy sinh',\n",
              " 'uit_001267': 'má»t Äá»i quÃ¢n cÃ³ quá»c gia',\n",
              " 'uit_001268': 'phá»¥c tÃ¹ng, lÃ m viá»c vÃ  hy sinh',\n",
              " 'uit_001269': 'quÃ¢n vÆ°Æ¡ng',\n",
              " 'uit_001270': 'Äáº§u Ã³c háº¹p hÃ²i vÃ  qua quÃ¢n Äá»i cÃ³ ká»· luáº­t má»t cÃ¡ch tÃ n báº¡o',\n",
              " 'uit_001271': 'Hai pháº§n ba, vÃ  cÃ³ lÃºc nÄm pháº§n sÃ¡u',\n",
              " 'uit_001272': 'nÄm pháº§n sÃ¡u',\n",
              " 'uit_001273': 'cÃ´ng quá»c Schleswig vÃ  Holstein',\n",
              " 'uit_001274': 'tráº­n Sedan',\n",
              " 'uit_001275': 'cÃ´ng quá»c Schleswig vÃ  Holstein',\n",
              " 'uit_001276': 'giáº£i tÃ¡n nghá» viá»n rá»i tá»± huy Äá»ng nguá»n kinh phÃ­',\n",
              " 'uit_001277': 'Ão',\n",
              " 'uit_001278': 'Äan Máº¡ch',\n",
              " 'uit_001279': 'giáº£i tÃ¡n nghá» viá»n rá»i tá»± huy Äá»ng nguá»n kinh phÃ­',\n",
              " 'uit_001280': 'phÃ¡t Äá»ng ba cuá»c chiáº¿n',\n",
              " 'uit_001281': 'Ão',\n",
              " 'uit_001282': 'tráº­n Sedan',\n",
              " 'uit_001283': 'Hanover, Hesse, Nassua, Frankfurt vÃ  Elbe',\n",
              " 'uit_001284': 'chÆ°a Äáº§y 2 thÃ¡ng',\n",
              " 'uit_001285': 'vÆ°Æ¡ng quá»c Bayern',\n",
              " 'uit_001286': 'NapolÃ©on III',\n",
              " 'uit_001287': 'Hanover, Hesse, Nassua, Frankfurt vÃ  Elbe',\n",
              " 'uit_001288': 'nÄm 1870',\n",
              " 'uit_001289': 'vÅ© trang tá»t hÆ¡n, tá» chá»©c cao hÆ¡n vÃ  chiáº¿n Äáº¥u tá»t hÆ¡n',\n",
              " 'uit_001290': 'chá» do ThÆ°á»£ng Äáº¿ trao cho, chá»© khÃ´ng pháº£i tá»« nghá» viá»n hoáº·c qua dÃ¢n chÃºng',\n",
              " 'uit_001291': 'cháº¿ Äá» chuyÃªn cháº¿ quÃ¢n phiá»t',\n",
              " 'uit_001292': 'Äáº¡i diá»n nhÃ¢n dÃ¢n bÃ n cÃ£i cho háº£ dáº¡ hoáº·c máº·c cáº£ quyá»n lá»£i nhá» nhoi cho giai cáº¥p mÃ  há» lÃ m Äáº¡i diá»n',\n",
              " 'uit_001293': 'HoÃ ng Äáº¿ Wilhelm II',\n",
              " 'uit_001294': 'nÆ°á»c Phá»',\n",
              " 'uit_001295': 'chá» do ThÆ°á»£ng Äáº¿ trao cho, chá»© khÃ´ng pháº£i tá»« nghá» viá»n hoáº·c qua dÃ¢n chÃºng',\n",
              " 'uit_001296': 'Äáº¡i diá»n nhÃ¢n dÃ¢n bÃ n cÃ£i cho háº£ dáº¡ hoáº·c máº·c cáº£ quyá»n lá»£i nhá» nhoi cho giai cáº¥p mÃ  há» lÃ m Äáº¡i diá»n',\n",
              " 'uit_001297': 'cháº¿ Äá» chuyÃªn cháº¿ quÃ¢n phiá»t',\n",
              " 'uit_001298': 'cuá»c cÃ¡ch máº¡ng cÃ´ng nghiá»p',\n",
              " 'uit_001299': 'cháº¿ Äá» quÃ¢n phiá»t',\n",
              " 'uit_001300': 'chá»u trÃ¡ch nhiá»m vá»i chÃ­nh Ã´ng',\n",
              " 'uit_001301': 'cháº¿ Äá» chuyÃªn cháº¿ cá»§a vÆ°Æ¡ng triá»u Hohenzollern',\n",
              " 'uit_001302': 'cháº¿ Äá» quÃ¢n chá»§ nghá» viá»n',\n",
              " 'uit_001303': 'ÄÆ°á»£c hÆ°á»ng lá»£i tá»« cuá»c cÃ¡ch máº¡ng cÃ´ng nghiá»p',\n",
              " 'uit_001304': 'cháº¿ Äá» chuyÃªn cháº¿ cá»§a vÆ°Æ¡ng triá»u Hohenzollern',\n",
              " 'uit_001305': 'chá»u trÃ¡ch nhiá»m vá»i chÃ­nh Ã´ng',\n",
              " 'uit_001306': 'tá»« chá»§ vÃ  thá»£',\n",
              " 'uit_001307': 'TÃ´i ÄÃ£ nghiÃªn cá»©u phÃ¡p cháº¿ chá»§ nghÄ©a xÃ£ há»i cá»§a Bismarck',\n",
              " 'uit_001308': 'TÃ´i ÄÃ£ nghiÃªn cá»©u phÃ¡p cháº¿ chá»§ nghÄ©a xÃ£ há»i cá»§a Bismarck',\n",
              " 'uit_001309': 'Hitler',\n",
              " 'uit_001310': 'thiáº¿t láº­p chÆ°Æ¡ng trÃ¬nh an ninh xÃ£ há»i rá»ng rÃ£i',\n",
              " 'uit_001311': 'ÄÃ¡nh giÃ¡ cao',\n",
              " 'uit_001312': 'tÆ°á»ng Paul von Hindenburg',\n",
              " 'uit_001313': 'Äá»©c hoÃ ng Wilhelm II',\n",
              " 'uit_001314': 'Äáº¿ quá»c Ão-Hung vÃ  Äáº¿ quá»c Nga',\n",
              " 'uit_001315': 'nÄm 1914',\n",
              " 'uit_001316': 'tÆ°á»ng Paul von Hindenburg',\n",
              " 'uit_001317': 'tráº­n Grunwald',\n",
              " 'uit_001318': 'tráº­n Grunwald',\n",
              " 'uit_001319': 'Äáº¿ quá»c Ão-Hung vÃ  Äáº¿ quá»c Nga',\n",
              " 'uit_001320': 'phÃ­a TÃ¢y lÃ  má»i Äe dá»a lá»n nháº¥t cho Äáº¿ quá»c Äá»©c',\n",
              " 'uit_001321': 'Bismarck khai máº¡c Nghá» viá»n Äáº§u tiÃªn cá»§a Äáº¿ cháº¿ thá»© hai vÃ o nÄm 1871',\n",
              " 'uit_001322': 'cÃ¡c hoÃ ng Äáº¿ vÆ°Æ¡ng triá»u Hohenzollern',\n",
              " 'uit_001323': 'cÃ¡c hoÃ ng Äáº¿ vÆ°Æ¡ng triá»u Hohenzollern',\n",
              " 'uit_001324': 'Cung Äiá»n Sanssouci',\n",
              " 'uit_001325': 'vinh quang cá»§a Äáº¿ quá»c Äá»©c trÆ°á»c ÄÃ³',\n",
              " 'uit_001326': 'TÃ²a nhÃ  Nghá» viá»n bá» chÃ¡y',\n",
              " 'uit_001327': 'vinh quang cá»§a Äáº¿ quá»c Äá»©c trÆ°á»c ÄÃ³',\n",
              " 'uit_001328': 'TÃ²a nhÃ  Nghá» viá»n bá» chÃ¡y',\n",
              " 'uit_001329': 'Cung Äiá»n Sanssouci',\n",
              " 'uit_001330': 'NhÃ  thá» Doanh tráº¡i Potsdam',\n",
              " 'uit_001331': 'Bismarck khai máº¡c Nghá» viá»n Äáº§u tiÃªn cá»§a Äáº¿ cháº¿ thá»© hai vÃ o nÄm 1871',\n",
              " 'uit_001332': 'thÃ nh tá»±u sÃ¡ng chÃ³i mÃ  ngÆ°á»i Äá»©c Äáº¡t ÄÆ°á»£c',\n",
              " 'uit_001333': 'viá»c dung dÆ°á»¡ng ngÆ°á»i Do ThÃ¡i vÃ  ngÆ°á»i theo MÃ¡c-xÃ­t, tÆ° tÆ°á»ng trá»ng váº­t cháº¥t vÃ  Ã­ch ká»· cá»§a giá»i trung lÆ°u, áº£nh hÆ°á»ng báº¥t chÃ­nh cá»§a nhá»¯ng káº» \"luá»n cÃºi vÃ  xu ná»nh\" quanh ngai vÃ ng Hohenzollern, \"chÃ­nh sÃ¡ch liÃªn minh tai háº¡i\" vá»i VÆ°Æ¡ng triá»u Habsburg suy Äá»i vÃ  ngÆ°á»i Ã khÃ´ng ÄÃ¡ng tin thay vÃ¬ vá»i Anh, thiáº¿u chÃ­nh sÃ¡ch vá» chá»§ng tá»c vÃ  xÃ£ há»i cÆ¡ báº£n',\n",
              " 'uit_001334': '\"luá»n cÃºi vÃ  xu ná»nh\" quanh ngai vÃ ng Hohenzollern',\n",
              " 'uit_001335': 'sáº½ kháº¯c phá»¥c',\n",
              " 'uit_001336': 'Äá»©c Quá»c xÃ£ sáº½ kháº¯c phá»¥c',\n",
              " 'uit_001337': 'viá»c dung dÆ°á»¡ng ngÆ°á»i Do ThÃ¡i vÃ  ngÆ°á»i theo MÃ¡c-xÃ­t, tÆ° tÆ°á»ng trá»ng váº­t cháº¥t vÃ  Ã­ch ká»· cá»§a giá»i trung lÆ°u, áº£nh hÆ°á»ng báº¥t chÃ­nh cá»§a nhá»¯ng káº» \"luá»n cÃºi vÃ  xu ná»nh\" quanh ngai vÃ ng Hohenzollern, \"chÃ­nh sÃ¡ch liÃªn minh tai háº¡i\" vá»i VÆ°Æ¡ng triá»u Habsburg suy Äá»i vÃ  ngÆ°á»i Ã khÃ´ng ÄÃ¡ng tin thay vÃ¬ vá»i Anh, thiáº¿u chÃ­nh sÃ¡ch vá» chá»§ng tá»c vÃ  xÃ£ há»i cÆ¡ báº£n',\n",
              " 'uit_001665': 'viá»c quan sÃ¡t quá»¹ Äáº¡o cá»§a chÃºng sáº½ giÃºp cho viá»c xÃ¡c Äá»nh khá»i lÆ°á»£ng cá»§a chÃºng',\n",
              " 'uit_001666': 'báº¡n Äá»ng hÃ nh',\n",
              " 'uit_001667': 'ngoáº¡i suy tá»« nhá»¯ng sao ÄÃ´i',\n",
              " 'uit_001668': 'viá»c quan sÃ¡t quá»¹ Äáº¡o cá»§a chÃºng sáº½ giÃºp cho viá»c xÃ¡c Äá»nh khá»i lÆ°á»£ng cá»§a chÃºng',\n",
              " 'uit_001669': 'ngoáº¡i suy tá»« nhá»¯ng sao ÄÃ´i',\n",
              " 'uit_001670': 'má»t há» thá»ng gá»m hai ngÃ´i sao chuyá»n Äá»ng trÃªn quá»¹ Äáº¡o cá»§a khá»i tÃ¢m hai ngÃ´i sao',\n",
              " 'uit_001671': 'báº¡n Äá»ng hÃ nh',\n",
              " 'uit_001672': 'binary',\n",
              " 'uit_001673': 'Äáº¡i HÃ¹ng',\n",
              " 'uit_001674': 'há» sao ÄÃ´i',\n",
              " 'uit_001675': 'binary',\n",
              " 'uit_001676': 'Äá»nh luáº­t háº¥p dáº«n',\n",
              " 'uit_001677': 'khoáº£ng 50 cáº·p',\n",
              " 'uit_001678': 'Sir William Herschel',\n",
              " 'uit_001679': 'hiá»u á»©ng Doppler cá»§a Ã¡nh sÃ¡ng phÃ¡t ra',\n",
              " 'uit_001680': 'hiá»u á»©ng Doppler cá»§a Ã¡nh sÃ¡ng phÃ¡t ra',\n",
              " 'uit_001681': 'ÄÆ°á»£c phÃ¢n biá»t báº±ng má»t kÃ­nh viá»n vá»ng Äá»§ máº¡nh',\n",
              " 'uit_001682': 'ÄÆ°á»£c phÃ¢n biá»t báº±ng má»t kÃ­nh viá»n vá»ng Äá»§ máº¡nh',\n",
              " 'uit_001683': 'cÃ¡c ÄÆ°á»ng quang phá» trong Ã¡nh sÃ¡ng tá»« má»i ngÃ´i sao ban Äáº§u bá» dá»ch chuyá»n xanh, sau ÄÃ³ bá» dá»ch chuyá»n Äá» khi nÃ³ Äáº§u tiÃªn di chuyá»n vá» phÃ­a chÃºng ta, rá»i láº¡i di chuyá»n ra xa chÃºng ta, trong khi chÃºng chuyá»n Äá»ng quanh khá»i tÃ¢m chung, vá»i chu ká»³ quá»¹ Äáº¡o chung cá»§a chÃºng',\n",
              " 'uit_001684': 'lá»±c háº«p dáº«n',\n",
              " 'uit_001685': 'nhá»¯ng cáº·p sao náº±m gáº§n nhau tá»i má»©c cÃ¡c ÄÆ°á»ng quang phá» trong Ã¡nh sÃ¡ng tá»« má»i ngÃ´i sao ban Äáº§u bá» dá»ch chuyá»n xanh, sau ÄÃ³ bá» dá»ch chuyá»n Äá» khi nÃ³ Äáº§u tiÃªn di chuyá»n vá» phÃ­a chÃºng ta, rá»i láº¡i di chuyá»n ra xa chÃºng ta, trong khi chÃºng chuyá»n Äá»ng quanh khá»i tÃ¢m chung, vá»i chu ká»³ quá»¹ Äáº¡o chung cá»§a chÃºng',\n",
              " 'uit_001686': 'nhanh',\n",
              " 'uit_001687': 'bá»i vÃ¬ chÃºng á» gáº§n nhau',\n",
              " 'uit_001688': 'nhanh',\n",
              " 'uit_001689': 'khÃ¡ gáº§n',\n",
              " 'uit_001690': 'khÃ¡ xa',\n",
              " 'uit_001691': 'Nhá»¯ng sao ÄÃ´i thá» giÃ¡c thÆ°á»ng cÃ¡ch nhau khÃ¡ xa vÃ  thÆ°á»ng cÃ³ nhá»¯ng tá»c Äá» quá»¹ Äáº¡o quÃ¡ nhá» Äá» cÃ³ thá» Äo Äáº¡c ÄÆ°á»£c báº±ng quang phá»',\n",
              " 'uit_001692': 'khÃ¡ gáº§n',\n",
              " 'uit_001693': 'ráº¥t hiáº¿m',\n",
              " 'uit_001694': 'khÃ´ng phÃ¡t ra báº¥t ká»³ má»t bá»©c xáº¡ Äiá»n tá»« nÃ o',\n",
              " 'uit_001695': 'sao neutron',\n",
              " 'uit_001696': 'sao neutron',\n",
              " 'uit_001697': 'gáº¥p khoáº£ng mÆ°á»i láº§n',\n",
              " 'uit_001698': 'ráº¥t tá»i',\n",
              " 'uit_001699': 'cÃ¡c sao ÄÃ´i dao Äá»ng astrometric binaries',\n",
              " 'uit_001700': 'ráº¥t tá»i',\n",
              " 'uit_001701': 'háº§u nhÆ° tiáº¿p xÃºc vá»i nhau',\n",
              " 'uit_001702': 'Äá» lá»ch tÃ¢m cá»§a quá»¹ Äáº¡o',\n",
              " 'uit_001703': '100',\n",
              " 'uit_001704': 'tiáº¿p xÃºc',\n",
              " 'uit_001705': 'Ã­t hÆ¡n',\n",
              " 'uit_001706': '100',\n",
              " 'uit_001707': 'cÃ¡c há» cÃ³ chu ká»³ quá»¹ Äáº¡o ngáº¯n thÃ¬ cÃ³ Äá» lá»ch tÃ¢m Ã­t hÆ¡n',\n",
              " 'uit_001708': 'HD 188753 Ab',\n",
              " 'uit_001709': 'má»t',\n",
              " 'uit_001710': 'HD 188753 Ab',\n",
              " 'uit_001711': 'TrÃªn thá»±c táº¿, Äa sá» cÃ¡c quá»¹ Äáº¡o cá»§a chÃºng khÃ´ng cÃ¢n báº±ng bá»n (hÃ nh tinh cÃ³ thá» bá» Äáº©y khá»i quá»¹ Äáº¡o cá»§a nÃ³ khÃ¡ nhanh chÃ³ng, cÃ³ thá» bá» báº¯n hoÃ n toÃ n khá»i há» hay bá» chuyá»n sang má»t quá»¹ Äáº¡o lá»ch vÃ o trong hay lÃ¹i ra ngoÃ i hÆ¡n), trong khi nhá»¯ng quá»¹ Äáº¡o khÃ¡c láº¡i thá»±c sá»± kháº¯c nghiá»t cho viá»c hÃ¬nh thÃ nh sinh quyá»n bá»i vÃ¬ chÃºng cÃ³ khÃ¡c biá»t ráº¥t lá»n vá» nhiá»t Äá» bá» máº·t á» nhá»¯ng khoáº£ng cÃ¡ch quá»¹ Äáº¡o khÃ¡c nhau',\n",
              " 'uit_001712': 'thÃ¡ng 7 nÄm 2005',\n",
              " 'uit_001713': 'thÃ¡ng 7 nÄm 2005',\n",
              " 'uit_001714': 'cÃ¡c hÃ nh tinh cá»§a cÃ¡c sao ÄÃ´i hay sao ba',\n",
              " 'uit_001858': 'do sá»± gia tÄng cÃ¡c hoáº¡t Äá»ng táº¡o ra cÃ¡c cháº¥t tháº£i khÃ­ nhÃ  kÃ­nh, cÃ¡c hoáº¡t Äá»ng khai thÃ¡c quÃ¡ má»©c cÃ¡c bá» háº¥p thá»¥ vÃ  bá» chá»©a khÃ­ nhÃ  kÃ­nh nhÆ° sinh khá»i, rá»«ng, cÃ¡c há» sinh thÃ¡i biá»n, ven bá» vÃ  Äáº¥t liá»n khÃ¡c',\n",
              " 'uit_001859': 'lÃ  sá»± thay Äá»i cá»§a há» thá»ng khÃ­ háº­u gá»m khÃ­ quyá»n, thá»§y quyá»n, sinh quyá»n, tháº¡ch quyá»n hiá»n táº¡i vÃ  trong tÆ°Æ¡ng lai bá»i cÃ¡c nguyÃªn nhÃ¢n tá»± nhiÃªn vÃ  nhÃ¢n táº¡o trong má»t giai Äoáº¡n nháº¥t Äá»nh tÃ­nh báº±ng tháº­p ká»· hay hÃ ng triá»u nÄm',\n",
              " 'uit_001860': 'khÃ­ quyá»n, thá»§y quyá»n, sinh quyá»n, tháº¡ch quyá»n',\n",
              " 'uit_001861': 'nÃ³ng lÃªn toÃ n cáº§u',\n",
              " 'uit_001862': 'lÃ  sá»± thay Äá»i cá»§a há» thá»ng khÃ­ háº­u gá»m khÃ­ quyá»n, thá»§y quyá»n, sinh quyá»n, tháº¡ch quyá»n hiá»n táº¡i vÃ  trong tÆ°Æ¡ng lai bá»i cÃ¡c nguyÃªn nhÃ¢n tá»± nhiÃªn vÃ  nhÃ¢n táº¡o trong má»t giai Äoáº¡n nháº¥t Äá»nh tÃ­nh báº±ng tháº­p ká»· hay hÃ ng triá»u nÄm',\n",
              " 'uit_001863': 'United Nations',\n",
              " 'uit_001864': 'CÃ´ng Æ°á»c Khung cá»§a LiÃªn há»£p Quá»c vá» Biáº¿n Äá»i KhÃ­ háº­u',\n",
              " 'uit_001865': 'do tÃ¡c Äá»ng cá»§a hoáº¡t Äá»ng con ngÆ°á»i',\n",
              " 'uit_001866': 'áº¥m lÃªn toÃ n cáº§u',\n",
              " 'uit_001867': 'lÃ  sá»± thay Äá»i cá»§a khÃ­ háº­u mÃ  hoáº·c trá»±c tiáº¿p hoáº·c giÃ¡n tiáº¿p do tÃ¡c Äá»ng cá»§a hoáº¡t Äá»ng con ngÆ°á»i dáº«n Äáº¿n thay Äá»i thÃ nh pháº§n khÃ­ quyá»n toÃ n cáº§u vÃ  ngoÃ i ra lÃ  nhá»¯ng biáº¿n thiÃªn tá»± nhiÃªn cá»§a khÃ­ háº­u ÄÆ°á»£c quan sÃ¡t trÃªn má»t chu ká»³ thá»i gian dÃ i',\n",
              " 'uit_001868': 'Climate justice',\n",
              " 'uit_001869': 'má»t thuáº­t ngá»¯ sá»­ dá»¥ng cho khung sá»± nÃ³ng lÃªn toÃ n cáº§u cÃ³ liÃªn quan tá»i váº¥n Äá» vá» Äáº¡o Äá»©c, vÃ  chÃ­nh trá», chá»© khÃ´ng chá» ÄÆ¡n thuáº§n lÃ  hoÃ n toÃ n vá» mÃ´i trÆ°á»ng, hoáº·c thiÃªn nhiÃªn ÄÆ¡n thuáº§n',\n",
              " 'uit_001870': 'lÃ  má»t thuáº­t ngá»¯ sá»­ dá»¥ng cho khung sá»± nÃ³ng lÃªn toÃ n cáº§u cÃ³ liÃªn quan tá»i váº¥n Äá» vá» Äáº¡o Äá»©c, vÃ  chÃ­nh trá», chá»© khÃ´ng chá» ÄÆ¡n thuáº§n lÃ  hoÃ n toÃ n vá» mÃ´i trÆ°á»ng, hoáº·c thiÃªn nhiÃªn ÄÆ¡n thuáº§n',\n",
              " 'uit_001871': 'vÃ¬ khá»i lÆ°á»£ng lá»n',\n",
              " 'uit_001872': 'hÃ ng tháº¿ ká»· hoáº·c lÃ¢u hÆ¡n',\n",
              " 'uit_001873': 'thay Äá»i bá»©c xáº¡ khÃ­ quyá»n, bao gá»m cÃ¡c quÃ¡ trÃ¬nh nhÆ° biáº¿n Äá»i bá»©c xáº¡ máº·t trá»i, Äá» lá»ch quá»¹ Äáº¡o cá»§a TrÃ¡i Äáº¥t, quÃ¡ trÃ¬nh kiáº¿n táº¡o nÃºi, kiáº¿n táº¡o trÃ´i dáº¡t lá»¥c Äá»a vÃ  sá»± thay Äá»i ná»ng Äá» khÃ­ nhÃ  kÃ­nh',\n",
              " 'uit_001874': 'cÃ³ thá» máº¥t hÃ ng tháº¿ ká»· hoáº·c lÃ¢u',\n",
              " 'uit_001875': 'cháº­m',\n",
              " 'uit_001876': 'Äáº¡i dÆ°Æ¡ng vÃ  chá»m bÄng, pháº£n á»©ng cháº­m vá»i biáº¿n Äá»i bá»©c xáº¡ máº·t trá»i vÃ¬ khá»i lÆ°á»£ng lá»n',\n",
              " 'uit_001877': 'ná»n táº£ng',\n",
              " 'uit_001878': 'vÃ i nÄm Äáº¿n vÃ i tháº­p niÃªn',\n",
              " 'uit_001879': 'ÄÃ³ng vai trÃ² quan trá»ng trong sá»± tÃ¡i phÃ¢n bá» nhiá»t trong Äáº¡i dÆ°Æ¡ng trÃªn tháº¿ giá»i',\n",
              " 'uit_001880': 'vÃ i nÄm Äáº¿n vÃ i tháº­p niÃªn',\n",
              " 'uit_001881': '3 kiá»u',\n",
              " 'uit_001882': 'gÃ¢y ra nhá»¯ng thay Äá»i vá» sá»± phÃ¢n bá» nÄng lÆ°á»£ng máº·t trá»i theo mÃ¹a trÃªn bá» máº·t TrÃ¡i Äáº¥t vÃ  cÃ¡ch nÃ³ ÄÆ°á»£c phÃ¢n bá» trÃªn toÃ n cáº§u',\n",
              " 'uit_001883': 'thay Äá»i quá»¹ Äáº¡o lá»ch tÃ¢m cá»§a TrÃ¡i Äáº¥t, thay Äá»i trá»¥c quay, vÃ  tiáº¿n Äá»ng cá»§a trá»¥c TrÃ¡i Äáº¥t',\n",
              " 'uit_001884': '3 kiá»u',\n",
              " 'uit_001885': 'cÃ³ thá» gÃ¢y biáº¿n Äá»i máº¡nh máº½ vá» sá»± phÃ¢n bá» cÃ¡c mÃ¹a vÃ  Äá»a lÃ½',\n",
              " 'uit_001886': 'áº£nh hÆ°á»ng máº¡nh máº½ Äáº¿n khÃ­ háº­u vÃ  má»i tÆ°Æ¡ng quan cá»§a chÃºng vá»i cÃ¡c chu ká»³ bÄng hÃ  vÃ  gian bÄng, quan há» cá»§a chÃºng vá»i sá»± phÃ¡t triá»n vÃ  thoÃ¡i lui cá»§a Sahara, vÃ  Äá»i vá»i sá»± xuáº¥t hiá»n cá»§a chÃºng trong cÃ¡c Äá»a táº§ng',\n",
              " 'uit_001887': 'cÃ³ thá» gÃ¢y biáº¿n Äá»i máº¡nh máº½ vá» sá»± phÃ¢n bá» cÃ¡c mÃ¹a vÃ  Äá»a lÃ½',\n",
              " 'uit_001888': 'quá»¹ Äáº¡o TrÃ¡i Äáº¥t',\n",
              " 'uit_001889': 'gÃ¢y ra sá»± áº¥m lÃªn toÃ n cáº§u vÃ  tuyá»t chá»§ng hÃ ng loáº¡t',\n",
              " 'uit_001890': 'Nhiá»t Äá» toÃ n cáº§u giáº£m khoáº£ng 0,5 Â°C (0.9 Â°F)',\n",
              " 'uit_001891': 'gÃ¢y ra lÃ m mÃ¡t (báº±ng má»t pháº§n ngÄn cháº·n sá»± lÃ¢y truyá»n cá»§a bá»©c xáº¡ máº·t trá»i Äáº¿n bá» máº·t TrÃ¡i Äáº¥t) trong thá»i gian má»t vÃ i nÄm',\n",
              " 'uit_001892': 'gÃ¢y ra lÃ m mÃ¡t (báº±ng má»t pháº§n ngÄn cháº·n sá»± lÃ¢y truyá»n cá»§a bá»©c xáº¡ máº·t trá»i Äáº¿n bá» máº·t TrÃ¡i Äáº¥t) trong thá»i gian má»t vÃ i nÄm',\n",
              " 'uit_001893': 'vá»¥ phun trÃ o nÄm 1912 cá»§a nÃºi lá»­a Novarupta',\n",
              " 'uit_001894': 'Novarupta',\n",
              " 'uit_001895': 'vá»¥ phun trÃ o cá»§a nÃºi lá»­a Pinatubo',\n",
              " 'uit_001896': 'gÃ¢y ra sá»± áº¥m lÃªn toÃ n cáº§u vÃ  tuyá»t chá»§ng hÃ ng loáº¡t',\n",
              " 'uit_001897': 'sá»± hÃ¬nh thÃ nh eo Äáº¥t Panama',\n",
              " 'uit_001898': 'khoáº£ng 300 Äáº¿n 365 triá»u nÄm trÆ°á»c',\n",
              " 'uit_001899': 'Äáº¡i TÃ¢y DÆ°Æ¡ng vÃ  ThÃ¡i BÃ¬nh DÆ°Æ¡ng',\n",
              " 'uit_001900': 'áº£nh hÆ°á»ng ráº¥t máº¡nh máº½ Äáº¿n cÃ¡c cháº¿ Äá» Äá»ng lá»±c há»c cá»§a Äáº¡i dÆ°Æ¡ng cá»§a háº£i lÆ°u Gulf Stream vÃ  ÄÃ£ lÃ m cho báº¯c bÃ¡n cáº§u bá» phá»§ bÄng',\n",
              " 'uit_001901': 'khoáº£ng 300 Äáº¿n 365 triá»u nÄm trÆ°á»c',\n",
              " 'uit_001902': 'ÄÃ³ng vai trÃ² quan trá»ng trong viá»c kiá»m soÃ¡t sá»± truyá»n nhiá»t vÃ  Äá» áº©m trÃªn toÃ n cáº§u vÃ  hÃ¬nh thÃ nh nÃªn khÃ­ háº­u toÃ n cáº§u',\n",
              " 'uit_001903': 'Vá» trÃ­ cá»§a cÃ¡c biá»n ÄÃ³ng vai trÃ² quan trá»ng trong viá»c kiá»m soÃ¡t sá»± truyá»n nhiá»t vÃ  Äá» áº©m trÃªn toÃ n cáº§u vÃ  hÃ¬nh thÃ nh nÃªn khÃ­ háº­u toÃ n cáº§u',\n",
              " 'uit_001904': 'áº£nh hÆ°á»ng ráº¥t máº¡nh máº½ Äáº¿n cÃ¡c cháº¿ Äá» Äá»ng lá»±c há»c cá»§a Äáº¡i dÆ°Æ¡ng cá»§a háº£i lÆ°u Gulf Stream vÃ  ÄÃ£ lÃ m cho báº¯c bÃ¡n cáº§u bá» phá»§ bÄng',\n",
              " 'uit_001905': 'nhiá»t Äá», lÆ°á»£ng tuyáº¿t rÆ¡i, lÆ°á»£ng nÆ°á»c náº±m giá»¯a vÃ  dÆ°á»i lá»p bÄng',\n",
              " 'uit_001906': 'má»t sÃ´ng bÄng vá»n hÃ¬nh thÃ nh tá»« nhiá»u sÃ´ng bÄng nhá» khÃ¡c nhau pháº£i tá»n trung bÃ¬nh hÃ ng tháº¿ ká» hoáº·c tháº­m chÃ­ lÃ¢u hÆ¡n Äá» tan ra bá»i tÃ¡c Äá»ng cá»§a nhá»¯ng biáº¿n Äá»i ngáº¯n háº¡n cá»§a vÃ¹ng',\n",
              " 'uit_001907': 'hÃ ng tháº¿ ká» hoáº·c tháº­m chÃ­ lÃ¢u hÆ¡n',\n",
              " 'uit_001908': 'Sá»± thay Äá»i vá» nhiá»t Äá», lÆ°á»£ng tuyáº¿t rÆ¡i, lÆ°á»£ng nÆ°á»c náº±m giá»¯a vÃ  dÆ°á»i lá»p bÄng',\n",
              " 'uit_001909': 'Sá»± thay Äá»i vá» nhiá»t Äá», lÆ°á»£ng tuyáº¿t rÆ¡i, lÆ°á»£ng nÆ°á»c náº±m giá»¯a vÃ  dÆ°á»i lá»p bÄng',\n",
              " 'uit_001910': 'thu háº¹p ÄÃ¡ng ká», vá»i sá»± lÃ¹i dáº§n máº¡nh cá»§a nhá»¯ng sÃ´ng bÄng trong nhá»¯ng nÄm 1940, cÃ³ Äiá»u kiá»n á»n Äá»nh hoáº·c phÃ¡t triá»n trong nhá»¯ng nÄm 1920 vÃ  1970, vÃ  má»t láº§n ná»¯a báº¯t Äáº§u giáº£m tá»« giá»¯a nhá»¯ng nÄm 1980 Äáº¿n nay',\n",
              " 'uit_001911': 'giáº£m',\n",
              " 'uit_001912': 'khoáº£ng 445.000 km2',\n",
              " 'uit_001913': 'tá»« nhá»¯ng nÄm 1970',\n",
              " 'uit_001914': 'khoáº£ng 240.000 km2',\n",
              " 'uit_001915': 'nhá»¯ng nÄm 1970',\n",
              " 'uit_001916': 'Tá» chá»©c GiÃ¡m sÃ¡t SÃ´ng bÄng Tháº¿ giá»i',\n",
              " 'uit_001917': 'khoáº£ng 445.000 km2',\n",
              " 'uit_001918': 'thay Äá»i quá»¹ Äáº¡o, nhá»¯ng pháº£n á»©ng nhÆ° tÄng hoáº·c giáº£m cÃ¡c dáº£i bÄng lá»¥c Äá»a vÃ  thay Äá»i má»±c nÆ°á»c biá»n táº¡o nÃªn khÃ­ háº­u',\n",
              " 'uit_001919': 'thay Äá»i quá»¹ Äáº¡o',\n",
              " 'uit_001920': 'khoáº£ng 11.700 nÄm',\n",
              " 'uit_001921': 'Dansgaard-Oeschger',\n",
              " 'uit_001922': 'khoáº£ng 11.700 nÄm',\n",
              " 'uit_001923': 'khoáº£ng 3 triá»u nÄm trÆ°á»c',\n",
              " 'uit_001924': 'hiá»n tÆ°á»£ng sa máº¡c hoÃ¡',\n",
              " 'uit_001925': 'nhiá»u loÃ i nhanh chÃ³ng biáº¿n máº¥t',\n",
              " 'uit_001926': 'khÃ­ háº­u',\n",
              " 'uit_001927': 'dáº«n Äáº¿n tÄng trÆ°á»ng thá»±c váº­t ÄÆ°á»£c cáº£i thiá»n vÃ  kÃ©o theo viá»c háº¥p thá»¥ nhiá»u CO2 trong khÃ´ng khÃ­ hÆ¡n',\n",
              " 'uit_001928': 'nhá»¯ng chá» sá» quan trá»ng vá» sá»± thay Äá»i lÆ°á»£ng CO2 qua hÃ ng ngÃ n nÄm, vÃ  tiáº¿p tá»¥c cung cáº¥p nhá»¯ng thÃ´ng tin cÃ³ giÃ¡ trá» vá» sá»± khÃ¡c nhau giá»¯a Äiá»u kiá»n khÃ´ng khÃ­ cá» xÆ°a vÃ  hiá»n Äáº¡i',\n",
              " 'uit_001929': 'CÃ¡c thÃ´ng tin tá»« viá»c phÃ¢n tÃ­ch pháº§n lÃµi bÄng khoan tá»« má»t khá»i bÄng nhÆ° khá»i bÄng Nam Cá»±c',\n",
              " 'uit_001930': 'KhÃ´ng khÃ­ bá» máº¯c káº¹t á» dáº¡ng bong bÃ³ng trong bÄng',\n",
              " 'uit_001931': 'CO2',\n",
              " 'uit_001932': 'KhÃ´ng khÃ­ bá» máº¯c káº¹t á» dáº¡ng bong bÃ³ng trong bÄng',\n",
              " 'uit_001933': 'CÃ¡c thÃ´ng tin tá»« viá»c phÃ¢n tÃ­ch pháº§n lÃµi bÄng khoan tá»« má»t khá»i bÄng nhÆ° khá»i bÄng Nam Cá»±c',\n",
              " 'uit_001934': 'lÃ  bá» mÃ´n khoa há»c hiá»n Äáº¡i nghiÃªn cá»©u vá» lÄ©nh vá»±c hÃ³a tháº¡ch á» kÃ­ch thÆ°á»c táº¿ bÃ o, bao gá»m cáº£ pháº¥n hoa',\n",
              " 'uit_001935': 'Äá» suy ra sá»± phÃ¢n bá» Äá»a lÃ½ cá»§a cÃ¡c loÃ i thá»±c váº­t tá»«ng thay Äá»i theo Äiá»u kiá»n khÃ­ háº­u khÃ¡c nhau',\n",
              " 'uit_001936': 'suy ra sá»± phÃ¢n bá» Äá»a lÃ½ cá»§a cÃ¡c loÃ i thá»±c váº­t tá»«ng thay Äá»i theo Äiá»u kiá»n khÃ­ háº­u khÃ¡c nhau',\n",
              " 'uit_001937': 'lÃ  bá» mÃ´n khoa há»c hiá»n Äáº¡i nghiÃªn cá»©u vá» lÄ©nh vá»±c hÃ³a tháº¡ch á» kÃ­ch thÆ°á»c táº¿ bÃ o, bao gá»m cáº£ pháº¥n hoa',\n",
              " 'uit_001938': 'lá»p ngoÃ i cá»§a pháº¥n hoa ÄÆ°á»£c cáº¥u thÃ nh tá»« má»t lá»p cháº¥t liá»u cÃ³ tÃ­nh ÄÃ n há»i ráº¥t cao',\n",
              " 'uit_001939': 'cÃ¡c thay Äá»i á» tháº¿ giá»i thá»±c váº­t',\n",
              " 'uit_001940': 'lá»p ngoÃ i cá»§a pháº¥n hoa ÄÆ°á»£c cáº¥u thÃ nh tá»« má»t lá»p cháº¥t liá»u cÃ³ tÃ­nh ÄÃ n há»i ráº¥t cao',\n",
              " 'uit_001941': 'cáº¥u trÃºc di truyá»n khÃ´ng thay Äá»i ÄÃ¡ng ká» qua hÃ ng ngÃ n nÄm',\n",
              " 'uit_001942': 'nhá»¯ng vÃ¹ng nÆ°á»c ngá»t vÃ  tráº§m tÃ­ch Äáº¥t Äai',\n",
              " 'uit_001943': 'viá»c nghiÃªn cá»©u dá»±a trÃªn nhá»¯ng loÃ i bá» cÃ¡nh cá»©ng khÃ¡c nhau sáº½ Äem láº¡i kiáº¿n thá»©c vá» pháº¡m vi khÃ­ háº­u hiá»n táº¡i, xÃ¡c Äá»nh ÄÆ°á»£c tuá»i cá»§a cÃ¡c tráº§m tÃ­ch cÃ²n sÃ³t láº¡i, tá»« ÄÃ³ cÃ³ thá» suy ra Äiá»u kiá»n khÃ­ háº­u trong quÃ¡ khá»©',\n",
              " 'uit_001944': 'khÃ´ng',\n",
              " 'uit_001945': 'mÃ¡y Äo Äá» cao - káº¿t há»£p vá»i sá»± Äá»nh vá» chÃ­nh xÃ¡c cá»§a cÃ¡c quá»¹ Äáº¡o vá» tinh',\n",
              " 'uit_001946': 'thÃ´ng qua cÃ¡c dáº¥u váº¿t trÃªn nhá»¯ng ráº·ng san hÃ´, nhá»¯ng lá»p tráº§m tÃ­ch ven biá»n, trÃªn thá»m biá»n, háº¡t trong ÄÃ¡ vÃ´i vÃ  nhá»¯ng di tÃ­ch kháº£o cá» cÃ²n sÃ³t láº¡i gáº§n bá» biá»n',\n",
              " 'uit_001947': 'mÃ¡y Äo thá»§y triá»u',\n",
              " 'uit_001948': 'sá»­ dá»¥ng cÃ¡c mÃ¡y Äo thá»§y triá»u, cÃ¡c sá» liá»u Äo ÄÆ°á»£c Äá»i chiáº¿u trong thá»i gian dÃ i Äá» ÄÆ°a ra má»t má»±c nÆ°á»c trung bÃ¬nh dÃ i háº¡n',\n",
              " 'uit_001949': 'mÃ¡y Äo Äá» cao - káº¿t há»£p vá»i sá»± Äá»nh vá» chÃ­nh xÃ¡c cá»§a cÃ¡c quá»¹ Äáº¡o vá» tinh',\n",
              " 'uit_001950': 'mÃ¡y Äo Äá» cao - káº¿t há»£p vá»i sá»± Äá»nh vá» chÃ­nh xÃ¡c cá»§a cÃ¡c quá»¹ Äáº¡o vá» tinh',\n",
              " 'uit_001951': 'phÆ°Æ¡ng phÃ¡p urani vÃ  cacbon phÃ³ng xáº¡',\n",
              " 'uit_001952': 'sá»­ dá»¥ng cÃ¡c mÃ¡y Äo thá»§y triá»u, cÃ¡c sá» liá»u Äo ÄÆ°á»£c Äá»i chiáº¿u trong thá»i gian dÃ i Äá» ÄÆ°a ra má»t má»±c nÆ°á»c trung bÃ¬nh dÃ i háº¡n',\n",
              " 'uit_001953': 'mÃ¡y Äo thá»§y triá»u',\n",
              " 'uit_001954': 'thÃ´ng qua cÃ¡c dáº¥u váº¿t trÃªn nhá»¯ng ráº·ng san hÃ´, nhá»¯ng lá»p tráº§m tÃ­ch ven biá»n, trÃªn thá»m biá»n, háº¡t trong ÄÃ¡ vÃ´i vÃ  nhá»¯ng di tÃ­ch kháº£o cá» cÃ²n sÃ³t láº¡i gáº§n bá» biá»n',\n",
              " 'uit_001955': 'phÆ°Æ¡ng phÃ¡p urani vÃ  cacbon phÃ³ng xáº¡',\n",
              " 'uit_001956': 'Edward I',\n",
              " 'uit_001957': 'tá»« 1307 cho Äáº¿n khi bá» láº­t Äá» vÃ o thÃ¡ng 1 nÄm 1327',\n",
              " 'uit_001958': 'con gÃ¡i cá»§a vua PhÃ¡p Philippe IV',\n",
              " 'uit_001959': 'Edward II',\n",
              " 'uit_001960': '21 thÃ¡ng 9, 1327',\n",
              " 'uit_001961': 'Alphonso',\n",
              " 'uit_001962': 'giÃ¡i quyáº¿t nhá»¯ng cÄng tháº³ng vÆ°Æ¡ng quyá»n giá»¯a Anh vÃ  PhÃ¡p',\n",
              " 'uit_001963': 'thu há»i sáº¯c lá»nh cáº£i cÃ¡ch vÃ  triá»u há»i sá»§ng tháº§n cá»§a mÃ¬nh',\n",
              " 'uit_001964': 'nÄm 1300',\n",
              " 'uit_001965': 'náº¡n ÄÃ³i lan rá»ng',\n",
              " 'uit_001966': 'lÆ°u ÄÃ y Ã´ng ta',\n",
              " 'uit_001967': 'CÃ¡c nam tÆ°á»c ÄÆ°á»£c trao quyá»n trá»¥c xuáº¥t Gaveston',\n",
              " 'uit_001968': 'má»i quan há» gáº§n thÃ¢n thiáº¿t gÃ¢y nhiá»u tranh cÃ£i vá»i Piers Gaveston',\n",
              " 'uit_001969': 'CÃ¡c nam tÆ°á»c ÄÆ°á»£c trao quyá»n trá»¥c xuáº¥t Gaveston',\n",
              " 'uit_001970': 'Piers Gaveston',\n",
              " 'uit_001971': 'tá»« nÄm 1300',\n",
              " 'uit_001972': 'bá» cÃ¡c hiá»p sÄ© cá»§a vÆ°Æ¡ng triá»u má»i Ã¡m sÃ¡t',\n",
              " 'uit_001973': 'Isabella láº­p liÃªn minh vá»i Roger Mortimer Äang bá» lÆ°u ÄÃ y, vÃ  xÃ¢m lÆ°á»£c Anh báº±ng má»t Äá»i quÃ¢n nhá» nÄm 1326',\n",
              " 'uit_001974': 'Gia ÄÃ¬nh Despenser, Äáº·c biá»t lÃ  Hugh Despenser tráº»',\n",
              " 'uit_001975': 'thÃ¡ng 1 nÄm 1327',\n",
              " 'uit_001976': 'láº­p liÃªn minh vá»i Roger Mortimer Äang bá» lÆ°u ÄÃ y, vÃ  xÃ¢m lÆ°á»£c Anh báº±ng má»t Äá»i quÃ¢n nhá» nÄm 1326',\n",
              " 'uit_001977': 'thÃ¡ng 1 nÄm 1327',\n",
              " 'uit_001978': 'cÃ¡c hiá»p sÄ© cá»§a vÆ°Æ¡ng triá»u má»i Ã¡m sÃ¡t',\n",
              " 'uit_001979': 'thÃ¡ng 1 nÄm 1327',\n",
              " 'uit_001980': 'vá» vua lÆ°á»i nhÃ¡c vÃ  thiáº¿u nÄng lá»±c, hay ÄÆ¡n giáº£n chá» lÃ  má»t nhÃ  cai trá» báº¥t Äáº¯c dÄ© vÃ  hoÃ n toÃ n tháº¥t báº¡i',\n",
              " 'uit_001981': 'nhá»¯ng ngÆ°á»i ÄÆ°Æ¡ng thá»i',\n",
              " 'uit_001982': 'Christopher Marlowe',\n",
              " 'uit_001983': 'sá»± tiáº¿n triá»n triá»n cá»§a cÃ¡c tá» chá»©c Quá»c há»i trong triá»u Äáº¡i cá»§a Ã´ng lÃ  sá»± chá» dáº¥u tÃ­ch cá»±c cho Äáº¥t nÆ°á»c Anh mÃ  trong thá»i gian dÃ i chÆ°a Äáº¡t ÄÆ°á»£c',\n",
              " 'uit_001984': 'Má»i quan há» giá»¯a Edward vÃ  Gaveston',\n",
              " 'uit_001985': 'sá»± chá» dáº¥u tÃ­ch cá»±c cho Äáº¥t nÆ°á»c Anh mÃ  trong thá»i gian dÃ i chÆ°a Äáº¡t ÄÆ°á»£c',\n",
              " 'uit_001986': 'lÆ°á»i nhÃ¡c vÃ  thiáº¿u nÄng lá»±c, hay ÄÆ¡n giáº£n chá» lÃ  má»t nhÃ  cai trá» báº¥t Äáº¯c dÄ© vÃ  hoÃ n toÃ n tháº¥t báº¡i',\n",
              " 'uit_001987': 'má»i quan há» Äá»ng tÃ­nh luyáº¿n Ã¡i Äá»n ÄoÃ¡n cá»§a hai ngÆ°á»i ÄÃ n Ã´ng',\n",
              " 'uit_001988': 'má»t nhÃ  lÃ£nh Äáº¡o quÃ¢n sá»± tÃ i nÄng',\n",
              " 'uit_001989': 'má»t nhÃ  lÃ£nh Äáº¡o quÃ¢n sá»± tÃ i nÄng',\n",
              " 'uit_001990': 'VÆ°Æ¡ng quá»c Castile',\n",
              " 'uit_001991': 'má»t vá» vua ÄÃ¡ng sá»£ vÃ  ÄÃ¡ng kÃ­nh',\n",
              " 'uit_001992': 'Ã´ng thá» hiá»n kháº£ nÄng kiá»m soÃ¡t quyá»n hÃ nh rá»ng lá»n cá»§a cÃ¡c bÃ¡ tÆ°á»c trong hÃ ng ngÅ© giá»i quÃ½ tá»c Anh',\n",
              " 'uit_001993': 'Eleanor xá»© Castile',\n",
              " 'uit_001994': 'tuyÃªn bá» bÃ¡ quyá»n Äá»i vá»i nÆ°á»c nÃ y',\n",
              " 'uit_001995': 'tuyÃªn bá» bÃ¡ quyá»n Äá»i vá»i nÆ°á»c nÃ y',\n",
              " 'uit_001996': 'trÃªn cÆ°Æ¡ng vá» chÆ° háº§u cáº§n tá» lÃ²ng tháº§n phá»¥c há»',\n",
              " 'uit_001997': 'CÃ¡c vua PhÃ¡p nháº¥n máº¡nh ráº±ng quá»c vÆ°Æ¡ng Anh trÃªn cÆ°Æ¡ng vá» chÆ° háº§u cáº§n tá» lÃ²ng tháº§n phá»¥c há», tuy nhiÃªn Ã´ng cho ráº±ng yÃªu cáº§u ÄÃ³ lÃ  má»t sá»± lÄng máº¡ Äáº¿n lÃ²ng kiÃªu hÃ£nh cá»§a nhÃ  vua vÃ  váº¥n Äá» nÃ y váº«n chÆ°a ÄÆ°á»£c giáº£i quyáº¿t triá»t Äá»',\n",
              " 'uit_001998': 'Viá»c Edward cai trá» vÃ¹ng Gascony',\n",
              " 'uit_001999': 'nÄm 1307',\n",
              " 'uit_002000': 'sá»± thá»ng trá» cá»§a Anh á» Scotland',\n",
              " 'uit_002001': 'Viá»c Edward cai trá» vÃ¹ng Gascony',\n",
              " 'uit_002002': 'Ã´ng ÄÃ¡nh thuáº¿ náº·ng vÃ  yÃªu cáº§u cung cáº¥p nhiá»u nguá»n lá»±c phá»¥c vá»¥ chiáº¿n tranh',\n",
              " 'uit_002003': 'Ã´ng ÄÃ¡nh thuáº¿ náº·ng vÃ  yÃªu cáº§u cung cáº¥p nhiá»u nguá»n lá»±c phá»¥c vá»¥ chiáº¿n tranh',\n",
              " 'uit_002004': 'nÄm 1307',\n",
              " 'uit_002005': 'Edward xá»© Caernarfon',\n",
              " 'uit_002006': 'ngÃ y 25 thÃ¡ng 4 nÄm 1284',\n",
              " 'uit_002007': 'LÃ¢u ÄÃ i Caernarfon',\n",
              " 'uit_002008': 'Edward xá»© Caernarfon',\n",
              " 'uit_002009': 'ngÃ y 25 thÃ¡ng 4 nÄm 1284',\n",
              " 'uit_002010': 'LÃ¢u ÄÃ i Caernarfon á» miá»n Báº¯c xá»© Wales',\n",
              " 'uit_002011': 'ngÃ y 25 thÃ¡ng 4 nÄm 1284',\n",
              " 'uit_002012': 'nÃ³ lÃ  má»t Äá»a Äiá»m cÃ³ tÃ­nh biá»u tÆ°á»£ng quan trá»ng Äá»i vá»i ngÆ°á»i Wales báº£n Äá»a, gáº¯n liá»n vá»i lá»ch sá»­ Äáº¿ quá»c La MÃ£, vÃ  lÃ¢u ÄÃ i Caernarfon cÅ©ng lÃ  nÆ¡i Äáº·t vÆ°Æ¡ng quyá»n má»i á» miá»n Báº¯c xá»© Wales',\n",
              " 'uit_002013': 'Eleanor',\n",
              " 'uit_002014': 'Alphonso',\n",
              " 'uit_002015': 'dÃ¹ng tÃªn tiáº¿ng Norman vÃ  Castilla',\n",
              " 'uit_002016': 'Alice de Leygrave',\n",
              " 'uit_002017': 'Alice de Leygrave',\n",
              " 'uit_002018': 'TÃªn Edward cÃ³ xuáº¥t xá»© tá»« tiáº¿ng Anh, liÃªn tÆ°á»ng Äáº¿n ThÃ¡nh ngÆ°á»i Anglo-Saxon lÃ  Edward xÆ°ng tá»i, vÃ  ÄÆ°á»£c lá»±a chá»n bá»i cha Ã´ng thay cho truyá»n thá»ng dÃ¹ng tÃªn tiáº¿ng Norman vÃ  Castilla ÄÃ£ ÄÆ°á»£c Äáº·t cho cÃ¡c anh trai cá»§a Edward',\n",
              " 'uit_002019': 'Mariota hoáº·c Mary Maunsel',\n",
              " 'uit_002020': 'má»¥c sÆ° Giles',\n",
              " 'uit_002021': 'má»¥c sÆ° Giles xá»© Oudenarde',\n",
              " 'uit_002022': 'lÃ m quÃ¢n sÆ° cho riÃªng Ã´ng',\n",
              " 'uit_002023': 'rÃ¨n luyá»n, huáº¥n luyá»n cÆ°á»¡i ngá»±a vÃ  ká»¹ nÄng quÃ¢n sá»± cho Edward',\n",
              " 'uit_002024': 'chi tiÃªu trong tÆ° tháº¥t',\n",
              " 'uit_002025': 'tiáº¿ng PhÃ¡p Anglo-Norman',\n",
              " 'uit_002026': 'cÃ¡c chi tiÃªu trong tÆ° tháº¥t cá»§a Edward',\n",
              " 'uit_002027': 'ná»n giÃ¡o dá»¥c CÃ´ng giÃ¡o tá»« cÃ¡c tu sÄ© dÃ²ng Äa Minh',\n",
              " 'uit_002028': 'William xá»© Blyborough',\n",
              " 'uit_002029': 'Guy Ferre',\n",
              " 'uit_002030': 'má»t tay cÆ°á»¡i ngá»±a giá»i',\n",
              " 'uit_002031': 'nháº¡c Wales vÃ  ÄÃ n crwth má»i vá»«a ÄÆ°á»£c phÃ¡t minh, vÃ  phong cáº§m',\n",
              " 'uit_002032': 'Äáº¥u thÆ°Æ¡ng',\n",
              " 'uit_002033': 'nháº¡c Wales vÃ  ÄÃ n crwth má»i vá»«a ÄÆ°á»£c phÃ¡t minh, vÃ  phong cáº§m',\n",
              " 'uit_002034': 'Ã´ng khÃ´ng cÃ³ nÄng khiáº¿u hoáº·c lÃ  táº¡i Ã´ng khÃ´ng ÄÆ°á»£c phÃ©p tham gia Äá» Äáº£m báº£o an toÃ n',\n",
              " 'uit_002035': 'giÃºp duy trÃ¬ ná»n hÃ²a bÃ¬nh lÃ¢u dÃ i giá»¯a Anh vá»i PhÃ¡p',\n",
              " 'uit_002036': 'NÄm 1290',\n",
              " 'uit_002037': 'Hiá»p Æ°á»c Birgham',\n",
              " 'uit_002038': 'NÄm 1290',\n",
              " 'uit_002039': 'Margaret',\n",
              " 'uit_002040': 'Margaret cá»§a Na Uy',\n",
              " 'uit_002041': 'Hiá»p Æ°á»c Birgham',\n",
              " 'uit_002042': 'NÄm 1290',\n",
              " 'uit_002043': 'Margaret táº¡ tháº¿ cÃ¹ng nÄm',\n",
              " 'uit_002044': 'Margaret táº¡ tháº¿',\n",
              " 'uit_002045': 'giÃºp duy trÃ¬ ná»n hÃ²a bÃ¬nh lÃ¢u dÃ i giá»¯a Anh vá»i PhÃ¡p',\n",
              " 'uit_002046': 'Edward',\n",
              " 'uit_002047': 'Edward',\n",
              " 'uit_002048': 'quan há» tá»t Äáº¹p',\n",
              " 'uit_002049': 'sá»± há» trá»£ vá» tÃ i chÃ­nh vÃ  cÃ¡c chá»©c danh',\n",
              " 'uit_002050': '1301',\n",
              " 'uit_002051': 'Isabella',\n",
              " 'uit_002052': 'NhÃ  vua tham gia chiáº¿n dá»ch Flanders chá»ng láº¡i Philippe IV',\n",
              " 'uit_002053': 'Isabella',\n",
              " 'uit_002054': 'LÃ£nh Äá»a BÃ¡ tÆ°á»c Chester vÃ  nhá»¯ng vÃ¹ng Äáº¥t trÃªn kháº¯p Báº¯c Wales',\n",
              " 'uit_002055': 'LÃ£nh Äá»a BÃ¡ tÆ°á»c Chester vÃ  nhá»¯ng vÃ¹ng Äáº¥t trÃªn kháº¯p Báº¯c Wales',\n",
              " 'uit_002056': 'chá» huy háº­u quÃ¢n trong cuá»c bao vÃ¢y Caerlaverock',\n",
              " 'uit_002057': 'sá»± tháº§n phá»¥c',\n",
              " 'uit_002058': 'sá»± Äá»c láº­p vá» tÃ i chÃ­nh',\n",
              " 'uit_002059': 'chá» huy háº­u quÃ¢n trong cuá»c bao vÃ¢y Caerlaverock',\n",
              " 'uit_002060': 'Ã´ng Äem con trai Äi theo',\n",
              " 'uit_002061': 'Ã´ng Äem con trai Äi theo, khiáº¿n cho Edward trá» thÃ nh chá» huy háº­u quÃ¢n trong cuá»c bao vÃ¢y Caerlaverock',\n",
              " 'uit_002062': 'sá»± Äá»c láº­p vá» tÃ i chÃ­nh',\n",
              " 'uit_002063': 'LÃ£nh Äá»a BÃ¡ tÆ°á»c Chester vÃ  nhá»¯ng vÃ¹ng Äáº¥t trÃªn kháº¯p Báº¯c Wales',\n",
              " 'uit_002064': 'huy Äá»ng má»t lá»±c lÆ°á»£ng quÃ¢n viá»n chinh má»i',\n",
              " 'uit_002065': 'Robert the Bruce',\n",
              " 'uit_002066': 'sá»± trá»«ng pháº¡t, sá»± tráº£ thÃ¹ khá»§ng khiáº¿p',\n",
              " 'uit_002067': 'huy Äá»ng má»t lá»±c lÆ°á»£ng quÃ¢n viá»n chinh má»i',\n",
              " 'uit_002068': 'sá»± trá»«ng pháº¡t, sá»± tráº£ thÃ¹ khá»§ng khiáº¿p',\n",
              " 'uit_002069': 'Robert the Bruce',\n",
              " 'uit_002070': 'ÄÃ m phÃ¡n vá» ngÃ y cÆ°á»i cá»§a Ã´ng vá»i Isabella',\n",
              " 'uit_002071': 'cÃ¡c cuá»c ÄÃ m phÃ¡n vá» ngÃ y cÆ°á»i cá»§a Ã´ng vá»i Isabella tiáº¿p tá»¥c',\n",
              " 'uit_002072': 'con trai Ã´ng',\n",
              " 'uit_002073': 'Piers Gaveston',\n",
              " 'uit_002074': 'hiá»p sÄ© trong gia trang cá»§a NhÃ  vua cÃ³ Äáº¥t phong giÃ¡p vá»i Gascony',\n",
              " 'uit_002075': 'ngÆ°á»i báº¡n thÃ¢n thiáº¿t',\n",
              " 'uit_002076': 'NhÃ  vua ÄÃ¡p láº¡i má»t cÃ¡ch giáº­n dá»¯, ÄÃ¡nh Äáº­p vÃ  giáº­t tÃ³c con trai mÃ¬nh, trÆ°á»c khi ÄÆ°a Gaveston Äi lÆ°u ÄÃ y',\n",
              " 'uit_002077': 'báº¡n thÃ¢n thiáº¿t',\n",
              " 'uit_002078': 'khÃ´ng rÃµ lÃ½ do',\n",
              " 'uit_002079': 'Eleanor de Clare',\n",
              " 'uit_002080': 'lÃªn Ã¡n quyáº¿t liá»t',\n",
              " 'uit_002081': 'Cáº£ Edward vÃ  Gaveston Äá»u cÃ³ quan há» vá»i vá»£ há», vÃ  Äá»u cÃ³ con; Edward cÅ©ng cÃ³ má»t Äá»©a con ngoáº¡i hÃ´n, vÃ  cÃ³ thá» ÄÃ£ cÃ³ cuá»c tÃ¬nh vá»i cÃ´ chÃ¡u gÃ¡i, Eleanor de Clare',\n",
              " 'uit_002082': 'Eleanor de Clare',\n",
              " 'uit_002083': 'ráº¥t phá»©c táº¡p bá»i nhá»¯ng báº±ng chá»©ng xÃ¡c Äá»nh vá» má»i quan há» chi tiáº¿t thá»±c sá»± cÃ²n khÃ¡ Ã­t á»i',\n",
              " 'uit_002084': 'Adam Orleton',\n",
              " 'uit_002085': 'mÃ´ táº£ cÃ¡ch mÃ  Edward \"cáº£m tháº¥y nhÆ° tÃ¬nh yÃªu\" dÃ nh cho Gaveston',\n",
              " 'uit_002086': 'bá» buá»c tá»i',\n",
              " 'uit_002087': 'GiÃ¡m má»¥c Winchester',\n",
              " 'uit_002088': 'Adam Orleton',\n",
              " 'uit_002089': 'Adam Orleton',\n",
              " 'uit_002090': '1320',\n",
              " 'uit_002091': 'cÃ¡ch mÃ  Edward \"cáº£m tháº¥y nhÆ° tÃ¬nh yÃªu\" dÃ nh cho Gaveston, ráº±ng \"Ã´ng dÃ­nh dÃ¡ng vÃ o má»t giao Æ°á»c khÃ´ng thay Äá»i, vÃ  rÃ ng buá»c chÃ­nh Ã´ng vá»i Ã´ng ta trÆ°á»c táº¥t cáº£ nhá»¯ng ngÆ°á»i khÃ¡c vá»i má»t tÃ¬nh yÃªu báº¥t kháº£ phÃ¢n li, vá»¯ng bá»n quyáº¿n rÅ© vÃ  buá»c cháº·t báº±ng má»t má»i rÃ ng\"',\n",
              " 'uit_002092': '1308',\n",
              " 'uit_002093': 'GiÃ¡o hoÃ ng Boniface VIII vÃ  Hiá»p sÄ© Templar',\n",
              " 'uit_002094': 'lÃ½ cho chÃ¡nh trá»',\n",
              " 'uit_002095': 'GiÃ¡o hoÃ ng Boniface VIII vÃ  Hiá»p sÄ© Templar',\n",
              " 'uit_002096': 'cha vÃ  cha vá»£ cá»§a Edward',\n",
              " 'uit_002097': 'cha vÃ  cha vá»£',\n",
              " 'uit_002098': 'má»t pháº§n tá»« lÃ½ cho chÃ¡nh trá»',\n",
              " 'uit_002099': 'nhÆ° anh em, vÃ  nhá»¯ng chÃº thÃ­ch dá»©t khoÃ¡t ráº±ng Edward coi Gaveston lÃ  ngÆ°á»i anh nuÃ´i cá»§a Ã´ng',\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import Raise\n",
        "import json\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Chuáº©n hÃ³a cÃ¢u tráº£ lá»i: loáº¡i bá» kÃ½ tá»± thá»«a, viáº¿t thÆ°á»ng.\"\"\"\n",
        "    import re\n",
        "    s = s.lower()\n",
        "    # s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)  # Loáº¡i bá» cÃ¡c tá»« khÃ´ng cáº§n thiáº¿t\n",
        "    s = re.sub(r\"[^a-z0-9\\s]\", \"\", s)  # Loáº¡i bá» dáº¥u cÃ¢u\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()  # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a\n",
        "    return s\n",
        "\n",
        "def compute_scores(gold_answers, predicted_answers):\n",
        "    exact_matches = 0\n",
        "    total_f1 = 0\n",
        "    n = len(gold_answers)\n",
        "\n",
        "    for id_, gold in gold_answers.items():\n",
        "        pred = predicted_answers.get(id_, \"\")\n",
        "\n",
        "        # # Chuáº©n hÃ³a cÃ¢u tráº£ lá»i\n",
        "        gold = normalize_answer(gold)\n",
        "        pred = normalize_answer(pred)\n",
        "\n",
        "        # Exact Match\n",
        "        if gold == pred:\n",
        "            exact_matches += 1\n",
        "\n",
        "        # F1 Score\n",
        "        gold_tokens = gold.split()\n",
        "        pred_tokens = pred.split()\n",
        "        common_tokens = set(gold_tokens) & set(pred_tokens)\n",
        "        num_common = len(common_tokens)\n",
        "\n",
        "        if num_common == 0:\n",
        "            f1 = 0\n",
        "        else:\n",
        "            precision = num_common / len(pred_tokens)\n",
        "            recall = num_common / len(gold_tokens)\n",
        "            f1 = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "        total_f1 += f1\n",
        "\n",
        "    exact_match = exact_matches / n\n",
        "    f1_score_avg = total_f1 / n\n",
        "\n",
        "    return exact_match, f1_score_avg\n",
        "\n",
        "# Load dá»¯ liá»u tá»« file JSON\n",
        "with open(\"/content/ground_truth.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ground_truth = json.load(f)\n",
        "\n",
        "with open(\"/content/transformers/results/predictions/predict_predictions.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    predictions = json.load(f)\n",
        "\n",
        "# TÃ­nh toÃ¡n EM vÃ  F1\n",
        "em, f1 = compute_scores(ground_truth, predictions)\n",
        "\n",
        "print(f\"Exact Match: {em * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_obQdZoNsVLm",
        "outputId": "b6235ee7-01c7-46b9-a0fe-2d6fe96c49ac"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exact Match: 44.15%\n",
            "F1 Score: 65.19%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXACT MATCH : 44.15%\n",
        "# F1 SCORE : 65.19%"
      ],
      "metadata": {
        "id": "UJ0AUFP88vpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_fdOEHU8z8c",
        "outputId": "5933c113-9d16-4e6b-9787-c21073fb6203"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n"
          ]
        }
      ]
    }
  ]
}